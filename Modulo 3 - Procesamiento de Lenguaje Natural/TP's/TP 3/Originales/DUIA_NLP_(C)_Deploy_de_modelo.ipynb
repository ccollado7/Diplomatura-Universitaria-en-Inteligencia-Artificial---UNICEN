{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DUIA NLP (C) - Deploy de modelo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrsOVS_2Iyez"
      },
      "source": [
        "# Procesamiento de Lenguaje Natural - **Deploy de un modelo**\n",
        "\n",
        "En esta notebook vamos a jugar con la creación de un modelo basados en el procesamiento que hicimos en notebooks pasadas y vamos a hacer un mini deploy de dicho modelo.\n",
        "\n",
        "*Nota*. Para los efectos de esta notebook vamos a usar una estrategia simple de división del dataset en training-test. Se podrían optar por otras opciones u otros esquemas de división. Asimismo, si bien se muestra una posibilidad para la elección del \"mejor\" modelo mediante una optimización de parámetros, no será utilizada para el modelo a guardar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3udATCXHI7Zl"
      },
      "source": [
        "### Creación del modelo\n",
        "\n",
        "Primero, vamos a traernos los datos con los que vamos a estar trabajando. Vamos a seguir utilizando el dataset simple de detección de hate speech, del que nos vamos a quedar con un atributo de tipo texto y la clase numérica (0: No es hate speech, 1: hate speech, 2: offensive speech)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptD3Rd0jItpM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "2b598031-b24f-4118-e483-2c8d6523bce6"
      },
      "source": [
        "# Cargamos los datos necesarios\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv\"\n",
        "df = pd.read_csv(url, usecols=['class', 'tweet']) # de todas las columnas que tiene el dataset, nos vamos a quedar solo con el texto y la clase\n",
        "\n",
        "print(df[:1000]) # limitamos la cantidad de instancias para que no tarde ni el pre-processing ni el training"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     class                                              tweet\n",
            "0        2  !!! RT @mayasolovely: As a woman you shouldn't...\n",
            "1        1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
            "2        1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
            "3        1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
            "4        1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
            "..     ...                                                ...\n",
            "995      1  &#128514;&#128514;&#128514;&#128514; RT @SMASH...\n",
            "996      1  &#128514;&#128514;&#128514;&#128514; bitch if ...\n",
            "997      1  &#128514;&#128514;&#128514;&#128514; these fol...\n",
            "998      1  &#128514;&#128514;&#128514;&#128514;&#128514; ...\n",
            "999      1  &#128514;&#128514;&#128514;&#128514;&#128514;&...\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGR3bqdtKH-0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(df, test_size = 0.80,random_state=42) # limitamos el tamaño del training para que no tarde\n",
        "\n",
        "# recordemos que para entrenar tenemos separar la clase\n",
        "X_train = train_set.drop(\"class\", axis=1)  \n",
        "y_train = train_set[\"class\"].copy()\n",
        "\n",
        "X_test = test_set.drop(\"class\",axis=1) # nos dejamos también preparado el test set\n",
        "y_test = test_set[\"class\"].copy() # nos dejamos también preparado el test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwaiAEJ7K4Gp"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWUEAR0-LQtv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7dbcf60a-40cc-4409-dec7-aacc30725cd1"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj3slMA7LCL2"
      },
      "source": [
        "count_normal = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('count', count_normal, \"tweet\")]) # importante definir las columnas sobre las cuales se aplica\n",
        "\n",
        "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', SVC(probability=True))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FuOff5LLgRE"
      },
      "source": [
        "Finalmente, vamos a entrenar el modelo. De acuerdo al modelo que elijamos, puede tardar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgo2aIF6LLee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "be7f69d8-fe04-4c6c-f567-4c538f46a5e6"
      },
      "source": [
        "rf.fit(X_train,y_train)      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('count',\n",
              "                                                  CountVectorizer(analyzer='word',\n",
              "                                                                  binary=False,\n",
              "                                                                  decode_error='strict',\n",
              "                                                                  dtype=<class 'numpy.int64'>,\n",
              "                                                                  encoding='utf-8',\n",
              "                                                                  input='content',\n",
              "                                                                  lowercase=True,\n",
              "                                                                  max_df=1.0,\n",
              "                                                                  max_features=None,\n",
              "                                                                  min_df=1,\n",
              "                                                                  ngram_rang...\n",
              "                                                                  strip_accents=None,\n",
              "                                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                                                  tokenizer=None,\n",
              "                                                                  vocabulary=None),\n",
              "                                                  'tweet')],\n",
              "                                   verbose=False)),\n",
              "                ('classifier',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1, probability=True,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUDt1xGLL5FK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "41b8c072-dfbb-44d0-b3cf-9b69a415a522"
      },
      "source": [
        "rf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL7CrzKiUG76"
      },
      "source": [
        "### Model selection\n",
        "\n",
        "El pipeline que definimos también puede ser utilizado en el proceso de selección de modelos. En el siguiente fragmento de código se cicla por diferentes modelos de clasificación provistos por sklearn, para aplicar las transformaciones y luego entrenarlos.\n",
        "\n",
        "Nota. Hay más clasificadores disponibles para probar.\n",
        "\n",
        "Nota 2. Puede tardar!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwUloUnJUHMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "6ac46c8d-0267-46a1-f50c-035c65596d20"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    SVC()\n",
        "    ]\n",
        "\n",
        "for classifier in classifiers:\n",
        "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', classifier)])\n",
        "    pipe.fit(X_train, y_train)   \n",
        "    print(classifier)\n",
        "    print(\"model score: %.3f\" % pipe.score(X_test, y_test)) # qué retorna depende del modelo que se usa. En clasificación retorna accuracy promedio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
            "                     weights='uniform')\n",
            "model score: 0.822\n",
            "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')\n",
            "model score: 0.885\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "model score: 0.871\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "model score: 0.889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCK8dwi0UKxM"
      },
      "source": [
        "Finalmente, el pipeline que definimos también puede ser utilizado en un grid search para encontrar la mejor combinación de hiper-parámetros.\n",
        "\n",
        "Para hacer esto, lo primero que hay que hacer es crear una grilla de parámetros para el modelo elegido. Algo importante a notar es que a los nombres de los parámetros hay que agregarles el nombre que le dimos al parámetro que representaba al algoritmo (en este caso de clasificación, al que llamamos ``classifier``).\n",
        "\n",
        "Luego, creamos el objeto de grid search el cual incluye el pipeline original. Cuando llamemos al método ``fit``, antes de realizar la búsqueda del grid search se aplicarán las transformaciones.\n",
        "\n",
        "Nota. En este ejemplo se están considerando dos parámetros para el ``RandomForestClassifier``. De acuerdo al clasificador, los parámetros que se podrán optimizar.\n",
        "\n",
        "Nota 2. Hay múltiples métricas de [scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring) que pueden ser consideradas. Ver también la documentación referida a [model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
        "\n",
        "Nota 3. Puede tardar!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPeqlKYpUMgA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "46c0af00-3c4c-42cc-efcd-f039c2ad1e28"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "rfcv = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', RandomForestClassifier())])\n",
        "\n",
        "param_grid = { \n",
        "    'classifier__n_estimators': [1, 3, 10],\n",
        "    'classifier__max_features': ['auto', 'sqrt'],\n",
        "}\n",
        "\n",
        "CV = GridSearchCV(rfcv, param_grid, cv=5,\n",
        "                           scoring='f1_weighted') \n",
        "                  \n",
        "CV.fit(X_train, y_train)  \n",
        "print(CV.best_params_)    \n",
        "print(CV.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'classifier__max_features': 'sqrt', 'classifier__n_estimators': 3}\n",
            "0.8421755733496846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdGVERyw3tDY"
      },
      "source": [
        "#### Y cómo sabemos cuál es el \"mejor\" modelo?\n",
        "\n",
        "Ya hicimos el test de nuestros modelos varias veces, tenemos los valores de diversas métricas para cada ejecución de cada modelo, cómo sabemos ahora cuál es el mejor? Comparar los valores individuales de las métricas puede no ser suficiente. \n",
        "\n",
        "Supongamos que hicimos 10 ejecuciones para cada modelo, promediamos los resultados, nos quedamos con los promedios. Podemos asegurar que si la métrica del modelo A nos da mayor que la del modelo B (suponiendo que la métrica a mayor valor, mejor resultado) el modelo A es mejor que el modelo B? Por ejemplo, si el modelo A tiene un resultado de 0.88 y el modelo B un resultado de 0.86, podemos afirmar que el modelo A es mejor que el B. La respuesta es: **no necesariamente**.\n",
        "\n",
        "* Las métricas de performance pueden no ser el único criterio que tenemos para analizar el modelo. También pueden entrar en juego cuestiones como complejidad computacional, recursos consumidos, evaluaciones subjetivas de los usuarios, métricas de diversidad, etc.\n",
        "\n",
        "* No todas las diferencias observables entre los modelos son significativas. Que se observe una diferencia (como en el caso de 0.88 y 0.86) no quiere decir que la diferencia no se haya debido a la casualidad o al azar.\n",
        "\n",
        "\n",
        "Entonces, qué se puede hacer en estas situaciones? Aplicar tests estadísticos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Gm7AFbEtmt"
      },
      "source": [
        "import scipy.stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu0NhmjwuvPh"
      },
      "source": [
        "metrics_model_A = [0.516129, 0.444444, 0.631579, 0.516129, 0.545455, 0.344828, 0.5, 0.533333, 0.594595, 0.428571] # asumamos que estos son los resultados de la métrica X de haber ejecutado 10 veces el modelo A\n",
        "metrics_model_B = [0.516129, 0.645161, 0.571429, 0.4, 0.533333, 0.4375, 0.428571, 0.387097, 0.529412, 0.545455] # asumamos que estos son los resultados de la métrica X de haber ejecutado 10 veces el modelo B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k51i3goSGraU"
      },
      "source": [
        "Lo primero que hay que testear es normalidad, para saber qué tipo de test utilizar. En este test de normalidad hay que mirar el valor del ``p-value``. Si el ``p-value`` es menor que el valor de confianza ``alpha`` que se define (usualmente en ``0.01`` o ``0.05``), se puede rechazar la hipótesis nula de que las distribuciones son normales. Por el contrario, si el ``p-value`` es mayor que el ``alpha``, se debe asumir que la distribución es normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asDv9KTQGVG3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "0d32625a-7cc2-43b6-a14e-abf4c4ad52cb"
      },
      "source": [
        "print(scipy.stats.normaltest(metrics_model_A))\n",
        "print(scipy.stats.normaltest(metrics_model_B))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NormaltestResult(statistic=0.821362046254129, pvalue=0.6631984428307685)\n",
            "NormaltestResult(statistic=0.30729389511497246, pvalue=0.8575747364264382)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:1535: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
            "  \"anyway, n=%i\" % int(n))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz1gcrL-IKUB"
      },
      "source": [
        "Una vez que sabemos qué tipo de distribuciones tenemos, podemos aplicar el test.\n",
        "\n",
        "En este caso, asumimos que las ejecuciones para el modelo A y el modelo B fueron realizados para los mismas particiones de los datos, con lo que vamos a utilizar tests ``paired``. Es decir, por ejemplo, la ejecución 1 de ambos modelos se realizó con la partición de datos X1.\n",
        "\n",
        "En estos tests, hay que mirar de nuevo el ``p-value``. Si ``p-value < alpha``, podemos rechazar la hipótesis nula de que las diferencias entre las distribuciones no son significativas. Por el contrario, si ``p-value >= alpha``, no podemos rechazarla y debemos asumir que no hay una diferencia estadística entre las distribuciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Seh8QSkRGWct",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "09ad5637-e8b3-428d-8914-4db326db4c73"
      },
      "source": [
        "print(scipy.stats.ttest_rel(metrics_model_A,metrics_model_B)) # en el caso en el que las dos distribuciones sea normal\n",
        "\n",
        "print(scipy.stats.wilcoxon(metrics_model_A,metrics_model_B)) # en el caso en el que al menos una de las distribuciones no sea normal."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ttest_relResult(statistic=0.17429440352789408, pvalue=0.865491728742791)\n",
            "WilcoxonResult(statistic=21.0, pvalue=0.8589549227374824)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_o0V8iNLIlB"
      },
      "source": [
        "Vamos a probar con otras distribuciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plm3jULGLLM1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "18e3e1c7-414b-4185-a3ca-643fc954f200"
      },
      "source": [
        "metrics_model_A = [0.516129, 0.444444, 0.631579, 0.516129, 0.545455, 0.344828, 0.5, 0.533333, 0.594595, 0.428571] # asumamos que estos son los resultados de la métrica X de haber ejecutado 10 veces el modelo A\n",
        "metrics_model_B = [0.693878, 0.666667, 0.666667, 0.693878, 0.666667, 0.571429, 0.666667, 0.555556, 0.5, 0.693878]\n",
        "\n",
        "print(scipy.stats.normaltest(metrics_model_A))\n",
        "print(scipy.stats.normaltest(metrics_model_B))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NormaltestResult(statistic=0.821362046254129, pvalue=0.6631984428307685)\n",
            "NormaltestResult(statistic=3.0192637321155904, pvalue=0.2209913173914211)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:1535: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
            "  \"anyway, n=%i\" % int(n))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZkQlQF-LYbw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4235c366-5638-4158-8331-8c584f76164a"
      },
      "source": [
        "print(scipy.stats.ttest_rel(metrics_model_A,metrics_model_B))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ttest_relResult(statistic=-3.7233729994558984, pvalue=0.004745815527517936)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBzzpYKmujPc"
      },
      "source": [
        "Nota. Accuracy puede ser la primera métrica que se nos ocurre para evaluar la performance de un clasificador, pero puede que no sea la más adecuada en todos los contextos. Hay otras métricas más robustas que pueden darnos una mejor idea de cómo funciona nuestro modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFprFbx6KHyo"
      },
      "source": [
        "### Salvar/Exportar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTL5idLxKHKP"
      },
      "source": [
        "En este caso el modelo lo necesitamos solo acá, pero qué pasaría si nosotros quisieramos llevar este modelo a otro ambiente o simplemente reemplazar otro modelo que teníamos por este. Tenemos que repetir todos los pasos de definición del pipeline y reentrenar? No, no es necesario.\n",
        "\n",
        "Lo que podemos hacer es persistir el modelo, es decir, guardarlo en un archivo que luego podremos levantar en donde nosotros quisiéramos utilizarlo.\n",
        "\n",
        "La primera alternativa es usar ```joblib```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TySSFiv2KKJr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b33e618-928d-4582-e186-8a8e28d52ccd"
      },
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(rf, \"hate_speech_detection_model.pkl\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hate_speech_detection_model.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI1xK_6sKSRq"
      },
      "source": [
        "Luego, para cargarlo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzmDJOmDKMg3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a3d71e3-6769-406c-dcd4-09149ac0c90b"
      },
      "source": [
        "loaded_model = joblib.load(\"hate_speech_detection_model.pkl\")\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m9eHowwKRHe"
      },
      "source": [
        "Otra alternativa es usar ```Pickle```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8JY9pdyKOVl"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SntUNJmXKWke"
      },
      "source": [
        "Luego, para cargarlo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD-WapI6KPam",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9ab7dfd-dc89-437c-b53f-e56110a5eda5"
      },
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yeR9K9jKq-c"
      },
      "source": [
        "### Deploy del modelo\n",
        "\n",
        "Para hacer el deploy del modelo, vamos a crear una aplicación de hate speech detection para deployarla como un servicio REST básandonos en el modelo que creamos en los bloques anteriores.\n",
        "\n",
        "Vamos a usar:\n",
        "\n",
        "* [Flask](https://github.com/pallets/flask): uno de los micro web frameworks más populares.\n",
        "* [flask_ngrok](https://pypi.org/project/flask-ngrok/): herramienta que nos permite hacer demos de nuestras apps Flask. Nos permite servir nuestra applicación desde una simple notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XChYQegAMfx2"
      },
      "source": [
        "Instalamos las dependencias que vamos a necesitar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-xecug8Mbts",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "e108eb4d-8132-4bd9-f3bd-fb50da000d9b"
      },
      "source": [
        "!pip install flask\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask) (1.1.1)\n",
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgMmPHfyMnKI"
      },
      "source": [
        "Lo primero que vamos a hacer es dejar accessible nuestro modelo entrenado a nuestra aplicación. \n",
        "\n",
        "En este caso, si venimos ejecutando toda la notebook vamos a tener disponible nuestro ``hate_speech_detection_model.pkl``.\n",
        "\n",
        "Nota. Cada vez que reiniciemos el runtime, deberíamos generar o levantar nuestro modelo de algún lado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5HqKr94Mmtp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "24fd2c7d-6575-46b9-d0ac-6da7181ac3ce"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask,request,jsonify\n",
        "import pandas as pd \n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "  return \"<h1>Hate Speech Detector!</h1>\"\n",
        "\n",
        "model = joblib.load(\"hate_speech_detection_model.pkl\") # vamos a levantar nuestro modelo\n",
        "\n",
        "@app.route('/predict',methods=['GET','POST']) # los tipos de métodos que soportamos\n",
        "def predict():\n",
        "  \n",
        "  df_n = pd.DataFrame({\"tweet\":[request.args['text']]}) # tomamos el texto que nos pasaron para hacer la predicción\n",
        "\n",
        "  prediction = model.predict(df_n) # utilizamos el modelo para predecir\n",
        "  pred_proba = model.predict_proba(df_n) # obtenemos la probabilidad de la predicción --> No está disponible para todos los clasificadores!\n",
        "\n",
        "  if prediction == 0:\n",
        "    pred_text = 'Hate'\n",
        "  elif prediction == 1:\n",
        "    pred_text = 'Offensive'\n",
        "  else:\n",
        "    pred_text = 'Neither'   \n",
        "\n",
        "  print(request.args['text']) # en la consola de acá imprimimos el texto\n",
        "  print(pred_proba.dtype) # en la consola de acá imprimimos la probabilidad\n",
        "\n",
        "  return \"<h2>El texto \\\"\"+request.args['text']+\"\\\" fue clasificado como: \"+pred_text+\" con una probabilidad de: \"+str(pred_proba[0][prediction])+\"</h2>\"\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://1bf944b8874a.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [15/Aug/2020 17:34:01] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:02] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:02] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:02] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:03] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:04] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:05] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:06] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:08] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:08] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:11] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:12] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:13] \"\u001b[37mGET /predict?text=I%20hate%20everyone HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "I hate everyone\n",
            "float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [15/Aug/2020 17:34:15] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:16] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:29] \"\u001b[37mGET /predict?text=hello HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "hello\n",
            "float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [15/Aug/2020 17:34:35] \"\u001b[37mGET /predict?text=I%20hate%20you HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:35] \"\u001b[37mGET /predict?text=%22I%20love%20people%20(?%22 HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "I hate you\n",
            "float64\n",
            "\"I love people (?\"\n",
            "float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [15/Aug/2020 17:34:39] \"\u001b[37mGET /predict?text=i%27m_trying_this_text HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:39] \"\u001b[33mGET /Hate HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i'm_trying_this_text\n",
            "float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [15/Aug/2020 17:34:39] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:41] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:44] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:44] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:54] \"\u001b[37mGET /predict?text=I%20hate%20Twitter HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "I hate Twitter\n",
            "float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [15/Aug/2020 17:34:54] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:54] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:34:55] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:35:03] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu2SCKUZMrq4"
      },
      "source": [
        "Y listo! Ahora ya tenemos nuestro detector de hate speech disponible! Si le pasamos un argumento ```text``` con un string nos va a retornar si es o no hate speech! Para pasarle el argumento ``URL_NGROK/predict?text=TEXTO``. Por ejemplo, ``http://b36fd66da979.ngrok.io/predict?text=\"I hate everyone!!\"``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gn3VOBHMtQI"
      },
      "source": [
        "El formato de salida que le dimos no es muy amigable con el usuario si lo que queremos es dejarlo disponible y que otros lo usen. Para eso, vamos a modificar la salida para que nos retorne un json."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1awUhRZMqvX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "outputId": "3f8ea805-5cbd-4dd3-fb27-d556a6d82290"
      },
      "source": [
        "# es el mismo código que antes, solo cambia el return\n",
        "\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask,request,jsonify\n",
        "import pandas as pd \n",
        "from sklearn.externals import joblib\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "  return \"<h1>Hate Speech Detector!</h1>\"\n",
        "\n",
        "model = joblib.load(\"hate_speech_detection_model.pkl\") \n",
        "\n",
        "@app.route('/predict',methods=['GET','POST'])\n",
        "def predict():\n",
        "  \n",
        "  df_n = pd.DataFrame({\"tweet\":[request.args['text']]})\n",
        "\n",
        "  prediction = model.predict(df_n)\n",
        "  pred_proba = model.predict_proba(df_n)\n",
        "\n",
        "  if prediction == 0:\n",
        "    pred_text = 'Hate'\n",
        "  elif prediction == 1:\n",
        "    pred_text = 'Offensive'\n",
        "  else:\n",
        "    pred_text = 'Neither'  \n",
        "\n",
        "  output = {'text': request.args['text'], 'prediction': pred_text, 'confidence': str(pred_proba[0][prediction])}\n",
        "\n",
        "  return json.dumps(output)\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://fdf3de0310f0.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [15/Aug/2020 17:35:57] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:35:58] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:35:58] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:35:58] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:35:59] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:35:59] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:35:59] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:00] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:00] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:00] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:00] \"\u001b[37mGET /predict?text=I%20hate%20everyone HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:01] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:09] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:25] \"\u001b[37mGET /predict?text=%22I%20love%20everyone!!223%22 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:26] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:38] \"\u001b[37mPOST /predict?text=I%20think%20you%20are%20not%20pretty HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:39] \"\u001b[33mGET /%20i%20hate%20it HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:44] \"\u001b[37mPOST /predict?text=I%20think%20you%20are%20not%20pretty HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:45] \"\u001b[37mGET /predict?text=%22hello%22 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:52] \"\u001b[37mGET /predict?text=I%20hate%20everyone HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:53] \"\u001b[31m\u001b[1mGET /predict?todavia%20no%20termine%20el%20tp2%20:( HTTP/1.1\u001b[0m\" 400 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:36:59] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:01] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:02] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:13] \"\u001b[37mGET /predict?text=prueba HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:14] \"\u001b[37mGET /predict?text=hello HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:17] \"\u001b[33mGET /%20i%20hate%20it HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:18] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:21] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:23] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:30] \"\u001b[37mPOST /predict?text=I%20think%20you%20are%20not%20pretty HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:30] \"\u001b[37mGET /predict?text=%22hello%22 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:33] \"\u001b[37mGET /predict?text=hello HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:37] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:38] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:42] \"\u001b[37mPOST /predict?text=I%20think%20you%20are%20not%20pretty HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:37:54] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [15/Aug/2020 17:38:09] \"\u001b[37mGET /predict?text=hello HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zObccEldMw6-"
      },
      "source": [
        "Si queremos probar de consumirlo como un servicio \"normal\", podemos ejecutar el siguiente código (en otra notebook, dado que acá estamos ejecutando el server)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtVYH7KRMy9S"
      },
      "source": [
        "import requests\n",
        "\n",
        "text = \"I think you are not pretty\"\n",
        "base = \"COMPLETAR_URL_NGROK\"\n",
        "url = base+'/predict?text='+text\n",
        "\n",
        "response = requests.post(url)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJwdLAfZM1yl"
      },
      "source": [
        "Otra posibilidad sería esto mismo deployarlo en algún proveedor Cloud o incluso hacerlo accesible como una imagen de Docker.\n",
        "\n"
      ]
    }
  ]
}