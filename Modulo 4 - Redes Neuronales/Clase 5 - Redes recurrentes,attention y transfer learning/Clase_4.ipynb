{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase-4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M032A0y9xkQd"
      },
      "source": [
        "# Redes recurrentes\n",
        "\n",
        "Ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiyxuk_ztYit"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = False\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "class CharacterTable:\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = \"0123456789+ \"\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(\n",
        "        \"\".join(\n",
        "            np.random.choice(list(\"0123456789\"))\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))\n",
        "        )\n",
        "    )\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print(\"Total questions:\", len(questions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCrSdAnjttBG"
      },
      "source": [
        "print(questions[0])\n",
        "print(expected[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z30qcVguPFH"
      },
      "source": [
        "print(\"Vectorization...\")\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7gB5EV7uYOF"
      },
      "source": [
        "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "i = Input((MAXLEN, len(chars)))\n",
        "d = LSTM(128, return_sequences=False)(i)\n",
        "d = RepeatVector(DIGITS+1)(d)\n",
        "d = LSTM(128, return_sequences=True)(d)\n",
        "d = Dense(len(chars), activation='softmax')(d)\n",
        "\n",
        "model = Model(i, d)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRQoRYcnlbsV"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_eynZMLwAF_"
      },
      "source": [
        "epochs = 15 #30\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        if correct == guess:\n",
        "            print(\"☑ \" + guess)\n",
        "        else:\n",
        "            print(\"☒ \" + guess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB0lpnJLycsS"
      },
      "source": [
        "## IMDB "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzeO5BX2x0tD"
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQmCJbp2zCCl"
      },
      "source": [
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "\n",
        "def load_imdb_ds(path, shuffle_ds=True, random_state=None):\n",
        "    def load_text(path_sel):\n",
        "        xa = []\n",
        "        for f in os.listdir(path_sel):\n",
        "            f = open(path_sel + os.sep + f, 'r')\n",
        "            xa.append(next(f))\n",
        "        return xa\n",
        "    x = load_text(path + os.sep + 'pos')\n",
        "    y = [1] * len(x)\n",
        "    xn = load_text(path + os.sep + 'neg')\n",
        "    x.extend(xn)\n",
        "    y.extend([0] * len(xn))\n",
        "    if shuffle_ds:\n",
        "        shuffle(x, y, random_state=random_state)\n",
        "    return x, y\n",
        "\n",
        "x_train_text, y_train = load_imdb_ds('aclImdb/train', random_state=42)\n",
        "x_test_text, y_test = load_imdb_ds('aclImdb/test', random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7xuHRSQ1-OE"
      },
      "source": [
        "x_train_text[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hZwn7kx4_dW"
      },
      "source": [
        "!pip install bs4\n",
        "!pip install tqdm\n",
        "\n",
        "from collections import Counter, deque\n",
        "from tqdm.notebook import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "import re \n",
        "\n",
        "def preprocessor(text):\n",
        "    # remove HTML tags\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    \n",
        "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
        "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
        "    emoticons = re.findall(r, text)\n",
        "    text = re.sub(r, '', text)\n",
        "    \n",
        "    # convert to lowercase and append all emoticons behind (with space in between)\n",
        "    # replace('-','') removes nose of emoticons\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
        "    return text\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "#Baja los stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def tokenizer_stem_nostop(text):\n",
        "    porter = PorterStemmer()\n",
        "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
        "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
        "\n",
        "\n",
        "def tokenizer_simple(text):\n",
        "    return [w for w in re.split('\\s+', text.strip()) \\\n",
        "            if re.match('[a-zA-Z]+', w)]\n",
        "\n",
        "\n",
        "def process_corpus(data, words_id=None, min_reps=5, tokenizer=tokenizer_simple):\n",
        "    corpus = []\n",
        "    for text in tqdm(data):\n",
        "        corpus.append(tokenizer(preprocessor(text)))\n",
        "        \n",
        "    if words_id is None:\n",
        "        #Cuenta palabras en el corpus\n",
        "        words = Counter()\n",
        "\n",
        "        for s in corpus:\n",
        "            for w in s:\n",
        "                words[w] += 1\n",
        "\n",
        "        #Elimina palabras con menos de 5 repeticiones\n",
        "        words_id = {}\n",
        "        id_next = 1\n",
        "        for w, c in words.items():\n",
        "            if c >= min_reps:\n",
        "                words_id[w] = id_next\n",
        "                id_next += 1\n",
        "\n",
        "    id_words = { v:k for k, v in words_id.items()}\n",
        "    corpus_id = [[words_id[w] for w in s if w in words_id] for s in corpus]\n",
        "    return corpus_id, words_id, id_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XjE2V9I5YDH"
      },
      "source": [
        "x_train, words_id, id_words = process_corpus(x_train_text)\n",
        "x_test, _, _ = process_corpus(x_test_text, words_id=words_id)\n",
        "y_train = np.expand_dims(np.asarray(y_train), axis=-1)\n",
        "y_test = np.expand_dims(np.asarray(y_test), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDhVJy_-dEKE"
      },
      "source": [
        "print(x_train[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi_Enpie7lOc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "MAXLEN = 50\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=MAXLEN)\n",
        "x_test = pad_sequences(x_test, maxlen=MAXLEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji4H8bVGdo5b"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufo0sgOs8YD1"
      },
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "i = Input((None,))\n",
        "d = Embedding(len(words_id) + 1, 300, mask_zero=True)(i)\n",
        "d = Dropout(0.5)(d)\n",
        "d = LSTM(128, return_sequences=True)(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = LSTM(128, return_sequences=False)(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moyL69E1lfii"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOPRzx5a9xGK"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=2, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEFtG2fZDWMC"
      },
      "source": [
        "## Embeddings preentrenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px1WjIVSIC_r"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L_mNZ3AIGgt"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import gensim\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove2word2vec(glove_input_file='glove.6B.300d.txt', word2vec_output_file='vectors.txt')\n",
        "glove = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExvYd2scDVOb"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "emb = Embedding(len(words_id) + 1, 300, mask_zero=True, trainable=False)\n",
        "i = Input((None,))\n",
        "d = emb(i)\n",
        "d = Dropout(0.5)(d)\n",
        "d = LSTM(128, return_sequences=True)(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = LSTM(128, return_sequences=False)(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYcrC9tH7z8"
      },
      "source": [
        "base_emb = np.zeros(emb.embeddings.shape)\n",
        "\n",
        "for w, i in words_id.items():\n",
        "    if w in glove:\n",
        "        base_emb[i, :] = (glove[w] / np.sum(glove[w]))\n",
        "\n",
        "K.set_value(emb.embeddings, base_emb)\n",
        "emb = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rThVs-cYNV-K"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=2, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQmQ2aZ5zebx"
      },
      "source": [
        "## Generacion de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTAXOKhYzdsz"
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "path = get_file(\n",
        "    'nietzsche.txt',\n",
        "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('corpus length:', len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# build the model: a single LSTM\n",
        "print('Build model...')\n",
        "i = Input((maxlen, len(chars)))\n",
        "d = LSTM(128)(i)\n",
        "d = Dense(len(chars), activation='softmax')(d)\n",
        "\n",
        "model = Model(i, d)\n",
        "\n",
        "optimizer = 'adam'#RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(60):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mi_WgiG0Akr"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzP_CEaeXXG2"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7yxmip7XZcd"
      },
      "source": [
        "from tensorflow.keras.layers import Attention, Add, GlobalAveragePooling1D, Dropout, Lambda, Embedding, Input, Dense\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "i = Input((None,))\n",
        "e = Embedding(len(words_id) + 1, 60, mask_zero=True, name='base_emb')(i)\n",
        "len_s = Lambda(lambda x: tf.expand_dims(tf.range(start=0, limit=MAXLEN, delta=1), axis=0))(i)\n",
        "pos_e = Embedding(MAXLEN, 60, mask_zero=True, name='base_pos')(len_s)\n",
        "\n",
        "sum = Add()([e, pos_e])\n",
        "dq = Dense(60)(sum)\n",
        "dk = Dense(60)(sum)\n",
        "\n",
        "mask = Lambda(lambda x: x != 0)(i)\n",
        "\n",
        "att = Attention()([dq, dk], mask=[mask, mask])\n",
        "attd = Dropout(0.1)(att)\n",
        "\n",
        "d = GlobalAveragePooling1D()(attd)\n",
        "d = Dropout(0.1)(d)\n",
        "d = Dense(100)(d)\n",
        "d = Dropout(0.1)(d)\n",
        "d = Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yo6wT831wSO"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnO2qHn-laKx"
      },
      "source": [
        "from tensorflow.keras.layers import Attention, Add, GlobalAveragePooling1D, Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "i = Input((None,))\n",
        "e = Embedding(len(words_id) + 1, 60, mask_zero=True, name='base_emb')(i)\n",
        "\n",
        "dq = Dense(60)(e)\n",
        "dk = Dense(60)(e)\n",
        "\n",
        "att = Attention()([dq, dk])\n",
        "attd = Dropout(0.1)(att)\n",
        "\n",
        "d = GlobalAveragePooling1D()(attd)\n",
        "d = Dropout(0.1)(d)\n",
        "d = Dense(100)(d)\n",
        "d = Dropout(0.1)(d)\n",
        "d = Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RohgCj9SZ8Y9"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV-H3d3sae5f"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU-5VAcAagqD"
      },
      "source": [
        "x_exam = ['The movie was excelent. It is probably the best movie ever', 'The movie was not good']\n",
        "x_exam_v, _, _ = process_corpus(x_exam, words_id)\n",
        "x_exam_v = pad_sequences(x_exam_v, maxlen=MAXLEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywKCQwRrbfQV"
      },
      "source": [
        "print(model.predict(x_exam_v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGFAm9PIbwol"
      },
      "source": [
        "x_exam_v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnRMH3D8e-z9"
      },
      "source": [
        "model_att = Model(i, [dq, dk])\n",
        "\n",
        "vq, vk = model_att(x_exam_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pSH0zFbfOfD"
      },
      "source": [
        "att_sal = tf.nn.softmax(tf.matmul(vq, vk, transpose_b=True))\n",
        "print(att_sal[0,-10:, -10:])\n",
        "print(att_sal[0,-5:, -5:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrxwQM4yfsuA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.imshow(att_sal[0,...])\n",
        "plt.show()\n",
        "plt.imshow(att_sal[0, -10:, -10:])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDj2Q-tehv1K"
      },
      "source": [
        "plt.imshow(att_sal[1,...])\n",
        "plt.show()\n",
        "plt.imshow(att_sal[1, -5:, -5:])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyrbzBq3v3-I"
      },
      "source": [
        "print(x_test_text[0])\n",
        "print(model.predict(np.expand_dims(x_test[0, :], axis=0)))\n",
        "vq, vk = model_att(np.expand_dims(x_test[0, :], axis=0))\n",
        "att_sal = tf.nn.softmax(tf.matmul(vq, vk, transpose_b=True))\n",
        "plt.imshow(att_sal[0,...])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}