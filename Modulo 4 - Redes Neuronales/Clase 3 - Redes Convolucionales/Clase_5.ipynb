{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Clase-5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NEesqJXkadx"
      },
      "source": [
        "# Redes Convolucionales\n",
        "Las redes convolucionales, o CNN (Convolutional Neural Network), son un tipo especializado de redes neuronales que han sido aplicado con mucho éxito en problemas en cuales los datos tienen forma de grillas, como son las imagenes. Se las conocen como redes convolucionales ya que aplican una operación matemática conocida como convolución. La convolución no es más que un operador móvil que se aplica repetidamente sobre los datos de entrada. Este operador está definido por una matríz pequeña, generalmente llamada kernel, que se aplica repetidamente sobre la imagen. Por ejemplo, imaginemos un kernel de 2 X 2.\n",
        "\n",
        "$$K=\\left[\\begin{array}{cc}\n",
        "k_{1,1} & k_{1,2}\\\\\n",
        "k_{2,1} & k_{2,2}\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "y una imagen de n X m:\n",
        "\n",
        "$$I=\\left[\\begin{array}{cc}\n",
        "i_{1,1} & i_{1,2} & ... & i_{1, m}\\\\\n",
        "i_{2,1} & i_{2,2} & ... & i_{2, m}\\\\\n",
        "... & ... & ... & ... \\\\\n",
        "i_{n,1} & i_{n,2} & ... & i_{n, m}\\\\\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "el resultado de aplicar la convolución sería:\n",
        "\n",
        "$$C=\\left[\\begin{array}{cc}\n",
        "c_{1,1} & c_{1,2} & ... & c_{1, m-1}\\\\\n",
        "c_{2,1} & c_{2,2} & ... & c_{2, m-1}\\\\\n",
        "... & ... & ... & ... \\\\\n",
        "c_{n-1,1} & c_{n-1,2} & ... & c_{n-1, m-1}\\\\\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "donde:\n",
        "\n",
        "$$c_{i, j} = i_{i, j} * k_{1,1} + i_{i, j+1} * k_{1,2} + i_{i+1, j} * k_{2,1} + i_{i+1, j+2} * k_{2,2}$$\n",
        "\n",
        "La operación de convolución ha sido usada con mucho exito en procesamiento de imagenes para detección de bordes, mejoramiento de imagenes, aplicación de blur, etc. Por ejemplo, Kirsch[1] propuso en 1971 una técnica que permite detectar estrucuras en las imagenes. Para esto, utiliza distintas [matrices de convolución](https://en.wikipedia.org/wiki/Kirsch_operator). Para ilustrar el la convolución utilizaremos $g^{(1)}$.\n",
        "\n",
        "$$g^{(1)}=\\left[\\begin{array}{cc}\n",
        "5 & 5 & 5 \\\\\n",
        "-3 & 0 & -3 \\\\\n",
        "-3 & -3 & -3\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "[1] Kirsch, R. (1971). \"[Computer determination of the constituent structure of biological images](https://www.sciencedirect.com/science/article/pii/0010480971900346)\". Computers and Biomedical Research. 4: 315–328. doi:10.1016/0010-4809(71)90034-6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dcA70RQkad1"
      },
      "source": [
        "#Cargamos bibliotecas y demás\n",
        "!pip install tqdm\n",
        "#!pip install pil\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Concatenate, MaxPooling2D, BatchNormalization, GaussianNoise, GaussianDropout, Activation, Add\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from cv2 import imread\n",
        "from tqdm.notebook import tqdm\n",
        "import os.path\n",
        "import urllib.request\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "url = 'https://upload.wikimedia.org/wikipedia/commons/7/7b/R%C3%A9plica_de_la_piedra_movediza_de_Tandil.jpg'\n",
        "filename = 'movediza.jpg'\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = int(progress_size / (1024 * duration))\n",
        "    percent = int(count * block_size * 100 / total_size)\n",
        "    sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
        "                    (percent, progress_size / (1024 * 1024), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# como vamos a necesitar descargar más archivos posteriormente, definimos una función que lo haga\n",
        "\n",
        "def descargar_archivo(url,datapath):\n",
        "    if not os.path.exists(datapath): # antes de descargar el archivo controlamos que no exista\n",
        "       print(\"File does not exist\")\n",
        "  \n",
        "       print(\"Downloading file...\") \n",
        "       urllib.request.urlretrieve(url, datapath, reporthook)\n",
        "\n",
        "\n",
        "descargar_archivo(url, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqmEVM5xkad-"
      },
      "source": [
        "import cv2\n",
        "#Cargo una imagen.\n",
        "movediza = imread('movediza.jpg')\n",
        "movediza = cv2.cvtColor(movediza, cv2.COLOR_BGR2RGB)\n",
        "print(movediza.shape)\n",
        "plt.imshow(movediza)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smLiXHMxkaeF"
      },
      "source": [
        "#Cargo la matriz\n",
        "kirsch_matrix = [[5, 5, 5], \n",
        "                [-3, 0, -3],\n",
        "                [-3, -3, -3]]\n",
        "kirsch_matrix = np.asarray(kirsch_matrix, dtype=np.float32) \n",
        "#Inicializo el placeholder como una matriz con garbage\n",
        "movediza_k1 = np.empty((movediza.shape[0]-2, movediza.shape[1]-2, 3))\n",
        "#Convolucion por fila, por columna, \n",
        "for i in tqdm(range(0, movediza.shape[0]-2)):\n",
        "    for j in range(0, movediza.shape[1]-2):\n",
        "        #Trato los canales de forma independiente\n",
        "        for c in range(3): \n",
        "            # El operador * multiplica la matriz elemento a elemento y luego la\n",
        "            # reduzco con una suma\n",
        "            movediza_k1[i, j, c] = np.sum(movediza[i:i+3, j:j+3, c] * kirsch_matrix)\n",
        "# Normalizo la imagen para poder mostrarla\n",
        "movediza_k1 = (movediza_k1 - np.min(movediza_k1))/(np.max(movediza_k1) - np.min(movediza_k1))\n",
        "plt.imshow(movediza_k1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGdLLXcXkaeL"
      },
      "source": [
        "## ¡Ahora con keras!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuF1JvuZkaeN"
      },
      "source": [
        "# De entrada tengo una matriz de 3 dimensiones, las primeras dos\n",
        "# son de tamaño variable\n",
        "i = Input(shape=(None, None, 3)) \n",
        "# Agrego una capa convolucional de 2 dimensiones\n",
        "d = Conv2D(3, (3,3), activation='linear', use_bias=False)(i)\n",
        "# Creo el modelo\n",
        "kirsch = Model(inputs=i, outputs=d)\n",
        "kirsch.summary()\n",
        "# Compilo el modelo. No importa el loss ni optimizer.\n",
        "kirsch.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "print('Forma de los parámetros de la convolución: {}'.format(kirsch.layers[1].kernel.shape))\n",
        "# Cargo la matrix en un kernel apropiado\n",
        "kirsch_kernel = np.zeros((3, 3, 3, 3))\n",
        "for i in range(3):\n",
        "    kirsch_kernel[:, :, i, i] = kirsch_matrix\n",
        "# Utilizo el backend para cargar el kernel construido.\n",
        "K.set_value(kirsch.layers[1].kernel, kirsch_kernel)\n",
        "# Aplico la convolución.\n",
        "movediza_k2 = kirsch.predict(np.asarray([movediza]))[0]\n",
        "# Normalizo la imagen para poder mostrarla\n",
        "movediza_k2 = (movediza_k2 - np.min(movediza_k2))/(np.max(movediza_k2) - np.min(movediza_k2))\n",
        "plt.imshow(movediza_k2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJwdG9URkaeT"
      },
      "source": [
        "## Explicación del código\n",
        "Como primera etapa, creo la estructura de la CNN. En particular es de interes la creación de la capa convolucional 2D:\n",
        "\n",
        "```d = Conv2D(3, (3,3), activation='linear', use_bias=False)(i)```\n",
        "\n",
        "El primer parámetro indica cuantos canales tiene la imagen de salida. En este caso, como el objetivo es una imagen RGB la cantidad de canales es 3. El segundo parámetro indica que función de activación se aplica sobre la convolución. En este caso, se aplica la función lineal/identidad $f(x)=x$. Finalmente, indicamos que no se utilizará un parametro de bías, equivalente al $b$ en la regresión lineal o logística. \n",
        "\n",
        "Como segunda etapa, compilamos el modelo. Se especifican función de perdida y optimizador porque son obligatorios, pero solo son interesa compilar por lo que estos parámetros pueden ser cualquiera.\n",
        "\n",
        "```kirsch = Model(inputs=i, outputs=d)\n",
        "kirsch.summary()\n",
        "kirsch.compile(loss='categorical_crossentropy', optimizer='sgd')```\n",
        "\n",
        "A diferencia de la matriz original de 3x3, la matriz kernel de la capa convolucional es de 3x3x3x3. Esto se debe a que la operación de convolución en este caso es más generico. Las primeras dos dimensiones efectivamente se corresponde con las dimensiones de la convolución. La tercera dimensión se corresponde con la cantidad de canales de la imagen, ya que la convolución en este caso tambíen considera los canales. Finalmente, la cuarta dimensión se corresponde con la cantidad de canales de salida.\n",
        "\n",
        "```kirsch_kernel = np.zeros((3, 3, 3, 3))\n",
        "for i in range(3):\n",
        "    kirsch_kernel[:, :, i, i] = kirsch_matrix\n",
        "K.set_value(kirsch.layers[1].kernel, kirsch_kernel)```\n",
        "\n",
        "Como queremos que la convolución se aplice por cada canal, lo único que hay que hace es asignarle la matriz de convolución al canal correspondiente, dejando todos los otros valores en cero. Como resultado, se ignoran los valores en los otros canales, ya que se multiplican por cero (elemento absorbente para la multiplicación) y luego se suman (elemento neutro en la suma).\n",
        "\n",
        "## Intuiciones\n",
        "Como se mostró en el ejemplo anterior, los filtros/kernels convolucionales pueden aplicarse para detectar ciertas características de las imagenes como son los bordes. Además, estos kernels tiene relativamente pocos parámetros. En este caso, la cantidad de parámetros eran 81 (3x3x3x3). Si se aplicara una red densa sobre la imagen de entrada de 640x480x3 y se espera una salida de aproximadamente el mismo tamaño se requeririan 8.4924656e11 ((640x480x3)^2) parámetros.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvc6fLLAkaeU"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print('Cargando el dataset')\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "size = x_train.shape[1]*x_train.shape[2]\n",
        "x_train = np.expand_dims(x_train, axis=-1) / 255\n",
        "x_test = np.expand_dims(x_test, axis=-1) / 255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_train.shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXlVz2AnkaeZ"
      },
      "source": [
        "i = Input((x_train.shape[1:]))\n",
        "d = Conv2D(3, (3,3), activation='relu')(i)\n",
        "d = Conv2D(5, (5,5), activation='relu')(d)\n",
        "d = Conv2D(5, (3,3), activation='relu')(d)\n",
        "d = Conv2D(5, (3,3), activation='relu')(d)\n",
        "d = Flatten()(d)\n",
        "d = Dense(10, activation='softmax')(d)\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, to_categorical(y_train), epochs=10, \n",
        "          validation_data=(x_test, to_categorical(y_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E9RBQnakaee"
      },
      "source": [
        "## ¿Qué ven las capas?\n",
        "\n",
        "La idea qe que las capas más cercanas al input capturan caracteristicas simples, como bordes y gradientes de color, mientras que más abajo en la red capturan caracteristicas más complejas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-WVbkMLkaeh"
      },
      "source": [
        "idx = 6\n",
        "print('Número de ejemplo')\n",
        "ax = plt.subplot(111)\n",
        "ax.matshow(x_test[idx, :, :, 0], cmap='gray')\n",
        "ax.set_yticklabels([])\n",
        "ax.set_xticklabels([])\n",
        "plt.show()\n",
        "#Ponga su código para ver las capas intermedias de su modelo aquí"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNzLSpbmvUE2"
      },
      "source": [
        "max_depth = 3\n",
        "\n",
        "pred = model.layers[1](x_test[idx:idx+1, ...])\n",
        "for i in range(2, max_depth+1):\n",
        "    pred = model.layers[i](pred)\n",
        "pred = pred[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkD3ROVqvpeB"
      },
      "source": [
        "print(pred.shape)\n",
        "plt.matshow(x_test[idx, :,:,0])\n",
        "plt.matshow(pred.numpy()[:,:,0])\n",
        "plt.matshow(pred.numpy()[:,:,1])\n",
        "plt.matshow(pred.numpy()[:,:,2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN6Dn871kaen"
      },
      "source": [
        "### Pooling\n",
        "Pooling es una operación que reduce la salida de una red en una posición utilizando alguna función estádistica de la misma. Por ejemplo, *Max pooling* reemplaza una posición y sus vecinos por el máximo valor en el vecindario. Keras proporciona diversas capas de pooling.\n",
        "* `MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`: Obtine el maximo de un vecindario de tamaño `pool_size`, moviendose en la imagen utilizando los desplazamientos indicados en `strides` (si es `Nono` `strides=pool_size`).\n",
        "* `AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`: Obtine el maximo de un vecindario de tamaño `pool_size`, moviendose en la imagen utilizando los desplazamientos indicados en `strides` (si es `Nono` `strides=pool_size`).\n",
        "* `GlobalMaxPooling2D(data_format=None)`: Calcula el máximo por canal, transformando una entrada de forma `(batch_size, rows, cols, channels)` a una salida de forma `(batch_size, channels)`.\n",
        "* `GlobalAveragePooling2D(data_format=None)`: Calcula el promedio por canal, transformando una entrada de forma `(batch_size, rows, cols, channels)` a una salida de forma `(batch_size, channels)`.\n",
        "\n",
        "## Overfitting\n",
        "Overfitting es un fenómeno que ocurre cuando el modelo entrenado se ajusta tan bien a los datos de entrenamiento, que falla al generalizar para nuevos datos. Este es un problema muy común en las redes neuronales ya que estas tienen mucha capacidad para adaptarse a los datos de entrenamiento. En general, cuantos más parámetros entrenables tenga una red neuronal es más probable que sufra algún tipo de overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIa0Nz7jkaeo"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar100 \n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
        "\n",
        "#y_train = np.reshape(y_train, (y_train.shape[0]))\n",
        "#y_test = np.reshape(y_test, (y_test.shape[0]))\n",
        "\n",
        "\n",
        "print('100 primeros elementos del conjunto de entrenaimento')\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(x_train[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRyfjP1SbmDF"
      },
      "source": [
        "i = Input((x_train.shape[1:]))\n",
        "d = Conv2D(3, (3,3), activation='relu')(i)\n",
        "d = Conv2D(5, (5,5), activation='relu')(d)\n",
        "d = Conv2D(5, (3,3), activation='relu')(d)\n",
        "d = Conv2D(5, (3,3), activation='relu')(d)\n",
        "d = Flatten()(d)\n",
        "d = Dense(100, activation='softmax')(d)\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, \n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oByfsBbxkaew"
      },
      "source": [
        "i = Input((x_train.shape[1:]))\n",
        "d = Conv2D(32, (3,3), activation='relu')(i)\n",
        "d = Conv2D(32, (5,5), activation='relu')(d)\n",
        "d = Conv2D(64, (3,3), activation='relu')(d)\n",
        "d = Conv2D(128, (3,3), activation='relu')(d)\n",
        "d = Flatten()(d)\n",
        "d = Dense(100, activation='softmax')(d)\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, \n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToSPQjx9RYtN"
      },
      "source": [
        "model.layers[3].kernel.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFqodC0Ikae3"
      },
      "source": [
        "def inception(x, filters=16):\n",
        "    x_1_1 = Conv2D(filters, (1, 1), padding='same')(x)\n",
        "\n",
        "    x_3_3 = Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    x_3_3 = Conv2D(filters, (1, 1), padding='same')(x_3_3)\n",
        "    \n",
        "    x_5_5 = Conv2D(filters, (5, 5), padding='same')(x)\n",
        "    x_5_5 = Conv2D(filters, (1, 1), padding='same')(x_5_5)\n",
        "\n",
        "    m_3_3 = Conv2D(filters, (1, 1), padding='same')(x)\n",
        "    m_3_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(m_3_3)\n",
        "\n",
        "    x = Concatenate()([x_1_1, x_3_3, x_5_5, m_3_3])\n",
        "    #x = BatchNormalization(epsilon=1e-5)(x)\n",
        "    return Activation('relu')(x)\n",
        "\n",
        "\n",
        "i = Input((x_train.shape[1:]))\n",
        "d = GaussianDropout(0.2)(i)\n",
        "d = inception(d)\n",
        "d = inception(d)\n",
        "d = MaxPooling2D()(d)\n",
        "d = GaussianDropout(0.2)(d)\n",
        "d = inception(d, 32)\n",
        "d = Flatten()(d)\n",
        "d = Dense(100, activation='softmax')(d)\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, \n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75akaFmHknZP"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USWmjGwtu_Mt"
      },
      "source": [
        "def block1(x, filters, kernel_size=3, stride=1,\n",
        "           conv_shortcut=True, name=None):\n",
        "\n",
        "    bn_axis = 3 \n",
        "\n",
        "    if conv_shortcut is True:\n",
        "        shortcut = Conv2D(4 * filters, 1, strides=stride,\n",
        "                                 name=name + '_0_conv')(x)\n",
        "        shortcut = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                             name=name + '_0_bn')(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    x = Conv2D(filters, 1, strides=stride, name=name + '_1_conv')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_1_bn')(x)\n",
        "    x = Activation('relu', name=name + '_1_relu')(x)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size, padding='SAME',\n",
        "                      name=name + '_2_conv')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_2_bn')(x)\n",
        "    x = Activation('relu', name=name + '_2_relu')(x)\n",
        "\n",
        "    x = Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_3_bn')(x)\n",
        "\n",
        "    x = Add(name=name + '_add')([shortcut, x])\n",
        "    x = Activation('relu', name=name + '_out')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "i = Input((x_train.shape[1:]))\n",
        "d = block1(i, 32, name='block1')\n",
        "d = MaxPooling2D()(d)\n",
        "d = block1(d, 64, name='block2')\n",
        "d = MaxPooling2D()(d)\n",
        "d = block1(d, 128, name='block3')\n",
        "d = Flatten()(d)\n",
        "d = Dense(100, activation='softmax')(d)\n",
        "model = Model(i, d)\n",
        "model.summary()\n",
        "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, \n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qiRCQKDwSbx"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvdkMzWfyXY-"
      },
      "source": [
        "# Utilización de modelos preentrenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKcMmGKKyfzW"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16, decode_predictions, preprocess_input\n",
        "\n",
        "model = VGG16()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMrAaugS30mJ"
      },
      "source": [
        "model2 = VGG16(include_top=False)\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCohlZb94a5O"
      },
      "source": [
        "i = Input((None, None, 3))\n",
        "d = model2(i)\n",
        "fc1 = Conv2D(4096, (7,7), activation='relu')(d)\n",
        "fc2 = Conv2D(4096, (1,1), activation='relu')(fc1)\n",
        "p = Conv2D(1000, (1,1), activation='softmax')(fc2)\n",
        "\n",
        "m = Model(i, p)\n",
        "m.compile(loss='categorical_crossentropy', optimizer='nadam')\n",
        "\n",
        "K.set_value(m.layers[-3].kernel, np.reshape(K.get_value(model.layers[-3].kernel), m.layers[-3].kernel.shape))\n",
        "K.set_value(m.layers[-3].bias, np.reshape(K.get_value(model.layers[-3].bias), m.layers[-3].bias.shape))\n",
        "\n",
        "K.set_value(m.layers[-2].kernel, np.reshape(K.get_value(model.layers[-2].kernel), m.layers[-2].kernel.shape))\n",
        "K.set_value(m.layers[-2].bias, np.reshape(K.get_value(model.layers[-2].bias), m.layers[-2].bias.shape))\n",
        "\n",
        "K.set_value(m.layers[-1].kernel, np.reshape(K.get_value(model.layers[-1].kernel), m.layers[-1].kernel.shape))\n",
        "K.set_value(m.layers[-1].bias, np.reshape(K.get_value(model.layers[-1].bias), m.layers[-1].bias.shape))\n",
        "\n",
        "model = m\n",
        "del m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n19OP-tPWzEu"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBqVtjsh4lmY"
      },
      "source": [
        "minivan = 656 #705\n",
        "\n",
        "#url = 'https://www.autocar.co.uk/sites/autocar.co.uk/files/styles/gallery_slide/public/images/car-reviews/first-drives/legacy/large-2479-s-classsaloon.jpg?itok=QTxMln2k'\n",
        "#url = 'https://images.squarespace-cdn.com/content/v1/59cf0541d55b41b151ace075/1575580994414-FLMU5ZJEOB3I1OLY2VOZ/ke17ZwdGBToddI8pDm48kHoY9blY_oPV5Bzcqi3t-roUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcx5yuVS_0jJ0plzpb6sRSkaYELcEKb0geq5qayGSPLYpUQGlT0O5pGW3qo4jTaCvA/Cincinnati_Street_View_Image.jpg?format=2500w'\n",
        "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Anawrahta_road_traffic.jpg/1599px-Anawrahta_road_traffic.jpg'\n",
        "url = 'https://urbanfarmsuganda.files.wordpress.com/2020/03/traffic-jam.jpg'\n",
        "filename = 'cars.jpg'\n",
        "\n",
        "descargar_archivo(url, filename)\n",
        "\n",
        "cars = imread('cars.jpg')\n",
        "cars = cv2.cvtColor(cars, cv2.COLOR_BGR2RGB)\n",
        "print(cars.shape)\n",
        "plt.imshow(cars)\n",
        "plt.show()\n",
        "\n",
        "pred = model.predict(np.expand_dims(preprocess_input(cars), 0))[0, :, :, :]\n",
        "print(pred.shape)\n",
        "\n",
        "plt.imshow(pred[:, :, minivan])\n",
        "plt.show()\n",
        "plt.imshow((cars * np.expand_dims(cv2.resize(pred[:, :, minivan], cars.shape[0:2][::-1]), axis=-1)/np.max(pred[:, :, minivan])).astype(np.uint8))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSbWPq44ySTA"
      },
      "source": [
        "classes = [407, 436, 468, 511, 565, 609, 627, 656, 705, 751, 817]\n",
        "p_max = np.max(pred[:, :, classes], axis=-1)\n",
        "plt.imshow(p_max)\n",
        "plt.show()\n",
        "plt.imshow((cars * np.expand_dims(cv2.resize(p_max, cars.shape[0:2][::-1]), axis=-1)).astype(np.uint8))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hbNRj0Pcr7a"
      },
      "source": [
        "from skimage.transform import pyramid_gaussian\n",
        "from skimage import color\n",
        "from imutils.object_detection import non_max_suppression\n",
        "from collections import deque\n",
        "import imutils\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "\n",
        "descargar_archivo('https://users.exa.unicen.edu.ar/~jmrodri/car3b2.jpg', 'car3b2.jpg')\n",
        "model = VGG19()\n",
        "\n",
        "# Generador de ventana deslizante \n",
        "def sliding_window(image, stepSize, windowSize):\n",
        "    for y in range(0, image.shape[0], stepSize):\n",
        "        for x in range(0, image.shape[1], stepSize):\n",
        "            yield (x, y, image[y: y + windowSize[1], x:x + windowSize[0]])\n",
        "\n",
        "\n",
        "img= cv2.imread(\"car3b2.jpg\")\n",
        "\n",
        "preproc_img = preprocess_input(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "#Acá agrego los boxes detectados\n",
        "detections = []\n",
        "\n",
        " \n",
        "#Tamaño de las imagenes en el dataset de training\n",
        "windowSize=(224, 89)\n",
        "downscale=1.5\n",
        "\n",
        "for scale, resized in enumerate(pyramid_gaussian(preproc_img, multichannel=True, downscale=downscale)): \n",
        "    print('Scale: {}'.format(scale))\n",
        "    imgs = []\n",
        "    x_y = []\n",
        "    for (x,y,window) in sliding_window(resized, stepSize=10, windowSize=windowSize):\n",
        "        if window.shape[0] != windowSize[1] or window.shape[1] !=windowSize[0]:\n",
        "            continue\n",
        "        imgs.append(cv2.resize(window, (224, 224)))\n",
        "        x_y.append((x, y))\n",
        "    print('Predicting')\n",
        "    if len(imgs) == 0:\n",
        "        break\n",
        "    print(len(imgs))\n",
        "    imgs = np.asarray(imgs)\n",
        "    predictions = decode_predictions(model.predict(imgs, verbose=1, batch_size=100))\n",
        "    for i, (decoded, (x, y)) in enumerate(zip(predictions, x_y)):\n",
        "        if decoded[0][1] == 'minivan' and decoded[0][2] > .8:\n",
        "                '''cv2_imshow(imgs[i, ...])\n",
        "                print(\"Detection:: Location -> ({}, {})\".format(x, y))\n",
        "                print(\"Scale ->  {} | Confidence Score {} \\n\".format(scale,pred))'''\n",
        "                detections.append((int(x * (downscale**scale)), int(y * (downscale**scale)), [decoded[0][2]],\n",
        "                                   int(windowSize[0]*(downscale**scale)), \n",
        "                                      int(windowSize[1]*(downscale**scale))))  \n",
        "\n",
        "    \n",
        "\n",
        "for (x_tl, y_tl, scale, w, h) in detections:\n",
        "    cv2.rectangle(img, (x_tl, y_tl), (x_tl + w, y_tl + h), (0, 0, 255), thickness = 2)    \n",
        "    size = cv2.getTextSize('Prob: {:0.2f}%'.format(scale[0]*100),\n",
        "                           fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=1.5, \n",
        "                           thickness=1)[0]\n",
        "    cv2.rectangle(img, (x_tl + 5, y_tl + h - 5 - size[1]), \n",
        "                  (x_tl + 5 + size[0], y_tl + h - 2), (0, 0, 0), thickness = -1)\n",
        "    cv2.putText(img, text='Prob: {:0.2f}%'.format(scale[0]*100),\n",
        "                org=(x_tl+3, y_tl + h - 3),\n",
        "                fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=1.5, color=(255,255,255))\n",
        "rects = np.array([[x, y, x + w, y + h] for (x, y, _, w, h) in detections])\n",
        "sc = [score[0] for (x, y, score, w, h) in detections]\n",
        "print(\"detection confidence score: \", sc)\n",
        "sc = np.array(sc)\n",
        "pick = non_max_suppression(rects, probs = sc, overlapThresh = 0.1)\n",
        "\n",
        "#Bounding boxes seleccionados        \n",
        "for (xA, yA, xB, yB) in pick:\n",
        "    cv2.rectangle(img, (xA, yA), (xB, yB), (0,255,0), 2)\n",
        "cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PWyOR8Pe3ra"
      },
      "source": [
        "## Modelos para esto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZW-zUFBe2v3"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\" #@param [\"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\", \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"]\n",
        "\n",
        "detector = hub.load(module_handle).signatures['default']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GCq03rRfFw0"
      },
      "source": [
        "def load_img(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    return tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...], img\n",
        "\n",
        "\n",
        "result = detector(load_img('car3b2.jpg')[0])\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZAKvpJQgeCZ"
      },
      "source": [
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "import random\n",
        "\n",
        "def display_image(image):\n",
        "  fig = plt.figure(figsize=(20, 15))\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image)\n",
        "\n",
        "\n",
        "def download_and_resize_image(url, new_width=256, new_height=256,\n",
        "                              display=False):\n",
        "  _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "  response = urlopen(url)\n",
        "  image_data = response.read()\n",
        "  image_data = BytesIO(image_data)\n",
        "  pil_image = Image.open(image_data)\n",
        "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "  pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "  pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "  print(\"Image downloaded to %s.\" % filename)\n",
        "  if display:\n",
        "    display_image(pil_image)\n",
        "  return filename\n",
        "\n",
        "\n",
        "def draw_bounding_box_on_image(image,\n",
        "                               ymin,\n",
        "                               xmin,\n",
        "                               ymax,\n",
        "                               xmax,\n",
        "                               color,\n",
        "                               font,\n",
        "                               thickness=4,\n",
        "                               display_str_list=()):\n",
        "  \"\"\"Adds a bounding box to an image.\"\"\"\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)],\n",
        "            width=thickness,\n",
        "            fill=color)\n",
        "\n",
        "  # If the total height of the display strings added to the top of the bounding\n",
        "  # box exceeds the top of the image, stack the strings below the bounding box\n",
        "  # instead of above.\n",
        "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "  # Each display_str has a top and bottom margin of 0.05x.\n",
        "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "\n",
        "  if top > total_display_str_height:\n",
        "    text_bottom = top\n",
        "  else:\n",
        "    text_bottom = top + total_display_str_height\n",
        "  # Reverse list and print from bottom to top.\n",
        "  for display_str in display_str_list[::-1]:\n",
        "    text_width, text_height = font.getsize(display_str)\n",
        "    margin = np.ceil(0.05 * text_height)\n",
        "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "    draw.text((left + margin, text_bottom - text_height - margin),\n",
        "              display_str,\n",
        "              fill=\"black\",\n",
        "              font=font)\n",
        "    text_bottom -= text_height - 2 * margin\n",
        "\n",
        "\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
        "  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
        "  colors = list(ImageColor.colormap.values())\n",
        "  random.shuffle(colors)\n",
        "\n",
        "  try:\n",
        "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "  except IOError:\n",
        "    print(\"Font not found, using default font.\")\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "  for i in range(min(boxes.shape[0], max_boxes)):\n",
        "    if scores[i] >= min_score:\n",
        "      ymin, xmin, ymax, xmax = tuple(boxes[i])\n",
        "      display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),\n",
        "                                     int(100 * scores[i]))\n",
        "      color = colors[hash(class_names[i]) % len(colors)]\n",
        "      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "      draw_bounding_box_on_image(\n",
        "          image_pil,\n",
        "          ymin,\n",
        "          xmin,\n",
        "          ymax,\n",
        "          xmax,\n",
        "          color,\n",
        "          font,\n",
        "          display_str_list=[display_str])\n",
        "      np.copyto(image, np.array(image_pil))\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSqV2iEXfQOl"
      },
      "source": [
        "def run_detector(detector, path):\n",
        "    img, imgOrg = load_img(path)\n",
        "    start_time = time.time()\n",
        "    result = detector(img)\n",
        "    end_time = time.time()\n",
        "\n",
        "    result = {key:value.numpy() for key,value in result.items()} #Extraigo los valores de TF a numpy\n",
        "\n",
        "    print(\"%d Objetos encontrados.\" % len(result[\"detection_scores\"]))\n",
        "    print(\"Tiempo de la inferencia: \", end_time-start_time)\n",
        "\n",
        "    image_with_boxes = draw_boxes(\n",
        "        imgOrg.numpy(), result[\"detection_boxes\"],\n",
        "        result[\"detection_class_entities\"], result[\"detection_scores\"])\n",
        "\n",
        "    display_image(image_with_boxes)\n",
        "\n",
        "\n",
        "run_detector(detector, 'car3b2.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uwnpS9nyq7c"
      },
      "source": [
        "# Face recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u1WFiKAytNr"
      },
      "source": [
        "!pip install face_recognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmRu2Is8zqEp"
      },
      "source": [
        "!wget https://users.exa.unicen.edu.ar/~jmrodri/faces.zip\n",
        "!unzip faces.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1vh3AC_zBcY"
      },
      "source": [
        "import face_recognition as fr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def encodings(names):\n",
        "    enc = []\n",
        "    for n in names:\n",
        "        img = fr.load_image_file('{}.jpg'.format(n.lower()))\n",
        "        face = fr.face_encodings(img, fr.face_locations(img))\n",
        "        enc.append(face[0])\n",
        "    return enc\n",
        "\n",
        "names = ['Rachel', 'Ross', 'Monica', 'Phoebe','Joey']\n",
        "\n",
        "known = encodings(names)\n",
        "\n",
        "img = fr.load_image_file('ad.jpg')\n",
        "face_loc = fr.face_locations(img)\n",
        "face_enc = fr.face_encodings(img, face_loc)\n",
        "res = ''\n",
        "\n",
        "ad_names = []\n",
        "for enc in face_enc:\n",
        "    name = 'Unknown'\n",
        "    match = fr.compare_faces(known, enc, tolerance=0.6)\n",
        "    dist = fr.face_distance(known, enc)\n",
        "    idx = np.argmin(dist)\n",
        "    if match[idx]:\n",
        "        name = names[idx]\n",
        "    ad_names.append(name)\n",
        "\n",
        "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "for (top, right, bottom, left), name in zip(face_loc, ad_names):\n",
        "        cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "\n",
        "        cv2.rectangle(img, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
        "        font = cv2.FONT_HERSHEY_DUPLEX\n",
        "        cv2.putText(img, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
        "\n",
        "cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2bfCt1GfJl1"
      },
      "source": [
        "known"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}