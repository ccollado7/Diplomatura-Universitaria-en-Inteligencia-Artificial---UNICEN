{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrtaq0IvCHzI"
      },
      "source": [
        "# Aprendizaje por el gradiente\n",
        "\n",
        "En esta clase estudiaremos las bases del aprendizaje por el gradiente, técnica muy usada en las redes neuronales artificiales. El objetivo principal de esta notebook es ganar las intuiciones de cómo funcionan estas técnicas.  Para esto trabajaremos con conjuntos de datos simples y modelos simples como regresiones lineales y logísticas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr8ybt7MOrjo"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "'''Esta función dibuja bonita la matríz de confunsión.\n",
        "'''\n",
        "def show_confusion_matrix(cm, labels):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm)\n",
        "    plt.title('Matriz de confusión')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticklabels([''] + labels)\n",
        "    ax.set_yticklabels([''] + labels)\n",
        "    plt.xlabel('Predicho')\n",
        "    plt.ylabel('Verdadero')\n",
        "    for i, row in zip(range(len(cm)), cm):\n",
        "        for j, val in zip(range(len(row)), row):\n",
        "            ax.text(i, j, str(val), va='center', ha='center').set_backgroundcolor('white')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "mpl.rcParams['figure.figsize'] = [12.0, 8.0]\n",
        "print(mpl.rcParams['figure.figsize'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up8IXEg_C6sk"
      },
      "source": [
        "## Regresión Lineal\n",
        "La regresión lineal es la aplicación de un modelo lineal entre una variable dependiente ($y$) y una o más variables dependientes ($X$).\n",
        "\n",
        "$$\\hat{y}=x_0w_0+x_1w_1+...+x_iw_i+b$$\n",
        "\n",
        "donde, considerando un error $\\varepsilon$:\n",
        "\n",
        "$$y = \\hat{y}+\\varepsilon$$\n",
        "\n",
        "Siendo el caso de una variable:\n",
        "\n",
        "$$\\hat{y}=xw+b$$\n",
        "\n",
        "A continuación, se presenta un ejemplo basado en datos generados. Para este ejemplo se generarán 50 puntos con la siguiente distribución:\n",
        "\n",
        "$$y=3*x+(rand-0.5)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZtZGGnEYHXR"
      },
      "source": [
        "def gen_random_data(mult):\n",
        "    _x = np.linspace(-1, 1, 100)\n",
        "    _error = (np.random.rand(*_x.shape) - .5)\n",
        "    _y = _x * mult + _error\n",
        "    return _x, _y\n",
        "\n",
        "\n",
        "x, y = gen_random_data(3)\n",
        "plt.plot(x, y, 'ro')\n",
        "plt.show()\n",
        "print('x: {}'.format(x))\n",
        "print('y: {}'.format(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtTYT5kVYZR2"
      },
      "source": [
        "def lineal(x, w, b):\n",
        "    return x*w+b\n",
        "\n",
        "plt.plot(x, y, 'ro', x, lineal(x, 3, 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iozl-xMgDHBU"
      },
      "source": [
        "## Objetivo\n",
        "Considerando la varaible independiente $x$ y la variable dependiente $y$, el objetivo de un regresión lineal es encontrar $w$ y $b$ tal que dada una función de error $E(y, \\hat{y})$ sea mínimo. Es decir:\n",
        "\n",
        "$$\\underset{w,b}{arg\\,min}=E(y,xw+b)$$\n",
        "\n",
        "## Función de error\n",
        "Una función de error utilizada para este tipo de problemas es el error medio cuadrático (_mean squared error_), que se define como:\n",
        "\n",
        "$$MSE(y,\\hat{y})=\\frac{1}{N}\\sum(y-\\hat{y})^{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv5oFTnDYHqX"
      },
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true-y_pred)**2) \n",
        "\n",
        "print('El MSE es {}'.format(mse(y, lineal(x, 3, 0))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGH4saAMDgQP"
      },
      "source": [
        "Para encontrar los parámetros que cumplen con el objetivo de $\\underset{w,b}{arg\\,min}=E(y,xw+b)$ podríamos explorar extensivamente posibles valores de los parámetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQaUzF3aZ8Vl"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "# Construyendo datos\n",
        "w = np.arange(1, 5, 0.1)\n",
        "b = np.arange(-1, 1, 0.01)\n",
        "w, b = np.meshgrid(w, b)\n",
        "e = np.empty_like(w)\n",
        "for i in range(w.shape[0]):\n",
        "    for j in range(w.shape[1]):\n",
        "        e[i, j] = mse(y, lineal(x, w[i, j], b[i, j]))\n",
        "\n",
        "\n",
        "ax.plot_surface(w, b, e, cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "\n",
        "ax.set_xlabel('Valores de w')\n",
        "ax.set_ylabel('Valores de b')\n",
        "ax.set_zlabel('Función de perdida')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1XaafGjETsi"
      },
      "source": [
        "Si bien esta técnica podría servir para encontrar los parámetros no es escalable. Es decir, si hay más parámetros no se podría encontrar los mejores valores en un tiempo razonable. Es importante notar que muchas de las redes neuronales modernas tienen miles o millones de parámetros.\n",
        "\n",
        "## Optimización de parámetros\n",
        "A continuación, analizaremos el problema de optimización de parámetros. Para simplificar el problema, solo trabajaremos con el parámetro $w$ y fijaremos $b=0$. Podemos hacer esto, porque para el conjunto de datos artificial, conocemos la parametrización optima.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxa1RwO5Z8ZZ"
      },
      "source": [
        "def exp_error(y, x, ws):\n",
        "    def single_error(w):\n",
        "        return mse(y, lineal(x, w, 0))\n",
        "    _s = np.vectorize(single_error)\n",
        "    return _s(ws)\n",
        "\n",
        "ws = np.linspace(1, 5, 51)\n",
        "plt.plot(ws, exp_error(y, x, ws))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKjBSch3FxBY"
      },
      "source": [
        "## Pendiente del error\n",
        "Dado que la función de error tiene un solo mínimo, se podrían tomar 2 valores cercanos de manera de conocer en qué dirección es conveniente explorar. La función lineal en realidad es una función que depende no solo de los datos $x$, sino que también del parámetro a aprender $w$, entonces la notaremos como $h(x,w)$. Para conocer la pendiente de la función de error dado el parámetro a conocer debemos hacer:\n",
        "\n",
        "$$pendiente_w(w_{1}, w_{0})=\\frac{MSE(y,h(x,w_{1}))-MSE(y,h(x,w_{0}))}{w_{1}-w_{0}}$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_lOchc9Z8cS"
      },
      "source": [
        "errors = exp_error(y, x, ws)\n",
        "pendiente = (errors[10]-errors[20])/(ws[10]-ws[20])\n",
        "correccion_ordenada_origen = -pendiente*ws[10] + errors[10]\n",
        "plt.plot(ws, errors, ws[10:21], lineal(pendiente, ws[10:21], 0)+correccion_ordenada_origen)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAsMvvC8GtBD"
      },
      "source": [
        "Entonces, se podría inicializar $w$ de forma aleatoria e ir actualizando el valor en contra de la pendiente, ya que la pendiente indica la dirección en la que crece el error con respecto a $w$. El problema es determinar cuanto moverse. En una técnica básica es moverse proporcionalmente al la “inclinación” de la pendiente.\n",
        "\n",
        "$$w_{n+1}=w_n – lr*pendiente_w $$\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLh0RNbsZ8iv"
      },
      "source": [
        "def pendiente(y_true, x, w, delta=1e-6):\n",
        "    return (mse(y_true, lineal(x, w + delta, 0))-mse(y_true, lineal(x, w, 0))) / delta\n",
        "\n",
        "w = 0 #Podría ser cualquier valor\n",
        "ciclos = 100\n",
        "lr = 0.1 \n",
        "errors = []\n",
        "ws = []\n",
        "for i in range(ciclos):\n",
        "    p = pendiente(y, x, w)\n",
        "    w = w - lr * p\n",
        "    ws.append(w)\n",
        "    errors.append(mse(y, lineal(x, w, 0)))\n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print(ws)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSjtWwk2Gx-Z"
      },
      "source": [
        "### ¿Funcionaría si tenemos más parámetros? ¿Cómo?\n",
        "Podríamos considerar los parámetros independientemente, es decir, para calcular la pendiente de $w$ fijamos $b$ en el valor arbitrario y viceversa. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAEUn0qeS8W"
      },
      "source": [
        "def pendiente(y_true, x, w, b, delta=1e-6):\n",
        "    pw = (mse(y_true, lineal(x, w + delta, b))-mse(y_true, lineal(x, w, b))) / delta\n",
        "    pb = (mse(y_true, lineal(x, w, b + delta))-mse(y_true, lineal(x, w, b))) / delta\n",
        "    return pw, pb\n",
        "\n",
        "w = 0 #Podría ser cualquier valor\n",
        "b = 5\n",
        "ciclos = 100\n",
        "lr = 0.1 \n",
        "errors = []\n",
        "for i in range(ciclos):\n",
        "    pw, pb = pendiente(y, x, w, b)\n",
        "    errors.append(mse(y, lineal(x, w, b)))\n",
        "    w = w - lr * pw\n",
        "    b = b - lr * pb\n",
        "    #Actualicé el valor de b\n",
        "print('Errores a medida que se actualiza el valor de w y b')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print('El b final es {}'.format(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSED-yx-Hrl-"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "El problema de utilizar la pendiente es que nos agrega muchos cálculos y el hiper-parámetros $\\delta$. Pero, como queremos que el $\\delta \\rightarrow 0$, podríamos tomar el límite con $\\delta$ tendiendo a $0$.\n",
        "\n",
        "### Pendiente de $w$\n",
        "Desde el punto de vista de la pendiente de $w$, si definimos a $w_{1}=w_{0}+\\Delta$ entonces:\n",
        "\n",
        "$$\\lim_{\\Delta \\rightarrow 0} pendiente_w(w_{0}+\\Delta,w_{0})= \\lim_{\\Delta \\rightarrow 0} \\frac{MSE(y,h(x,w_{0}+\\Delta, b))-MSE(y,h(x,w_{0}, b))}{\\Delta} =\\frac{dMSE(y,h(x,w,b))}{dw}$$\n",
        "\n",
        "Esta derivada se puede resolver por regla de la cadena:\n",
        "\n",
        "$$\\frac{dMSE(y,h(x,w,b))}{dw}=\\frac{dMSE(Y,h(x,w,b))}{d(h(x,w,b))}.\\frac{(h(x,w,b))}{dw}$$\n",
        "\n",
        "La primer derivada se resuelve, devuelta por regla de la cadena:\n",
        "\n",
        "$$\\frac{dMSE(y,h(x,w,b))}{d(h(x,w,b)}=\\frac{d(\\frac{1}{N}\\sum(y-h(x,w,b))^{2}}{d(h(x,w,b))}=-\\frac{2}{N}\\sum(y-h(x,w,b))$$\n",
        "\n",
        "La segunda derivada se resuelve así:\n",
        "\n",
        "$$\\frac{dh(x,w,b)}{dw}=\\frac{d(xw+b)}{dw}=x$$\n",
        "\n",
        "Finalmente, resulta en:\n",
        "\n",
        "$$\\frac{dMSE(y,h(x,w,b))}{dw}=\\frac{dMSE(Y,h(x,w,b))}{d(h(x,w,b))}.\\frac{(h(x,w,b))}{dw}=\\frac{-2\\sum(y-(xw+b))*x}{N}$$\n",
        "\n",
        "### Pendiente $b$\n",
        "Desde el punto de vista de la pendiente de $b$, si definimos a $b_{1}=b_{0}+\\Delta$ entonces:\n",
        "\n",
        "$$\\lim_{\\Delta \\rightarrow 0} pendiente_b(b_{0}+\\Delta,b_{0})= \\lim_{\\Delta \\rightarrow 0} \\frac{MSE(y,h(x,w, b_{0}+\\Delta))-MSE(y,h(x,w,b_{0}))}{\\Delta} =\\frac{dMSE(y,h(x,w,b))}{db}$$\n",
        "\n",
        "Esta derivada se puede resolver por regla de la cadena:\n",
        "\n",
        "$$\\frac{dMSE(y,h(x,w,b))}{db}=\\frac{dMSE(Y,h(x,w,b))}{d(h(x,w,b))}.\\frac{(h(x,w,b))}{db}$$\n",
        "\n",
        "La primer derivada ya se resolvió arriba.\n",
        "\n",
        "La segunda derivada se resuelve así:\n",
        "\n",
        "$$\\frac{dh(x,w,b)}{db}=\\frac{d(xw+b)}{db}=1$$\n",
        "\n",
        "Finalmente, resulta en:\n",
        "\n",
        "$$\\frac{dMSE(y,h(x,w,b))}{db}=\\frac{dMSE(Y,h(x,w,b))}{d(h(x,w,b))}.\\frac{(h(x,w,b))}{db}=\\frac{-2\\sum(y-(xw+b))*1}{N}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-q9MEp36s_E"
      },
      "source": [
        "def gradiente(y_true, x, w, b):\n",
        "    dp = y_true - lineal(x, w, b)\n",
        "    gw = -2 * np.mean(dp * x)\n",
        "    gb = -2 * np.mean(dp)\n",
        "    return gw, gb\n",
        "               \n",
        "def gradient_check(x, y_true, ciclos=100, delta=1e-6, error=1e-5):\n",
        "    ok = True\n",
        "    for i in range(ciclos):\n",
        "        w = random.uniform(-50, 50) #genera un flotante aleatorio\n",
        "        b = random.uniform(-50, 50) #genera un flotante aleatorio\n",
        "        pw, pb = pendiente(y_true, x, w, b, delta)\n",
        "        gw, gb = gradiente(y_true, x, w, b)\n",
        "        if abs(pw - gw) > error or abs(pb - gb) > error:\n",
        "            print('Error para w={}, b={}. pw={}, pb={}, gw={},gb={}'.format(w, b, pw, pb, gw, gb))\n",
        "            print(pw-gw)\n",
        "            print(pb-gb)\n",
        "            ok = False\n",
        "    if ok:\n",
        "        print('No hubo errores en la prueba')\n",
        "        \n",
        "gradient_check(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6xT-jPHYjrg"
      },
      "source": [
        "w = random.uniform(-50, 50) #genera un flotante aleatorio\n",
        "b = random.uniform(-50, 50) #genera un flotante aleatorio\n",
        "print('Valores iniciales. w={} b={}'.format(w, b))\n",
        "ciclos = 100\n",
        "lr = 0.1\n",
        "errors = []\n",
        "for i in range(ciclos):\n",
        "    gw, gb = gradiente(y, x, w, b)\n",
        "    errors.append(mse(y, lineal(x, w, b)))\n",
        "    w = w - lr * gw\n",
        "    b = b - lr * gb\n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print('El b final es {}'.format(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g1eL3esIrhH"
      },
      "source": [
        "## Tensorflow\n",
        "\n",
        "Como esta es la base de las técnicas para entrenar muchos modelos de aprendizaje de máquina, existen librerías que nos asisten a implementar este tipo de algoritmos. Tensorflow es una de estas librerías que puede calcular gradientes de forma automática, permite aceleración utilizando GPU o TPU, y nos brinda diversas abstracciones de más alto nivel que facilitan la implementación de redes neuronales.\n",
        "\n",
        "A continuación, veremos una implementación del modelo utilizado para analizar el gradient descent utilizando Tensorflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQTy3c3e7gs5"
      },
      "source": [
        "def tf_lineal(x, w, b):\n",
        "    return x * w + b\n",
        "\n",
        "def tf_mse(y_true, y_pred):\n",
        "    return tf.math.reduce_mean((y_true-y_pred)**2) \n",
        "\n",
        "def tf_cost(y_true, x, w, b):\n",
        "    return tf_mse(y_true, tf_lineal(x, w, b))\n",
        "\n",
        "\n",
        "#Tipos de datos esperados por tf\n",
        "x = x.astype(np.float32)\n",
        "y = y.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axfVmgo38Wn7"
      },
      "source": [
        "w = tf.random.uniform(shape=[], minval=-1, maxval=1)\n",
        "b = tf.random.uniform(shape=[], minval=-1, maxval=1)\n",
        "ciclos = 100\n",
        "lr = 0.1 \n",
        "errors = []\n",
        "for i in range(ciclos):\n",
        "    with tf.GradientTape() as g:\n",
        "        g.watch([w, b])\n",
        "        loss = tf_cost(y, x, w, b)\n",
        "        errors.append(loss.numpy())\n",
        "    gw, gb = g.gradient(loss, [w, b])\n",
        "    w = w - lr * gw\n",
        "    b = b = - lr * gb\n",
        "\n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print('El b final es {}'.format(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tMgpwBsJqwc"
      },
      "source": [
        "## Clasificación con regresión logística.\n",
        "La regresión logística es un modelo que permite clasificar instancias en 2 clases. Por este motivo, imagen de esta función está en $(0, 1)$.\n",
        "\n",
        "$$P(y|x)= sigmoid(xw + b)$$\n",
        "\n",
        "En este contexto, la función seleccionada para hacer esta estimación por excelencia es la sigmoide.\n",
        "\n",
        "$$sigmoid(z)=\\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "### Función de error\n",
        "Para calcular el error, se utiliza la entropía cruzada entre el valor esperado y el valor obtenido.\n",
        "$$CE(y,\\hat{y})=\\frac{\\sum(-y*log(\\hat{y})-(1-y)*log(1-\\hat{y}))}{N}$$\n",
        "En este contexto, la entropía curzada se interpreta como la información promedio (en bits) necesaria para determinar el valor de $y$ dado que se conoce el valor de $\\hat{y}$.\n",
        "\n",
        "### Ejemplo\n",
        "Para el ejemplo de regresión logística se utilizará el conjunto de datos de cancer de pecho provisto. Este conjuntos de datos fue recolectado por investigadores de la Universisda de Wisconsin y provisto por la [UCI](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). Para acceder al conjunto de datos, no es necesario descargarlo y convertirlo al formato, ya que en encuentra provisto por el módulo de [_sklearn.datasets_](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) de la librería sickit-learn, que es una librería de _machine learning_ que se utilizará durante el curso.\n",
        "\n",
        "El dataset tiene 569 instancias, con 30 atributos cada una. Las instancias pueden ser clasificadas entre Malignos y Benignos. El dataset está ligeramente desbalanceado, lo que significa que existen más instancias de una clase que de la otra. En partícular, 37,25% de las instancias son Malignas y 62,75% son Benignas. La siguiente tabla resume el conjunto de datos:\n",
        "\n",
        "| Propiedad | Valor |\n",
        "| --- | --- |\n",
        "| Clases | 2 |\n",
        "| Ejemplos por clase | 212(M-0), 357(B-1) | \n",
        "| Total de instancias | 569 |\n",
        "| Dimensionalidad | 30|\n",
        "\n",
        "El siguiente código:\n",
        "1. Levanta los datos divididos en `x` (atributos) e `y` (clase).\n",
        "1. Divide los datos en entrenamiento y testing.\n",
        "1. Escala los datos de entrenamiento a valores entre 0 y 1.\n",
        "1. Aplica las correcciones de escalado al conjunto de testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcRgiEvSPOPD"
      },
      "source": [
        "x, y = load_breast_cancer(True)\n",
        "x_train = x[:500,:]\n",
        "y_train = y[:500]\n",
        "x_test = x[500:,:]\n",
        "y_test = y[500:]\n",
        "\n",
        "maxs = np.max(x_train, axis=0)\n",
        "mins = np.min(x_train, axis=0)\n",
        "x_train = (x_train - mins) / (maxs - mins)\n",
        "x_test = (x_test - mins) / (maxs - mins)\n",
        "\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws5wNvZxPQsw"
      },
      "source": [
        "ts_rep = TSNE().fit_transform(x_train)\n",
        "for point, label in zip(ts_rep, y_train):\n",
        "    rep = 'b*' if label == 1 else 'r*'\n",
        "    plt.plot([point[0]], [point[1]], rep)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ych6eGPZg_"
      },
      "source": [
        "def logisticRegresion(x, w, b):\n",
        "    return 1/(1+tf.exp(-(tf.matmul(x, w) + b)))[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBM2iXdEPwVY"
      },
      "source": [
        "w = tf.random.uniform(shape=[30, 1]) - 0.5\n",
        "b = tf.random.uniform(shape=[]) - 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8mpJ6KPQWxf"
      },
      "source": [
        "def crossentropy(yt, yp):\n",
        "    return tf.math.reduce_mean(-yt*tf.math.log(tf.clip_by_value(yp, 1e-6, 1)) - (1-yt)*tf.math.log(tf.clip_by_value(1-yp, 1e-6, 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOFLcCbCQEK7"
      },
      "source": [
        "crossentropy(y_train, logisticRegresion(x_train, w, b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXpZQ844o9hz"
      },
      "source": [
        "v = logisticRegresion(x_train, w, b).numpy()\n",
        "print(np.min(v))\n",
        "print(np.max(v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vxvfIFjldIW"
      },
      "source": [
        "w = tf.random.uniform(shape=[30, 1], minval=-1, maxval=1)\n",
        "b = tf.random.uniform(shape=[], minval=-1, maxval=1)\n",
        "ciclos = 100\n",
        "lr = 0.1 \n",
        "errors = []\n",
        "for i in range(ciclos):\n",
        "    with tf.GradientTape() as g:\n",
        "        g.watch([w, b])\n",
        "        loss = crossentropy(y_train, logisticRegresion(x_train, w, b))\n",
        "        errors.append(loss.numpy())\n",
        "    gw, gb = g.gradient(loss, [w, b])\n",
        "    w = w - lr * gw\n",
        "    b = b - lr * gb\n",
        "\n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print('El b final es {}'.format(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWyBgw-6qns6"
      },
      "source": [
        "y_pred = logisticRegresion(x_test, w, b).numpy()\n",
        "show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), ['Maligno', 'Benigno'])\n",
        "print(classification_report(y_test, y_pred > 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZkR3cbXLLCV"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "Hasta el momento calculamos el gradiente sobre todo el conjunto de datos de entrenamiento. Sin embargo, esto puede no ser posible o eficiente cuando tenemos un conjunto de datos grande. Por esto, se suele dividir el conjunto de datos en mini-batch y entrenar sobre estos mini-batchs. La idea, es que en promedio la agregación de los efectos de la actualización en cada mini-batch nos acerque al mínimo. Obviamente, esto no significa que cada actualización nos acerque al mínimo global.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccKCnQqQrJtz"
      },
      "source": [
        "w = tf.random.uniform(shape=[30, 1], minval=-1, maxval=1)\n",
        "b = tf.random.uniform(shape=[], minval=-1, maxval=1)\n",
        "ciclos = 100\n",
        "lr = 0.1 \n",
        "errors = []\n",
        "errors_minibatch = []\n",
        "errors_minibatch2 = []\n",
        "for i in range(ciclos):\n",
        "    x_s, y_s = shuffle(x_train, y_train)\n",
        "    for mini_batch in range(0, 500, 50):\n",
        "        with tf.GradientTape() as g:\n",
        "            g.watch([w, b])\n",
        "            loss = crossentropy(y_s[mini_batch:mini_batch+50], logisticRegresion(x_s[mini_batch:mini_batch+50], w, b))\n",
        "            errors_minibatch2.append(loss.numpy())\n",
        "            errors_minibatch.append(crossentropy(y_train, logisticRegresion(x_train, w, b)).numpy())\n",
        "        gw, gb = g.gradient(loss, [w, b])\n",
        "        w = w - lr * gw\n",
        "        b = b - lr * gb\n",
        "    errors.append(crossentropy(y_train, logisticRegresion(x_train, w, b)).numpy())\n",
        "\n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "plt.plot(errors_minibatch)\n",
        "plt.show()\n",
        "plt.plot(errors_minibatch2)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print('El b final es {}'.format(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxgSZJXwwzkd"
      },
      "source": [
        "y_pred = logisticRegresion(x_test, w, b).numpy()\n",
        "show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), ['Maligno', 'Benigno'])\n",
        "print(classification_report(y_test, y_pred > 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78BIP_o5MEQg"
      },
      "source": [
        "## Momentum\n",
        "Para mejorar los resultados una técnica muy utilizada es agregar un momentum. La idea del momentum es que vaya llevando una historia del movimiento entre los mini-batchs, y como en promedio esperamos que nos lleve a un mínimo global el momentum llevaría nuestras actualizaciones más hacia el este mínimo.\n",
        "\n",
        "$$vel_n=momentum * vel_{n-1} – lr * grad_n$$\n",
        "\n",
        "$$w_{n+1} = w_{n} + vel_n$$ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBT5YbeHtgXA"
      },
      "source": [
        "w = tf.random.uniform(shape=[30, 1], minval=-1, maxval=1)\n",
        "b = tf.random.uniform(shape=[], minval=-1, maxval=1)\n",
        "ciclos = 100\n",
        "lr = 0.1 \n",
        "momentum = 0.9\n",
        "errors = []\n",
        "errors_minibatch = []\n",
        "errors_minibatch2 = []\n",
        "vw = tf.zeros(shape=[30, 1])\n",
        "vb = tf.zeros(shape=[])\n",
        "for i in range(ciclos):\n",
        "    x_s, y_s = shuffle(x_train, y_train)\n",
        "    for mini_batch in range(0, 500, 50):\n",
        "        with tf.GradientTape() as g:\n",
        "            g.watch([w, b])\n",
        "            loss = crossentropy(y_s[mini_batch:mini_batch+50], logisticRegresion(x_s[mini_batch:mini_batch+50], w, b))\n",
        "            errors_minibatch.append(crossentropy(y_train, logisticRegresion(x_train, w, b)).numpy())\n",
        "            errors_minibatch2.append(loss.numpy())\n",
        "        gw, gb = g.gradient(loss, [w, b])\n",
        "        vw = momentum * vw - lr * gw\n",
        "        vb = momentum * vb - lr * gb\n",
        "        w = w + vw\n",
        "        b = b + vb\n",
        "    errors.append(crossentropy(y_train, logisticRegresion(x_train, w, b)).numpy())\n",
        "\n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "plt.plot(errors_minibatch)\n",
        "plt.show()\n",
        "plt.plot(errors_minibatch2)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print('El b final es {}'.format(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUyw3id2uC7T"
      },
      "source": [
        "y_pred = logisticRegresion(x_test, w, b).numpy()\n",
        "show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), ['Maligno', 'Benigno'])\n",
        "print(classification_report(y_test, y_pred > 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dv0ykpaNSXV"
      },
      "source": [
        "## Clasificación multiclase\n",
        "Para la clasificación multiclase una función muy utilizada es el softmax. Esta función toma por entrada un vector y retorna un vectos tal que la suma de todos sus elementos es $1$ y todos los valores están entre $0$ y $1$. Si nuestro problema de clasificación tiene n clases, podemos usar la función softmax y un vector n-dimensiones. Entonces, podemos interpretar la salida de esta función como la distribución de probabilidades de las clases.\n",
        "$$softmax_i(x) = \\frac{e^{x}}{\\sum e^{x}} $$\n",
        "\n",
        "## Categorical Crossentropy\n",
        "Esta función de error considera el error sobre la categoría real, normalizando el valor de la predicción. Considerando $\\hat{y}=(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_C)$ in vector de valores asociados a las clases\n",
        "$$P_\\hat{y}=\\frac{\\hat{y}}{\\sum\\hat{y}_i}$$\n",
        "$$CCE(y,P_\\hat{y})=-\\frac{\\sum y * log(P_\\hat{y})}{N} $$\n",
        "Notese que el valor de error se considera solo sobre las clases verdaderas, las otras son afectadas a través de la normalización de la salida $\\hat{y}$.\n",
        "\n",
        "## Problema de OCR de dígitos\n",
        "Para este trabajos utilizaremos el conjunto de datos conocido como [MNIST](http://yann.lecun.com/exdb/mnist/). Este conjunto de datos ya se encuentra dividido entre entrenamiento y testing. El problema consiste en clasificar imagenes de dígistos escritos a mano al dígito correspondiente.\n",
        "\n",
        "| Propiedad | Valor |\n",
        "| --- | --- |\n",
        "| Clases | 10 |\n",
        "| Tamaño de las imagenes | 28 X 28 |\n",
        "| Instancias de entrenemiento | 60.000 |\n",
        "| Instancias de entrenemiento | 10.000 |\n",
        "| Valor mínimo de cada pixel | 0 |\n",
        "| Valor máximo de cada pixel | 255 |\n",
        "\n",
        "A continuación se carga el dataset y se dibujan los primeros 100 ejemplos del conjunto de entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl65e8AEXf42"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('100 primeros elementos del conjunto de entrenaimento')\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(x_train[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS-nY44bX2u4"
      },
      "source": [
        "size = x_train.shape[1]*x_train.shape[2]\n",
        "x_train = x_train.reshape((x_train.shape[0], size)) / 255\n",
        "x_test = x_test.reshape((x_test.shape[0], size)) / 255\n",
        "\n",
        "yc_train, yc_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)\n",
        "yc_train = yc_train.astype(np.float32)\n",
        "yc_test = yc_test.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSx6AtmuX4qj"
      },
      "source": [
        "def softmax(z):\n",
        "    exp = tf.exp(z)\n",
        "    return exp / tf.expand_dims(tf.math.reduce_sum(exp, axis=1), axis=-1)\n",
        "\n",
        "def predict(x, w, b):\n",
        "    return softmax(tf.matmul(x, w) + b)\n",
        "\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    return tf.math.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(tf.clip_by_value(y_pred, 1e-6, 1)), axis=1))\n",
        "\n",
        "def loss_f(y_true, x, w, b):\n",
        "    return categorical_crossentropy(y_true, predict(x, w, b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCoG4uaNzRhP"
      },
      "source": [
        "w = tf.random.uniform(shape=[size, 10], minval=-1, maxval=1)\n",
        "b = tf.random.uniform(shape=[10], minval=-1, maxval=1)\n",
        "ciclos = 100\n",
        "lr = 0.01 \n",
        "momentum = 0.9\n",
        "mini_batch_size = 500\n",
        "errors = []\n",
        "errors_minibatch = []\n",
        "vw = tf.zeros(shape=[size, 10])\n",
        "vb = tf.zeros(shape=[10])\n",
        "for i in tqdm(range(ciclos)):\n",
        "    x_s, y_s = shuffle(x_train, yc_train)\n",
        "    for mini_batch in range(0, x_s.shape[0], mini_batch_size):\n",
        "        with tf.GradientTape() as g:\n",
        "            g.watch([w, b])\n",
        "            loss = loss_f(y_s[mini_batch:mini_batch+mini_batch_size], x_s[mini_batch:mini_batch+mini_batch_size], w, b)\n",
        "            errors_minibatch.append(loss.numpy())\n",
        "        gw, gb = g.gradient(loss, [w, b])\n",
        "        vw = momentum * vw - lr * gw\n",
        "        vb = momentum * vb - lr * gb\n",
        "        w = w + vw\n",
        "        b = b + vb\n",
        "    errors.append(loss_f(yc_train, x_train, w, b).numpy())\n",
        "    \n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "plt.plot(errors_minibatch)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print('El b final es {}'.format(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLY_K9Vtz5_7"
      },
      "source": [
        "y_pred = predict(x_test, w, b).numpy()\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "show_confusion_matrix(confusion_matrix(y_test, y_pred), list(map(str, range(10))))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWX6dNrW2whx"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "array = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(array, index = [str(i) for i in range(10)],\n",
        "                  columns = [str(i) for i in range(10)])\n",
        "\n",
        "sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFlRe4xjPMas"
      },
      "source": [
        "# Optimización con variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xavgM08l1vvk"
      },
      "source": [
        "w = tf.Variable(tf.random.uniform(shape=[size, 10], minval=-1, maxval=1))\n",
        "b = tf.Variable(tf.random.uniform(shape=[10], minval=-1, maxval=1))\n",
        "ciclos = 100\n",
        "lr = 0.01 \n",
        "momentum = 0.9\n",
        "mini_batch_size = 500\n",
        "errors = []\n",
        "errors_minibatch = []\n",
        "vw = tf.Variable(tf.zeros(shape=[size, 10]))\n",
        "vb = tf.Variable(tf.zeros(shape=[10]))\n",
        "for i in tqdm(range(ciclos)):\n",
        "    x_s, y_s = shuffle(x_train, yc_train)\n",
        "    for mini_batch in range(0, x_s.shape[0], mini_batch_size):\n",
        "        with tf.GradientTape() as g:\n",
        "            g.watch([w, b])\n",
        "            loss = loss_f(y_s[mini_batch:mini_batch+mini_batch_size], x_s[mini_batch:mini_batch+mini_batch_size], w, b)\n",
        "            errors_minibatch.append(loss.numpy())\n",
        "        gw, gb = g.gradient(loss, [w, b])\n",
        "        vw.assign(momentum * vw - lr * gw) \n",
        "        vb.assign(momentum * vb - lr * gb)\n",
        "        w.assign(w + vw)\n",
        "        b.assign(b + vb)\n",
        "    errors.append(loss_f(yc_train, x_train, w, b).numpy())\n",
        "    \n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n",
        "plt.plot(errors_minibatch)\n",
        "plt.show()\n",
        "print('El w final es {}'.format(w))\n",
        "print('El b final es {}'.format(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKSeQNOx2Udh"
      },
      "source": [
        "y_pred = predict(x_test, w, b).numpy()\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "show_confusion_matrix(confusion_matrix(y_test, y_pred), list(map(str, range(10))))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAhISMxb2ed-"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Activation\n",
        "x = np.linspace(-10, 10, 100).astype(np.float32)\n",
        "for f in ['relu','selu', 'tanh', 'softplus', 'softsign', 'sigmoid', 'tanh', 'softsign']:\n",
        "    print(f)\n",
        "    v = Activation(f)(x).numpy()\n",
        "    plt.ylabel(f)\n",
        "    plt.plot(x, v)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}