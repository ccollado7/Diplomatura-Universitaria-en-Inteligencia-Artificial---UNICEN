{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcp5NoQRHoH2"
      },
      "source": [
        "# Redes Neuronales Artificiales\n",
        "## Perceptrón\n",
        "\n",
        "El preceptrón es una estructura que trata de imitar el funcionamiento de una neurona.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Blausen_0657_MultipolarNeuron.png/1024px-Blausen_0657_MultipolarNeuron.png\" alt=\"Neurona\" style=\"width: 400px;\"/>\n",
        "\n",
        "> Fig. 1: [Imágen Neurona Wikipedia](https://en.wikipedia.org/wiki/Neuron) <br>\n",
        "\n",
        "Donde el modelo se simplifica a: <br>\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/31/Perceptron.svg\" alt=\"Perceptrón\" style=\"width: 400px;\"/>\n",
        "\n",
        "> Fig. 2: [Imágen Perceptrón Wikipedia](https://en.wikipedia.org/wiki/Perceptron) <br>\n",
        "\n",
        "Considerando $\\overline{x}=(x_{1}, x_{2},...,x_{n})$ y $\\overline{w}=(w_{1}, w_{2},...,w_{n})$: <br>\n",
        "$$percept(\\overline{x})=f(\\overline{x} \\cdot \\overline{w} + b)=f(\\sum(x_{i} \\cdot x_{i} )+ b)$$\n",
        "Dependiendo como se seleccione $f(o)$, un perceptrón es simirar a una regresión lineal ($f(o)=o$) o a una regresión lógistica $f(o)=\\frac{1}{1+e^{-o}}$. Obviamente, existen otras funciones de activación que se irán discutiendo a lo largo del curso.\n",
        "\n",
        "Combinando diversos preceptrones en forma paralela se forma una capa de una red neuronal. En este caso, la capa se conoce como una capa densa, ya que todas las salidas de la capa están conectadas con cada entrada. En el caso de una capa, $W$ es una matriz de dimensiones $cantidad\\ de\\ características$ X $cantidad\\ de\\ perceptrones$, $b$ es un vector de dimensionalidad $cantidad\\ de\\ perceptrones$, y $f(o)$ se aplíca elemento a elemento del resultado. Considerando el trabajo práctico de la clase anterior $W$ es una matriz de 786 X 10, y $b$ es un vector de 10 elementos.\n",
        "\n",
        "## Keras\n",
        "[Keras](https://keras.io/) en una librería de alto nivel para redes neuronales que abstrae las operaciones más comunes de las redes neuronales facilitando la escritura de un código más limpio. Keras está construida sobre [TensorFlow](https://keras.io/backend/), por lo que no es necesario instalar ningún elemento extra. Todas las abstracciones de Keras se encuentran en el paquete tensorflow.keras.\n",
        "\n",
        "A continuación, se muesta un ejemplo de utilización de Keras para el problema clasificación de MNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ifz2kaQIW0D"
      },
      "source": [
        "%matplotlib inline \n",
        "import matplotlib as mpl \n",
        "import seaborn as sn \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, GaussianNoise, GaussianDropout, LayerNormalization\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.optimizers import SGD \n",
        "from tensorflow.keras.datasets import mnist \n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "def show_confusion_matrix(cm, labels): \n",
        "    df_cm = pd.DataFrame(cm, index=labels, columns=labels) \n",
        "    sn.heatmap(df_cm, annot=True, fmt=\"d\") \n",
        "    plt.show()\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = [12.0, 8.0] \n",
        "print(mpl.rcParams['figure.figsize'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivTSeoaM0rYF"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('100 primeros elementos del conjunto de entrenaimento')\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(x_train[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[:100])\n",
        "\n",
        "size = x_train.shape[1]*x_train.shape[2]\n",
        "x_train = x_train.reshape((x_train.shape[0], size)) / 255\n",
        "x_test = x_test.reshape((x_test.shape[0], size)) / 255\n",
        "\n",
        "yc_train, yc_test = to_categorical(y_train), to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Br3po19WG9r"
      },
      "source": [
        "## Keras capas y modelos\n",
        "Keras nos permite definir nuestro modelo a través de la combinación de diversas capas en un modelo. Para comenzar, definiremos un modelo basado en una regresión simple y una activación softmax (como en la clase pasada). Para esot utilizaremos dos tipos de capas:\n",
        "\n",
        "* Input: esta capa nos permite definir la entrada de nuestro modelo.\n",
        "*Dense: es una capa que multiplica las entradas por los pesos y les suma los bias para finalmente aplicar una función de activación.\n",
        "\n",
        "Además, definimos el modelo. En este caso, le decimos cual es nuestra entrada y salida. Adicionalmente, lo compilamos indicando la función de perdida y que optimizador utilizaremos. Es importante destacar que como utilizamos elementos estándar de Keras podemos definirlo simplemente con un string, pero podemos definir cualquier función o comportamiento que desearamos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ly4dhfEMXH"
      },
      "source": [
        "i = Input((size,))\n",
        "d = Dense(10, activation='softmax')(i)\n",
        "\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, yc_train, batch_size=50, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwT06d5ZzBBI"
      },
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "show_confusion_matrix(confusion_matrix(y_test, y_pred), list(map(str, range(10))))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSaDL8ZOKw9S"
      },
      "source": [
        "En este ejemplo podemos ver como cambiamos los parámetros para el algoritmo de optimización."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oXFDalU6xVO"
      },
      "source": [
        "i = Input((size,))\n",
        "d = Dense(10, activation='softmax')(i)\n",
        "\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01, momentum=0.9))\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, yc_train, batch_size=50, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlWGnSUn6xhP"
      },
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "show_confusion_matrix(confusion_matrix(y_test, y_pred), list(map(str, range(10))))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OVMGqC3W18l"
      },
      "source": [
        "# El problema del Xor\n",
        "Como se describió en las Slides, la función Xor no puede ser aprendida por una regresión lógistica.\n",
        "\n",
        "| $X_0$ | $X_1$ | $Y$ |\n",
        "| --- | --- | --- |\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ZbC2BrA7M8"
      },
      "source": [
        "x = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.asarray([0, 1, 1, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xpFsGJjCVI0"
      },
      "source": [
        "plt.scatter(x[:, 0], x[:, 1], c=['green' if i==1 else 'red' for i in y])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoIs3pRS0t5v"
      },
      "source": [
        "i = Input((2,))\n",
        "d = Dense(1, activation='sigmoid')(i)\n",
        "model = Model(i, d)\n",
        "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "h = model.fit(x, y, epochs=1000, verbose=0)\n",
        "\n",
        "plt.title('Función de perdida')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Binary Crossentropy')\n",
        "plt.plot(h.history['loss'])\n",
        "plt.show()\n",
        "\n",
        "plt.title('Metrica accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(h.history['accuracy'])\n",
        "plt.show()\n",
        "print(model.predict(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmaCmHxrXjA4"
      },
      "source": [
        "# Perceptrón multicapas\n",
        "¿Qué pasa si en vez de usar una sola función lineal concatenamos 2 funciones lineales?\n",
        "\n",
        "La red neuronal más sencilla, conocida como perceptrón multi-capa, no es más que capas de preceptrones aplicadas una sobre la otra.\n",
        "\n",
        "Entonces un preceptron multicapas tiene la siguiente forma:\n",
        "\n",
        "$$l_{1}=f_{1}(\\overline{x} \\cdot W_{1} + \\overline{bias_{1}})$$\n",
        "\n",
        "$$l_{2}=f_{2}(\\overline{l_{1}} \\cdot W_{2} + \\overline{bias_{2}})$$\n",
        "\n",
        "$$...$$\n",
        "\n",
        "$$l_{N}=f_{N}(\\overline{l_{N-1}} \\cdot W_{N} + \\overline{bias_{N}})$$\n",
        "\n",
        "Es importante destacar que, dado una función de error, calcular el gradiente para cada parámetro de la red, sea $W_{i}$ o $bias_{i}$, es simplemente aplicar la regla de la cadena en repetidas oscaciones. Esto hace que puedan ser calculados de forma automática por librerias como Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d50axpWCVLt"
      },
      "source": [
        "i = Input((2,))\n",
        "d = Dense(30, activation='tanh')(i)\n",
        "d = Dense(1, activation='sigmoid')(d)\n",
        "model = Model(i, d)\n",
        "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "h = model.fit(x, y, epochs=500, verbose=0)\n",
        "\n",
        "plt.title('Función de perdida')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Binary Crossentropy')\n",
        "plt.plot(h.history['loss'])\n",
        "plt.show()\n",
        "\n",
        "plt.title('Metrica accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(h.history['accuracy'])\n",
        "plt.show()\n",
        "print(model.predict(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBRnJr0lL69b"
      },
      "source": [
        "## Overfitting\n",
        "Uno de los grandes problemas de las redes neuronales, en especial cuando hay pocos datos de entrenamiento, es que tienden a sobreaprender los datos en el conjunto de entrenamiento y no generalizan bien.\n",
        "\n",
        "Supongamos el caso artificial de la clase anterior donde los datos tiene la forma:\n",
        "\n",
        "$$y = 3*x + rand(-0.5, 0.5)$$\n",
        "\n",
        "Obviamente un modelo lineal sería más que suficiente para aprender este conjunto de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vL3CFjCCVOv"
      },
      "source": [
        "np.random.seed(42)\n",
        "'''def gen_random_data(mult):\n",
        "    _x = np.linspace(-1, 1, 20)\n",
        "    _error = 0.5 * (np.random.rand(*_x.shape) - .5)\n",
        "    _y = _x * mult + _error\n",
        "    return _x, _y\n",
        "\n",
        "\n",
        "x, y = gen_random_data(3)\n",
        "plt.plot(x, y, 'ro')\n",
        "plt.show()'''\n",
        "def gen_random_data(mult):\n",
        "    _x = np.linspace(-1, 1, 100)\n",
        "    _error = (np.random.rand(*_x.shape) - .5)\n",
        "    _y = _x * mult + _error\n",
        "    return _x, _y\n",
        "\n",
        "\n",
        "x, y = gen_random_data(3)\n",
        "plt.plot(x, y, 'ro')\n",
        "plt.show()\n",
        "print('x: {}'.format(x))\n",
        "print('y: {}'.format(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ujIbfylCVSD"
      },
      "source": [
        "i = Input((1,))\n",
        "d = Dense(1)(i)\n",
        "model = Model(i, d)\n",
        "model.compile(loss='mse', optimizer='sgd')\n",
        "\n",
        "h = model.fit(np.expand_dims(x, axis=-1), y, epochs=300, verbose=0)\n",
        "plt.plot(x, y, 'ro', x, model.predict(np.expand_dims(x, axis=-1)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6P5qWERM_QT"
      },
      "source": [
        "Pero, si aplicamos un modelo con más capas, este modelo tiende a aprender los \"errores\" en las observacciones del conjunto de entrenamiento. Con lo que surge la pregunta ¿Cuál es el mejor modelo el representado por la linea azul o la linea verde?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33PzisXNH4ZU"
      },
      "source": [
        "i = Input((1,))\n",
        "d = Dense(2, activation='sigmoid')(i)\n",
        "d = Dense(1)(d)\n",
        "model1 = Model(i, d)\n",
        "model1.compile(loss='mse', optimizer='sgd')\n",
        "\n",
        "h = model1.fit(np.expand_dims(x, axis=-1), y, epochs=1000, verbose=0, callbacks=[ReduceLROnPlateau('loss')])\n",
        "plt.plot(x, y, 'ro', x, model1.predict(np.expand_dims(x, axis=-1)),'g-' ,x, model.predict(np.expand_dims(x, axis=-1)), 'b-')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5yCIcgXOmVY"
      },
      "source": [
        "## Regularizaciones\n",
        "\n",
        "En las siguientes celdas se muestran ejemplos de técnicas de regularización generalmente utilizadas en las redes neuronales. Es importante destacar que no son las únicas técnicas que se utilizan para evitar el overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05krAzD-dOOh"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255\n",
        "\n",
        "Y_train = to_categorical(y_train) \n",
        "Y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmkRdX9OdZW1"
      },
      "source": [
        "i = Input((X_train.shape[1],))\n",
        "d = Dense(512, activation='relu')(i)\n",
        "d = Dense(256, activation='relu')(d)\n",
        "d = Dense(128, activation='relu')(d)\n",
        "d = Dense(10, activation='softmax')(d)\n",
        "model = Model(i, d)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA-S-t1LeCh3"
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=20, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEyd-FfleQs7"
      },
      "source": [
        "plt.plot(range(1, 21), history.history['loss'], 'r-',label='Training')\n",
        "plt.plot(range(1, 21), history.history['val_loss'], 'b-', label='Test')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss: Categorical CrossEntropy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5Ho06igw96"
      },
      "source": [
        "plt.plot(range(1, 21), history.history['accuracy'], 'r-',label='Training')\n",
        "plt.plot(range(1, 21), history.history['val_accuracy'], 'b-', label='Test')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krqjgkiifUUK"
      },
      "source": [
        "i = Input((X_train.shape[1],))\n",
        "d = Dense(512, activation='relu', kernel_regularizer=L2(0.0001))(i)\n",
        "d = Dense(256, activation='relu', kernel_regularizer=L2(0.0001))(d)\n",
        "d = Dense(128, activation='relu', kernel_regularizer=L2(0.0001))(d)\n",
        "d = Dense(10, activation='softmax', kernel_regularizer=L2(0.0001))(d)\n",
        "model = Model(i, d)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, Y_train, epochs=20, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-nTm1Gig70E"
      },
      "source": [
        "plt.plot(range(1, 21), history.history['loss'], 'r-',label='Training')\n",
        "plt.plot(range(1, 21), history.history['val_loss'], 'b-', label='Test')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss: Categorical CrossEntropy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izmWzB8Hg7_w"
      },
      "source": [
        "plt.plot(range(1, 21), history.history['accuracy'], 'r-',label='Training')\n",
        "plt.plot(range(1, 21), history.history['val_accuracy'], 'b-', label='Test')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1aVRY57fUrz"
      },
      "source": [
        "i = Input((X_train.shape[1],))\n",
        "d = Dense(512, activation='relu')(i)\n",
        "d = BatchNormalization()(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = Dense(256, activation='relu')(d)\n",
        "d = BatchNormalization()(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = Dense(128, activation='relu')(d)\n",
        "d = BatchNormalization()(d)\n",
        "d = Dropout(0.5)(d)\n",
        "d = Dense(10, activation='softmax')(d)\n",
        "model = Model(i, d)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, Y_train, epochs=20, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaRPh9TwhEdR"
      },
      "source": [
        "plt.plot(range(1, 21), history.history['loss'], 'r-',label='Training')\n",
        "plt.plot(range(1, 21), history.history['val_loss'], 'b-', label='Test')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss: Categorical CrossEntropy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebz2potThEjj"
      },
      "source": [
        "plt.plot(range(1, 21), history.history['accuracy'], 'r-',label='Training')\n",
        "plt.plot(range(1, 21), history.history['val_accuracy'], 'b-', label='Test')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imIwp5CdPlTI"
      },
      "source": [
        "## ¿Cuál es el efecto de las diferentes capas de regularización?\n",
        "\n",
        "En las siguientes celdas se muestra el efecto de la regularización."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkNBNpF-SQbX"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255\n",
        "\n",
        "d = Dropout(0.5)\n",
        "X_reg = d(X_train.astype(np.float32), training=True)\n",
        "\n",
        "\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(X_train[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[:100])\n",
        "\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(X_reg[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd60Ht26T0ik"
      },
      "source": [
        "d = GaussianNoise(0.3)\n",
        "X_reg = d(X_train.astype(np.float32), training=True)\n",
        "\n",
        "\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(X_reg[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKx4gqygT0us"
      },
      "source": [
        "d = GaussianDropout(0.5)\n",
        "X_reg = d(X_train.astype(np.float32), training=True)\n",
        "\n",
        "\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(X_reg[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KkKChClgH_m"
      },
      "source": [
        "d = BatchNormalization()\n",
        "X_reg = d(X_train.astype(np.float32), training=True)\n",
        "\n",
        "\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(X_reg[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIptjeNph0Hy"
      },
      "source": [
        "d = LayerNormalization()\n",
        "X_reg = d(X_train.astype(np.float32), training=True)\n",
        "\n",
        "\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(X_reg[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JJGzBsP2e5"
      },
      "source": [
        "## Early Stopping\n",
        "\n",
        "Otra técnica bastante usada para disminuir el efecto de overfitting es llamado Early Stopping. Básicamente consiste en dejar de entrenar cuando se comienza a haber overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0uSknGyxjMI"
      },
      "source": [
        "X, Y = make_moons(100, True, 0.2, random_state=42)\n",
        "\n",
        "for x, y in zip(X, Y):\n",
        "    if y == 0:\n",
        "        plt.plot(x[0], x[1], 'r*')\n",
        "    else:\n",
        "        plt.plot(x[0], x[1], 'b*')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJl7A4p13L2E"
      },
      "source": [
        "n_train = 30\n",
        "x_train, x_test = X[:n_train, :], X[n_train:, :]\n",
        "y_train, y_test = Y[:n_train], Y[n_train:]\n",
        "\n",
        "for x, y in zip(x_train, y_train):\n",
        "    if y == 0:\n",
        "        plt.plot(x[0], x[1], 'r*')\n",
        "    else:\n",
        "        plt.plot(x[0], x[1], 'b*')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for x, y in zip(x_test, y_test):\n",
        "    if y == 0:\n",
        "        plt.plot(x[0], x[1], 'r*')\n",
        "    else:\n",
        "        plt.plot(x[0], x[1], 'b*')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OQb334V3mij"
      },
      "source": [
        "i = Input((2,))\n",
        "d = Dense(500, activation='relu')(i)\n",
        "d = Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "model = Model(i, d)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "h = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2000, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY3__ZJ-4U0F"
      },
      "source": [
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdMWY7kc4kfE"
      },
      "source": [
        "plt.plot(h.history['loss'], 'r-', label='train')\n",
        "plt.plot(h.history['val_loss'], 'b-', label='test')\n",
        "plt.ylabel('Crossentropy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv7OPvDOhmvW"
      },
      "source": [
        "plt.plot(history.history['accuracy'], 'r-',label='Training')\n",
        "plt.plot(history.history['val_accuracy'], 'b-', label='Test')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ra2n186L7n"
      },
      "source": [
        "i = Input((2,))\n",
        "d = Dense(500, activation='relu')(i)\n",
        "d = Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "model = Model(i, d)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "h = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
        "              epochs=2000, verbose=0, callbacks=[EarlyStopping('val_loss', mode='min', patience=1)])\n",
        "\n",
        "plt.plot(h.history['loss'], 'r-', label='train')\n",
        "plt.plot(h.history['val_loss'], 'b-', label='test')\n",
        "plt.ylabel('Crossentropy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.show()\n",
        "plt.plot(h.history['accuracy'], 'r-',label='Training')\n",
        "plt.plot(h.history['val_accuracy'], 'b-', label='Test')\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}