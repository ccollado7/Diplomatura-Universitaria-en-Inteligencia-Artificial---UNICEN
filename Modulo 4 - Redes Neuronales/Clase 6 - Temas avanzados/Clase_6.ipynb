{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase-6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeJXqef3dOmc"
      },
      "source": [
        "# Autoencoders\n",
        "\n",
        "Un autoencoder son dos redes neuronales, una cuyo objetivo es reducir la representación de las instancias a un espacio dimensional más bajo. Por ejemplo, en el ejemplo de la MNIST existen 784 caractéristicas y se podría desear reducirlas a un número menor. Esto se llama reducción de dimensionalidad y tiene diversas aplicaciones, como entrenar modelos que no se comportan bien cuando hay muchas caractéristicas, eliminar caractéristicas redundantes, o reducir el nivel de ruido. Un ejemplo de utilización de autoencoders puede ser para comprimir imagenes, de hecho se ha probado que son competitivos cuando se comparan con estandares de la industria como JPEG2000 [1]. La arquitectura de un autoencoder es:\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png\"/>\n",
        "\n",
        "> [Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)\n",
        "\n",
        "\n",
        "\n",
        "[1] Theis, L., Shi, W., Cunningham, A., & Huszár, F. (2017). Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395.\n",
        "\n",
        "\n",
        "## Ejemplo de autoencoder\n",
        "\n",
        "En este ejemplo, se proyectan las imágeenes  eel conjunto de datos NMIST a un espacio 2D, que permite graficar las intancias en un plano, para posteriormente reconstruir las imágenes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELA_u72zblhT"
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Activation, Dense, Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, image_size * image_size])\n",
        "x_test = np.reshape(x_test, [-1, image_size * image_size])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXVfbvcHeEdn"
      },
      "source": [
        "i = Input((image_size * image_size,))\n",
        "d = Dense(100, activation='relu')(i)\n",
        "d = Dense(2, activation='linear')(d)\n",
        "encoder = Model(i, d, name='Encoder')\n",
        "encoder.summary()\n",
        "\n",
        "d_i = Input((2,))\n",
        "d_d = Dense(100, activation='relu')(d_i)\n",
        "d_d = Dense(image_size * image_size, activation='sigmoid')(d_d)\n",
        "decoder = Model(d_i, d_d, name='Decoders')\n",
        "decoder.summary()\n",
        "\n",
        "autoencoder = Model(i, decoder(encoder(i)), name='Autoencoder')\n",
        "autoencoder.compile(loss='mse', optimizer='nadam')\n",
        "\n",
        "autoencoder.fit(x_train,\n",
        "                x_train,\n",
        "                validation_data=(x_test, x_test),\n",
        "                epochs=30,\n",
        "                batch_size=128, verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rORQsurzgs1l"
      },
      "source": [
        "img = np.empty((image_size*10, image_size*10))\n",
        "img_pred = np.empty((image_size*10, image_size*10))\n",
        "x_test_pred = autoencoder.predict(x_test)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        img[i*image_size:(i+1)*image_size, j*image_size:(j+1)*image_size] = np.reshape(x_test[i*10+j, :], (image_size, image_size))\n",
        "        img_pred[i*image_size:(i+1)*image_size, j*image_size:(j+1)*image_size] = np.reshape(x_test_pred[i*10+j, :], (image_size, image_size))\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(img_pred, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ps6rSunGk0B"
      },
      "source": [
        "emb = encoder.predict(x_test)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "plt.scatter(emb[:, 0], emb[:, 1], c=y_test)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU-0zuc7JEFU"
      },
      "source": [
        "## Denoiser autoencoder\n",
        "\n",
        "El siguiente ejemplo, basado en los [ejemplos de Keras](https://github.com/keras-team/keras/blob/master/examples/mnist_denoising_autoencoder.py), utilizaremos un autoencoder para sacar ruido del MNIST. En el caso del ejemplo, se agregará ruido artificialmente. En particular a cada pixel se le agregará un ruido de media 0.5 y desviación estandard de 0.5. Notese que los pixeles están normalizados a valores entre 0 y 1, por lo que el ruido es significativo.\n",
        "\n",
        "El encoder tiene las siguiente arquitectura:\n",
        "\n",
        "1. Entrada de 28 x 28 x 1\n",
        "1.  Convolucional de 32 filtros y kernel de 3x3\n",
        "1.  Convolucional de 64 filtros y kernel de 3x3\n",
        "1. Capa de aplanado. Cada imagen resulta en vectores de 3136 elementos\n",
        "1. Densa con 16 neuronas\n",
        "\n",
        "\n",
        "Es decir, al final del encoder cada imagen queda representada por un vector de 16 caractéristicas en lugar de 784 pixeles.\n",
        "\n",
        "El decoder, quien es el encargado de regenerar la imagen tiene la siguiente arquitectura:\n",
        "\n",
        "1. Entrada de 16\n",
        "1. Una capa densa con 3136 salidas\n",
        "1. Deconvolución de 64 filtros\n",
        "1. Deconvolución de 32 filtros\n",
        "1. Deconvolución de 1 filtro. Reconstruendo la imagen original.\n",
        "\n",
        "\n",
        "Las deconvoluciones son operaciones que permiten reconstruir imagenes a las que se le aplicaron filtros convolucionales. Ver: [Deconvolutional Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S0A5NuNJOLz"
      },
      "source": [
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Generate corrupted MNIST images by adding noise with normal dist\n",
        "# centered at 0.5 and std=0.5\n",
        "noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n",
        "x_train_noisy = x_train + noise\n",
        "noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n",
        "x_test_noisy = x_test + noise\n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Network parameters\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 128\n",
        "kernel_size = 3\n",
        "latent_dim = 16\n",
        "# Encoder/Decoder number of CNN layers and filters per layer\n",
        "layer_filters = [32, 64]\n",
        "\n",
        "# Build the Autoencoder Model\n",
        "# First build the Encoder Model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = inputs\n",
        "# Stack of Conv2D blocks\n",
        "# Notes:\n",
        "# 1) Use Batch Normalization before ReLU on deep networks\n",
        "# 2) Use MaxPooling2D as alternative to strides>1\n",
        "# - faster but not as good as strides>1\n",
        "for filters in layer_filters:\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=2,\n",
        "               activation='relu',\n",
        "               padding='same')(x)\n",
        "\n",
        "# Shape info needed to build Decoder Model\n",
        "shape = K.int_shape(x)\n",
        "\n",
        "# Generate the latent vector\n",
        "x = Flatten()(x)\n",
        "latent = Dense(latent_dim, name='latent_vector')(x)\n",
        "\n",
        "# Instantiate Encoder Model\n",
        "encoder = Model(inputs, latent, name='encoder')\n",
        "print('Encoder')\n",
        "encoder.summary()\n",
        "\n",
        "# Build the Decoder Model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
        "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
        "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
        "\n",
        "# Stack of Transposed Conv2D blocks\n",
        "# Notes:\n",
        "# 1) Use Batch Normalization before ReLU on deep networks\n",
        "# 2) Use UpSampling2D as alternative to strides>1\n",
        "# - faster but not as good as strides>1\n",
        "for filters in layer_filters[::-1]:\n",
        "    x = Conv2DTranspose(filters=filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides=2,\n",
        "                        activation='relu',\n",
        "                        padding='same')(x)\n",
        "\n",
        "x = Conv2DTranspose(filters=1,\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding='same')(x)\n",
        "\n",
        "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
        "\n",
        "# Instantiate Decoder Model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "print('Decoder')\n",
        "decoder.summary()\n",
        "\n",
        "# Autoencoder = Encoder + Decoder\n",
        "# Instantiate Autoencoder Model\n",
        "print('Encoder-decoder apliado para entrenamiento')\n",
        "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
        "autoencoder.summary()\n",
        "\n",
        "autoencoder.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(x_train_noisy,\n",
        "                x_train,\n",
        "                validation_data=(x_test_noisy, x_test),\n",
        "                epochs=10,\n",
        "                batch_size=batch_size)\n",
        "\n",
        "# Predict the Autoencoder output from corrupted test images\n",
        "x_decoded = autoencoder.predict(x_test_noisy)\n",
        "\n",
        "# Display the 1st 8 corrupted and denoised images\n",
        "rows, cols = 10, 30\n",
        "num = rows * cols\n",
        "imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\n",
        "imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n",
        "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
        "imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n",
        "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
        "imgs = (imgs * 255).astype(np.uint8)\n",
        "plt.rcParams['figure.figsize'] = [25, 25]\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.title('Original images: top rows, '\n",
        "          'Corrupted Input: middle rows, '\n",
        "          'Denoised Input:  third rows')\n",
        "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJt8py4LOuuD"
      },
      "source": [
        "## Variational Autoencoder\n",
        "Los Variational Autoencoders intentan aprender una representación estádistica de las dimensiones latentes. El encoder retorna la distribución de las dimensiones latentes, retornando su media y desviación estándar. Por su parte, el decoder utiliza un muestreo sobre esta distribución para generar las imágenes.\n",
        "\n",
        "$autoencoder(x)=P(z|x)$\n",
        "\n",
        "$decoder(z)=p(x|z)$\n",
        "\n",
        "La función de perdida utilizada para este tipo de autoencoders es la **Divergencia de Kullback-Leibler**, también conocida como divergencia de información. Es una función de perdida no simetrica que evalúa cuan similares son dos distribuciones de probabilidad.\n",
        "\n",
        "$KL(p||q)=\\sum{p_i \\ln{\\frac{p_i}{q_i}}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTFogKngR2px"
      },
      "source": [
        "%matplotlib inline\n",
        "from tensorflow.keras.layers import Lambda, Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.losses import mse, binary_crossentropy\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "\n",
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128):\n",
        "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
        "    # Arguments\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "        model_name (string): which model is using this function\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, z_log_var, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_log_var[:, 0], z_log_var[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = (n - 1) * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "original_dim = image_size * image_size\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# network parameters\n",
        "input_shape = (original_dim, )\n",
        "intermediate_dim = 512\n",
        "batch_size = 128\n",
        "latent_dim = 2\n",
        "epochs = 50\n",
        "\n",
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "def run():\n",
        "    models = (encoder, decoder)\n",
        "    data = (x_test, y_test)\n",
        "\n",
        "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
        "    #reconstruction_loss = mse(inputs, outputs)\n",
        "    reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
        "\n",
        "    reconstruction_loss *= original_dim\n",
        "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "    kl_loss = K.sum(kl_loss, axis=-1)\n",
        "    kl_loss *= -0.5\n",
        "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "    vae.add_loss(vae_loss)\n",
        "    vae.compile(optimizer='adam')\n",
        "    vae.summary()\n",
        "    \n",
        "    # train the autoencoder\n",
        "    vae.fit(x_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(x_test, None))\n",
        "    #vae.save_weights('vae_mlp_mnist.h5')\n",
        "\n",
        "    plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=batch_size)\n",
        "    \n",
        "run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG6fF0zNrgh7"
      },
      "source": [
        "# Transfer learning\n",
        "\n",
        "\n",
        "Trasnfer learning es otra manera de utilizar las técnicas de Deep Learning. Se utiliza en casos donde los datos de entrenamiento son escasos, pero se tiene modelos entrenados para tareas similares. Para ejemplificar, utilizaremos el dataset conocido como [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "\n",
        "| Propiedad | Valor |\n",
        "| --- | --- |\n",
        "| Clases | 10 |\n",
        "| Tamaño de las imágenes | 32 X 32  |\n",
        "| Canales de las imágenes | 3 (RGB)  |\n",
        "| Instancias de entrenamiento | 50.000 |\n",
        "| Instancias de testeo | 10.000 |\n",
        "| Valor mínimo de cada pixel | 0 |\n",
        "| Valor máximo de cada pixel | 255 |\n",
        "\n",
        "El dataset contiene imágenes en color de 32 X 32 pixeles divididas en 10 clases:\n",
        "1. Avión\n",
        "1. Auto\t\t\t\t\t\t\t\t\t\t\n",
        "1. Pájaro\t\t\t\t\t\t\t\t\t\n",
        "1. Gato\t\t\t\t\t\t\t\n",
        "1. Venado\t\t\t\t\t\t\t\t\t\t\n",
        "1. Perro\t\t\t\t\t\t\n",
        "1. Rana\t\t\t\t\t\t\t\t\t\n",
        "1. Caballo\t\t\t\t\t\t\t\t\t\t\n",
        "1. Barco\t\t\t\t\t\t\t\n",
        "1. Camión\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWNzf9HxskQs"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('100 primeros elementos del conjunto de entrenamiento')\n",
        "f = plt.figure(111)\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
        "        ax.set_xticklabels('')\n",
        "        ax.set_yticklabels('')\n",
        "        ax.imshow(x_train[i + j*10, :, :], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6Gg3Xgdsw0T"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score as acc, confusion_matrix\n",
        "\n",
        "def show_confusion_matrix_nl(cm):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm)\n",
        "    plt.title('Matriz de confusión')\n",
        "    fig.colorbar(cax)\n",
        "    plt.xlabel('Verdadero')\n",
        "    plt.ylabel('Predicho')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "i = Input(shape=(32, 32, 3))\n",
        "d = Conv2D(5, (5,5), activation='relu')(i)\n",
        "d = Conv2D(5, (5,5), activation='relu')(d)\n",
        "d = Conv2D(5, (5,5), activation='relu')(d)\n",
        "d = Conv2D(10, (5,5), activation='relu')(d)\n",
        "d = Flatten()(d)\n",
        "d = Dense(10, activation='softmax')(d)\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])\n",
        "\n",
        "predict = lambda x: np.argmax(model.predict(x), axis=-1)\n",
        "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
        "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
        "\n",
        "h = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=100, \n",
        "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
        "\n",
        "\n",
        "print('Función de pérdidad:')\n",
        "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
        "plt.show()\n",
        "print('Accuracy:')\n",
        "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
        "plt.show()\n",
        "\n",
        "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
        "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzDdFEgJs-CQ"
      },
      "source": [
        "Supongamos que tenemos solo una porción de datos para entrenar, por ejemplo 2000 imágenes (200 por cada clases). ¿Sería posible entrenar la red neuronal?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x52WfBwtfdW"
      },
      "source": [
        "sample_per_class = 200\n",
        "\n",
        "x_small = np.empty((sample_per_class * 10, 32, 32, 3))\n",
        "y_small = np.empty((sample_per_class * 10,))\n",
        "\n",
        "\n",
        "counter = [0] * 10\n",
        "\n",
        "i = 0\n",
        "for x, y in zip(x_train, y_train):\n",
        "    if counter[y[0]] == sample_per_class:\n",
        "      continue\n",
        "    counter[y[0]] += 1\n",
        "    x_small[i, :, :, :] = x\n",
        "    y_small[i] = y\n",
        "    i += 1\n",
        "    if i == sample_per_class * 10: \n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzJObRxatyrx"
      },
      "source": [
        "i = Input(shape=(32, 32, 3))\n",
        "d = Conv2D(5, (5,5), activation='relu')(i)\n",
        "d = Conv2D(5, (5,5), activation='relu')(d)\n",
        "d = Conv2D(5, (5,5), activation='relu')(d)\n",
        "d = Conv2D(10, (5,5), activation='relu')(d)\n",
        "d = Flatten()(d)\n",
        "d = Dense(10, activation='softmax')(d)\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])\n",
        "\n",
        "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
        "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
        "\n",
        "h = model.fit(x_small, to_categorical(y_small), epochs=50, batch_size=100, \n",
        "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
        "\n",
        "\n",
        "print('Función de pérdidad:')\n",
        "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
        "plt.show()\n",
        "print('Accuracy:')\n",
        "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
        "plt.show()\n",
        "\n",
        "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
        "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJPqG4V8t5QQ"
      },
      "source": [
        "En el gráfico de accuracy podemos ver que la red aprende muy bien a identificar los ejemplos de entrenamiento. Llega a un accuracy del $40\\%$, pero cuando hacemos la evaluación con el conjunto de test, el valor es del $30\\%$. Este fenómeno se conoce como *overfitting* y es un problema importante cuando se usa este tipo de técnicas con pocos datos.\n",
        "\n",
        "Para este tipo de problemas se utiliza el *transfer learning*. Para esto, se debe considerad alguna red neuronal arbitraría entrenada para clasificar imágenes con un dataset grande. Hay muchas disponibles públicamente. Keras provee varias [redes preentrenadas](https://keras.io/applications/) con el dataset de [ImageNet](http://www.image-net.org/), más de 14 millones de imágenes con 1000 clases. Por ser una arquictura simple, podemos tomar VGG16 que tiene más de **138 millones de parámetros**. A continuación, se puede observar la arquitectura de la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_zT9jXmt9dt"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "model = VGG16(include_top=True)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALUUtgx7uDA-"
      },
      "source": [
        "Si consideramos que las capas ocultas aprenden las características de las imágenes, podemos separar la red en dos partes:\n",
        "\n",
        "1. Desde la capa de `Input` hasta la capa `block5_pool` como un extractor de características.\n",
        "2. Las capas `fc1` y `fc2` como un clasificador. \n",
        "\n",
        "Si nos quedamos con la primera parte podemos tener un extractor de características para imágenes genéricas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQSfVfvGuDv5"
      },
      "source": [
        "#El modelo es pesado y no queremos que se rompa por falta de memoria en la GPU\n",
        "del model \n",
        "#Ahora si, sin el tope!!\n",
        "model = VGG16(include_top=False)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSCutVEKuHM5"
      },
      "source": [
        "Por comparación vamos a crear 2 dataset nuevos:\n",
        "\n",
        "\n",
        "1. **x_small_t** y **x_test_t**: dataset small transformado con el modelo VGG16.\n",
        "2. **x_small_f** y **x_test_f**: dataset small con forma cambiada para que cada pixel de la imagen sea un valor en un vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-5DLIwJuKn_"
      },
      "source": [
        "#Dataset de transfer learning\n",
        "x_small_t = model.predict(x_small)\n",
        "#Esto hace las veces de flatten\n",
        "x_small_t = np.reshape(x_small_t, (x_small.shape[0], 512))\n",
        "print('Forma del dataset transformado con VGG16 {}'.format(x_small_t.shape))\n",
        "#Test set\n",
        "x_test_t = model.predict(x_test)\n",
        "x_test_t = np.reshape(x_test_t, (x_test.shape[0], 512))\n",
        "\n",
        "\n",
        "#Dataset de imagenes\n",
        "x_small_f = np.reshape(x_small, (x_small.shape[0], 32 * 32 * 3))\n",
        "print('Forma del dataset original {}'.format(x_small_f.shape))\n",
        "x_test_f = np.reshape(x_test, (x_test.shape[0], 32 * 32 * 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuflBbvruNAz"
      },
      "source": [
        "Podemos probar los dos tipos de características con una regresión logística:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJyfH0TruQf1"
      },
      "source": [
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#Los parámetros son para evitar warnings, estandades hasta la versión 0.22\n",
        "mt = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "mf = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "\n",
        "print('Entrenando Transfer')\n",
        "mt.fit(x_small_t, y_small)\n",
        "print('Entrenando Full')\n",
        "mf.fit(x_small_f, y_small)\n",
        "\n",
        "print('Accuracy: {}'.format(acc(y_test, mt.predict(x_test_t))))\n",
        "print('Accuracy: {}'.format(acc(y_test, mf.predict(x_test_f))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNpZrCokuT6E"
      },
      "source": [
        "Podemos observar que las características transferidas tienen una mejor performance que usar los pixeles de forma cruda.\n",
        "\n",
        "## Fine Tuning\n",
        "\n",
        "Otro uso de las redes preentrenadas para extraer características es incorporarlas en otras redes neuronales para acelerar su entrenamiento. Por ejemplo, en el siguiente caso se utiliza la VGG16 como una capa inicial en una red neuronal. Para que esto funcione, es necesario que las modificaciones en los pesos más sutil que cuando se entrena una red de cero, ya que se supone que la mayoría de los pesos ya están cerca de un valor óptimo. En consecuencia, podemos cambiar el **learning rate** del optimizador, en este caso **Stocastic Gradiant Descent**, de $0.01$ a $0.001$, es decir un orden de magnitud menor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq0dO5HCuXrA"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "i = Input((32, 32, 3))\n",
        "model = VGG16(include_top=False)(i)\n",
        "\n",
        "d = Flatten()(model)\n",
        "d = Dense(512, activation='relu')(d)\n",
        "d = Dense(10, activation='softmax')(d)\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer= \\\n",
        "              SGD(lr=1e-3, momentum=0.0, decay=0.0, nesterov=False), \\\n",
        "              metrics=['categorical_accuracy'])#1e-4:ok y 30 epocs\n",
        "\n",
        "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
        "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
        "\n",
        "h = model.fit(x_train, to_categorical(y_train), epochs=4, batch_size=100, \n",
        "              validation_data=(x_test, to_categorical(y_test)), verbose=1)\n",
        "\n",
        "\n",
        "print('Función de pérdidad:')\n",
        "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
        "plt.show()\n",
        "print('Accuracy:')\n",
        "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
        "plt.show()\n",
        "\n",
        "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
        "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tADHmxUcwfMG"
      },
      "source": [
        "# GAN\n",
        "\n",
        "Generative Adeversarial Networks es una técnica para generar nuevas instancias a partir de dos redes neuronales que compiten entre ellas:\n",
        "\n",
        "* El generador: es la red neuronal encargadas de generar instancias falsas.\n",
        "* El discriminador: es la red neuronal encargada de decidir si una instancia es falsa o verdadera.\n",
        "\n",
        "Para el entrenamiento, se realizan pasadas en batch. Por un lado, al discriminador se lo alimenta con mitad de datos reales y mitad de datos falso, y como objetivo se espera que clasifique los reales como reales y los falsos como falso. En una segunda pasada, se fijan los pesos del discriminador, se conecta el generador con el discriminador, y como objetivo se fija que determine que todos los datos salidos del discriminador son verdaderos.\n",
        "\n",
        "A continuación, se presenta un ejemplo basado en ek [AC-GAN](https://github.com/keras-team/keras/blob/master/examples/mnist_acgan.py) implementado en los ejemplos de Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR3mLgTqwfdF"
      },
      "source": [
        "from collections import defaultdict\n",
        "try:\n",
        "    import cPickle as pickle\n",
        "except ImportError:\n",
        "    import pickle\n",
        "from PIL import Image\n",
        "\n",
        "from six.moves import range\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Conv2DTranspose, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Progbar\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "np.random.seed(1337)\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "def build_generator(latent_size):\n",
        "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
        "    # label drawn from P_c, to image space (..., 28, 28, 1)\n",
        "    cnn = Sequential()\n",
        "\n",
        "    cnn.add(Dense(3 * 3 * 384, input_dim=latent_size, activation='relu'))\n",
        "    cnn.add(Reshape((3, 3, 384)))\n",
        "\n",
        "    # upsample to (7, 7, ...)\n",
        "    cnn.add(Conv2DTranspose(192, 5, strides=1, padding='valid',\n",
        "                            activation='relu',\n",
        "                            kernel_initializer='glorot_normal'))\n",
        "    cnn.add(BatchNormalization())\n",
        "\n",
        "    # upsample to (14, 14, ...)\n",
        "    cnn.add(Conv2DTranspose(96, 5, strides=2, padding='same',\n",
        "                            activation='relu',\n",
        "                            kernel_initializer='glorot_normal'))\n",
        "    cnn.add(BatchNormalization())\n",
        "\n",
        "    # upsample to (28, 28, ...)\n",
        "    cnn.add(Conv2DTranspose(1, 5, strides=2, padding='same',\n",
        "                            activation='tanh',\n",
        "                            kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # this is the z space commonly referred to in GAN papers\n",
        "    latent = Input(shape=(latent_size, ))\n",
        "\n",
        "    # this will be our label\n",
        "    image_class = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    cls = Embedding(num_classes, latent_size,\n",
        "                    embeddings_initializer='glorot_normal')(image_class)\n",
        "\n",
        "    # hadamard product between z-space and a class conditional embedding\n",
        "    h = layers.multiply([latent, cls])\n",
        "\n",
        "    fake_image = cnn(h)\n",
        "\n",
        "    return Model([latent, image_class], fake_image)\n",
        "\n",
        "\n",
        "def build_discriminator():\n",
        "    # build a relatively standard conv net, with LeakyReLUs as suggested in\n",
        "    # the reference paper\n",
        "    cnn = Sequential()\n",
        "\n",
        "    cnn.add(Conv2D(32, 3, padding='same', strides=2,\n",
        "                   input_shape=(28, 28, 1)))\n",
        "    cnn.add(LeakyReLU(0.2))\n",
        "    cnn.add(Dropout(0.3))\n",
        "\n",
        "    cnn.add(Conv2D(64, 3, padding='same', strides=1))\n",
        "    cnn.add(LeakyReLU(0.2))\n",
        "    cnn.add(Dropout(0.3))\n",
        "\n",
        "    cnn.add(Conv2D(128, 3, padding='same', strides=2))\n",
        "    cnn.add(LeakyReLU(0.2))\n",
        "    cnn.add(Dropout(0.3))\n",
        "\n",
        "    cnn.add(Conv2D(256, 3, padding='same', strides=1))\n",
        "    cnn.add(LeakyReLU(0.2))\n",
        "    cnn.add(Dropout(0.3))\n",
        "\n",
        "    cnn.add(Flatten())\n",
        "\n",
        "    image = Input(shape=(28, 28, 1))\n",
        "\n",
        "    features = cnn(image)\n",
        "\n",
        "    # first output (name=generation) is whether or not the discriminator\n",
        "    # thinks the image that is being shown is fake, and the second output\n",
        "    # (name=auxiliary) is the class that the discriminator thinks the image\n",
        "    # belongs to.\n",
        "    fake = Dense(1, activation='sigmoid', name='generation')(features)\n",
        "    aux = Dense(num_classes, activation='softmax', name='auxiliary')(features)\n",
        "\n",
        "    return Model(image, [fake, aux])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EelLcPI9wkly"
      },
      "source": [
        "# batch and latent size taken from the paper\n",
        "epochs = 2\n",
        "batch_size = 100\n",
        "latent_size = 100\n",
        "# Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
        "adam_lr = 0.0002\n",
        "adam_beta_1 = 0.5\n",
        "\n",
        "# build the discriminator\n",
        "print('Discriminator model:')\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(\n",
        "    optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
        "    loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# build the generator\n",
        "generator = build_generator(latent_size)\n",
        "\n",
        "latent = Input(shape=(latent_size, ))\n",
        "image_class = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "# get a fake image\n",
        "fake = generator([latent, image_class])\n",
        "\n",
        "# we only want to be able to train generation for the combined model\n",
        "discriminator.trainable = False\n",
        "fake, aux = discriminator(fake)\n",
        "combined = Model([latent, image_class], [fake, aux])\n",
        "\n",
        "print('Combined model:')\n",
        "combined.compile(\n",
        "    optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
        "    loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
        ")\n",
        "combined.summary()\n",
        "\n",
        "# get our mnist data, and force it to be of shape (..., 28, 28, 1) with\n",
        "# range [-1, 1]\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "num_train, num_test = x_train.shape[0], x_test.shape[0]\n",
        "\n",
        "train_history = defaultdict(list)\n",
        "test_history = defaultdict(list)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print('Epoch {}/{}'.format(epoch, epochs))\n",
        "\n",
        "    num_batches = int(np.ceil(x_train.shape[0] / float(batch_size)))\n",
        "    progress_bar = Progbar(target=num_batches)\n",
        "\n",
        "    epoch_gen_loss = []\n",
        "    epoch_disc_loss = []\n",
        "\n",
        "    for index in range(num_batches):\n",
        "        # get a batch of real images\n",
        "        image_batch = x_train[index * batch_size:(index + 1) * batch_size]\n",
        "        label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "        # generate a new batch of noise\n",
        "        noise = np.random.uniform(-1, 1, (len(image_batch), latent_size))\n",
        "\n",
        "        # sample some labels from p_c\n",
        "        sampled_labels = np.random.randint(0, num_classes, len(image_batch))\n",
        "\n",
        "        # generate a batch of fake images, using the generated labels as a\n",
        "        # conditioner. We reshape the sampled labels to be\n",
        "        # (len(image_batch), 1) so that we can feed them into the embedding\n",
        "        # layer as a length one sequence\n",
        "        generated_images = generator.predict(\n",
        "            [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n",
        "\n",
        "        x = np.concatenate((image_batch, generated_images))\n",
        "\n",
        "        # use one-sided soft real/fake labels\n",
        "        # Salimans et al., 2016\n",
        "        # https://arxiv.org/pdf/1606.03498.pdf (Section 3.4)\n",
        "        soft_zero, soft_one = 0, 0.95\n",
        "        y = np.array(\n",
        "            [soft_one] * len(image_batch) + [soft_zero] * len(image_batch))\n",
        "        aux_y = np.concatenate((label_batch, sampled_labels), axis=0)\n",
        "\n",
        "        # we don't want the discriminator to also maximize the classification\n",
        "        # accuracy of the auxiliary classifier on generated images, so we\n",
        "        # don't train discriminator to produce class labels for generated\n",
        "        # images (see https://openreview.net/forum?id=rJXTf9Bxg).\n",
        "        # To preserve sum of sample weights for the auxiliary classifier,\n",
        "        # we assign sample weight of 2 to the real images.\n",
        "        disc_sample_weight = [np.ones(2 * len(image_batch)),\n",
        "                              np.concatenate((np.ones(len(image_batch)) * 2,\n",
        "                                              np.zeros(len(image_batch))))]\n",
        "\n",
        "        # see if the discriminator can figure itself out...\n",
        "        epoch_disc_loss.append(discriminator.train_on_batch(\n",
        "            x, [y, aux_y], sample_weight=disc_sample_weight))\n",
        "\n",
        "        # make new noise. we generate 2 * batch size here such that we have\n",
        "        # the generator optimize over an identical number of images as the\n",
        "        # discriminator\n",
        "        noise = np.random.uniform(-1, 1, (2 * len(image_batch), latent_size))\n",
        "        sampled_labels = np.random.randint(0, num_classes, 2 * len(image_batch))\n",
        "\n",
        "        # we want to train the generator to trick the discriminator\n",
        "        # For the generator, we want all the {fake, not-fake} labels to say\n",
        "        # not-fake\n",
        "        trick = np.ones(2 * len(image_batch)) * soft_one\n",
        "\n",
        "        epoch_gen_loss.append(combined.train_on_batch(\n",
        "           [noise, sampled_labels.reshape((-1, 1))],\n",
        "           [trick, sampled_labels]))\n",
        "\n",
        "        progress_bar.update(index + 1)\n",
        "\n",
        "    print('Testing for epoch {}:'.format(epoch))\n",
        "\n",
        "    # evaluate the testing loss here\n",
        "\n",
        "    # generate a new batch of noise\n",
        "    noise = np.random.uniform(-1, 1, (num_test, latent_size))\n",
        "\n",
        "    # sample some labels from p_c and generate images from them\n",
        "    sampled_labels = np.random.randint(0, num_classes, num_test)\n",
        "    generated_images = generator.predict(\n",
        "        [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n",
        "\n",
        "    x = np.concatenate((x_test, generated_images))\n",
        "    y = np.array([1] * num_test + [0] * num_test)\n",
        "    aux_y = np.concatenate((y_test, sampled_labels), axis=0)\n",
        "\n",
        "    # see if the discriminator can figure itself out...\n",
        "    discriminator_test_loss = discriminator.evaluate(\n",
        "        x, [y, aux_y], verbose=False)\n",
        "\n",
        "    discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
        "\n",
        "    # make new noise\n",
        "    noise = np.random.uniform(-1, 1, (2 * num_test, latent_size))\n",
        "    sampled_labels = np.random.randint(0, num_classes, 2 * num_test)\n",
        "\n",
        "    trick = np.ones(2 * num_test)\n",
        "\n",
        "    generator_test_loss = combined.evaluate(\n",
        "        [noise, sampled_labels.reshape((-1, 1))],\n",
        "        [trick, sampled_labels], verbose=False)\n",
        "\n",
        "    generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
        "\n",
        "    # generate an epoch report on performance\n",
        "    train_history['generator'].append(generator_train_loss)\n",
        "    train_history['discriminator'].append(discriminator_train_loss)\n",
        "\n",
        "    test_history['generator'].append(generator_test_loss)\n",
        "    test_history['discriminator'].append(discriminator_test_loss)\n",
        "\n",
        "    print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
        "        'component', *discriminator.metrics_names))\n",
        "    print('-' * 65)\n",
        "\n",
        "    ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}'\n",
        "    print(ROW_FMT.format('generator (train)',\n",
        "                         *train_history['generator'][-1]))\n",
        "    print(ROW_FMT.format('generator (test)',\n",
        "                         *test_history['generator'][-1]))\n",
        "    print(ROW_FMT.format('discriminator (train)',\n",
        "                         *train_history['discriminator'][-1]))\n",
        "    print(ROW_FMT.format('discriminator (test)',\n",
        "                         *test_history['discriminator'][-1]))\n",
        "\n",
        "    # generate some digits to display\n",
        "    num_rows = 40\n",
        "    noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)),\n",
        "                     (num_classes, 1))\n",
        "\n",
        "    sampled_labels = np.array([\n",
        "        [i] * num_rows for i in range(num_classes)\n",
        "    ]).reshape(-1, 1)\n",
        "\n",
        "    # get a batch to display\n",
        "    generated_images = generator.predict(\n",
        "        [noise, sampled_labels], verbose=0)\n",
        "\n",
        "    # prepare real images sorted by class label\n",
        "    real_labels = y_train[(epoch - 1) * num_rows * num_classes:\n",
        "                          epoch * num_rows * num_classes]\n",
        "    indices = np.argsort(real_labels, axis=0)\n",
        "    real_images = x_train[(epoch - 1) * num_rows * num_classes:\n",
        "                         epoch * num_rows * num_classes][indices]\n",
        "\n",
        "    # display generated images, white separator, real images\n",
        "    img = np.concatenate(\n",
        "        (generated_images,\n",
        "         np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0),\n",
        "         real_images))\n",
        "\n",
        "    # arrange them into a grid\n",
        "    img = (np.concatenate([r.reshape(-1, 28)\n",
        "                           for r in np.split(img, 2 * num_classes + 1)\n",
        "                           ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(30, 30))\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSASv8DHy_xF"
      },
      "source": [
        "# Generación de texto desde una imagen\n",
        "\n",
        "Este es un ejemplo de generación de descripción de imagenes. En este ejemplo se utiliza el código disponible en [Github](https://github.com/knife982000/imageCaptioning). El modelo es una variación de [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). \n",
        "\n",
        "Este ejemplo combina: \n",
        "1. Attention.\n",
        "2. Una red neuronal recurrente.\n",
        "3. Un modelo preentrenado de Inception v3.\n",
        "4. Un beam search para buscar las descripciones más probables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyueR4qK0nCi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_file = '/content/drive/MyDrive/DUIA-Redes-Neuronales/image_captioning/model.h5'\n",
        "tokenizer_file = '/content/drive/MyDrive/DUIA-Redes-Neuronales/image_captioning/tokenizer.pickle'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a0W7yTYzaUM"
      },
      "source": [
        "from tensorflow.keras.layers import Layer, Embedding, Input, Dense\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.framework import dtypes\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class GRUAttention(Layer):\n",
        "\n",
        "    def __init__(self, units, attention_units=None,\n",
        "                 return_sequences=False, return_state=False,\n",
        "                 mask_zeros=False, **kargs):\n",
        "        super(GRUAttention, self).__init__(**kargs)\n",
        "        self.units = units\n",
        "        if attention_units is None:\n",
        "            attention_units = units\n",
        "        self.attention_units = attention_units\n",
        "        self.return_sequences = return_sequences\n",
        "        self.return_state = return_state\n",
        "        self.mask_zeros = mask_zeros\n",
        "        pass\n",
        "\n",
        "    # noinspection PyAttributeOutsideInit\n",
        "    def build(self, input_shape):\n",
        "        img_features = input_shape[0][-1]\n",
        "        text_features = input_shape[1][-1]\n",
        "        dtype = dtypes.as_dtype(self.dtype or K.floatx())\n",
        "        self.kernel = self.add_weight('kernel', shape=(img_features + text_features, 3 * self.units), dtype=dtype)\n",
        "        self.input_bias = self.add_weight('bias', shape=(3 * self.units,), dtype=dtype)\n",
        "        self.recurrent_kernel = self.add_weight('recurrent_kernel', shape=(self.units, 3 * self.units), dtype=dtype)\n",
        "        self.recurrent_bias = self.add_weight('recurrent_bias', shape=(3 * self.units,), dtype=dtype)\n",
        "\n",
        "        self.att_img_kernel = self.add_weight('att_img_kernel', shape=(img_features, self.attention_units),\n",
        "                                              dtype=dtype)\n",
        "        self.att_img_bias = self.add_weight('att_img_bias', shape=(self.attention_units,), dtype=dtype)\n",
        "\n",
        "        self.att_hidden_kernel = self.add_weight('att_hidden_kernel', shape=(self.units, self.attention_units),\n",
        "                                                 dtype=dtype)\n",
        "        self.att_hidden_bias = self.add_weight('att_hidden_bias', shape=(self.attention_units,), dtype=dtype)\n",
        "\n",
        "        self.att_v_kernel = self.add_weight('att_v_kernel', shape=(self.attention_units, 1),\n",
        "                                               dtype=dtype)\n",
        "        self.att_v_bias = self.add_weight('att_v_bias', shape=(1,), dtype=dtype)\n",
        "        pass\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'units': self.units,\n",
        "            'attention_units': self.attention_units,\n",
        "            'return_sequences': self.return_sequences,\n",
        "            'return_state': self.return_state,\n",
        "            'mask_zeros': self.mask_zeros\n",
        "        }\n",
        "        base_config = super(GRUAttention, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def dense(self, input, kernel, bias):\n",
        "        return K.dot(input, kernel) + bias\n",
        "\n",
        "    def attention(self, image, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "        attention_hidden_layer = K.tanh(self.dense(image, self.att_img_kernel, self.att_img_bias) +\n",
        "                                        self.dense(hidden_with_time_axis, self.att_hidden_kernel, self.att_hidden_bias))\n",
        "\n",
        "        # score shape == (batch_size, 64, 1)\n",
        "        # This gives you an unnormalized score for each image feature.\n",
        "        score = self.dense(attention_hidden_layer, self.att_v_kernel, self.att_v_bias)\n",
        "\n",
        "        # attention_weights shape == (batch_size, 64, 1)\n",
        "        attention_weights = K.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size) ??embedding_dim??\n",
        "        context_vector = attention_weights * image\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "    def call(self, input, initial_state=None):\n",
        "        img, text = input\n",
        "\n",
        "        def step(cell_inputs, cell_states):\n",
        "            \"\"\"Step function that will be used by Keras RNN backend.\"\"\"\n",
        "            h_tm1 = cell_states[0]\n",
        "            features = self.attention(img, h_tm1)\n",
        "            cell_inputs = K.concatenate([cell_inputs, features], axis=-1)\n",
        "\n",
        "            # inputs projected by all gate matrices at once\n",
        "            matrix_x = K.dot(cell_inputs, self.kernel)\n",
        "            matrix_x = K.bias_add(matrix_x, self.input_bias)\n",
        "\n",
        "            x_z, x_r, x_h = array_ops.split(matrix_x, 3, axis=1)\n",
        "\n",
        "            # hidden state projected by all gate matrices at once\n",
        "            matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n",
        "            matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n",
        "\n",
        "            recurrent_z, recurrent_r, recurrent_h = array_ops.split(matrix_inner, 3,\n",
        "                                                                    axis=1)\n",
        "            z = K.sigmoid(x_z + recurrent_z)\n",
        "            r = K.sigmoid(x_r + recurrent_r)\n",
        "            hh = K.tanh(x_h + r * recurrent_h)\n",
        "\n",
        "            # previous and candidate state mixed by update gate\n",
        "            h = z * h_tm1 + (1 - z) * hh\n",
        "            return h, [h]\n",
        "\n",
        "        if initial_state is None:\n",
        "            initial_state = (array_ops.zeros((array_ops.shape(text)[0], self.units)),)\n",
        "        last, sequence, hidden = K.rnn(step, text, initial_state, zero_output_for_mask=self.mask_zeros)\n",
        "        if self.return_state and self.return_sequences:\n",
        "            return sequence, hidden\n",
        "        if self.return_state:\n",
        "            return last, hidden\n",
        "        if self.return_sequences:\n",
        "            return sequence\n",
        "        return last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6u6yOdQ0AE1"
      },
      "source": [
        "import math\n",
        "\n",
        "def generate(i, pict, text, model, width, end_id):\n",
        "    c = []\n",
        "    p = []\n",
        "    e = []\n",
        "    preds = model.predict([pict, text])[0, i-1, :]\n",
        "    for _ in range(width):\n",
        "        m = np.argmax(preds)\n",
        "        c.append(m)\n",
        "        p.append(preds[m])\n",
        "        e.append(m == end_id)\n",
        "        preds[m] = 0\n",
        "    return c, p, e\n",
        "\n",
        "def beam(pict, model, tokenizer, width, maxlen):\n",
        "    end_id = tokenizer.word_index['<end>']\n",
        "    start = np.zeros((1, maxlen), dtype=np.int32)\n",
        "    start[0, 0] = tokenizer.word_index['<start>']\n",
        "    candidates = [start]\n",
        "    ended = [False]\n",
        "    probs = [[1]]\n",
        "    pict = np.reshape(pict, (1, -1, pict.shape[-1]))\n",
        "    for i in range(1, maxlen):\n",
        "        n_candidates = []\n",
        "        n_ended = []\n",
        "        n_probs = []\n",
        "        all_ended = True\n",
        "        for c, e, p in zip(candidates, ended, probs):\n",
        "            if e:\n",
        "                n_candidates.append(c)\n",
        "                n_ended.append(e)\n",
        "                n_probs.append(p)\n",
        "            else:\n",
        "                all_ended = False\n",
        "                nc, n_p, ne = generate(i, pict, c, model, width, end_id)\n",
        "                for vnc, vnp, vne in zip(nc, n_p, ne):\n",
        "                    n_c = c.copy()\n",
        "                    n_c[0, i] = vnc\n",
        "                    n_candidates.append(n_c)\n",
        "                    n_ended.append(vne)\n",
        "                    n_probs.append(list(p) + [vnp])\n",
        "                pass\n",
        "        if all_ended:\n",
        "            break\n",
        "        log_prob = {e: np.average([math.log(x) for x in p]) for e, p in enumerate(n_probs)}\n",
        "        index = list(range(len(n_probs)))\n",
        "        index.sort(key=lambda x: log_prob[x], reverse=True)\n",
        "        index = index[:width]\n",
        "        candidates = [n_candidates[i] for i in index]\n",
        "        probs = [n_probs[i] for i in index]\n",
        "        ended = [n_ended[i] for i in index]\n",
        "    return [c[0, :] for c in candidates], probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioRODFiG0GIe"
      },
      "source": [
        "image_pre = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnygszwP0cXJ"
      },
      "source": [
        "import pickle\n",
        "with open(tokenizer_file, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d40hPyPP0REl"
      },
      "source": [
        "img = Input((64, 2048))\n",
        "d_img = Dense(300)(img)\n",
        "\n",
        "txt = Input((None, ))\n",
        "emb = Embedding(len(tokenizer.word_index), 300)(txt)\n",
        "\n",
        "d = GRUAttention(300, mask_zeros=True, return_sequences=True)([d_img, emb])\n",
        "d = Dense(len(tokenizer.word_index), activation='softmax')(d)\n",
        "\n",
        "model = Model([img, txt], d)\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "model.load_weights(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qffNHpHU2F-w"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/knife982000/imageCaptioning/master/images/cafe.jpg\n",
        "!wget https://raw.githubusercontent.com/knife982000/imageCaptioning/master/images/landscape.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq0giosb3Wcb"
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    imgp = tf.image.resize(img, (299, 299))\n",
        "    imgp = tf.keras.applications.inception_v3.preprocess_input(imgp)\n",
        "    return imgp, img, image_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDuUvPWm4aqR"
      },
      "source": [
        "def process_file(file_name):\n",
        "    img, original, _ = load_image(file_name)\n",
        "\n",
        "    plt.imshow(original)\n",
        "    plt.show()\n",
        "\n",
        "    img = image_pre.predict(img.numpy()[np.newaxis, ...])\n",
        "    print(img.shape)\n",
        "\n",
        "    cands, probs = beam(img, model, tokenizer, 3, 20)\n",
        "    for c, p in zip(cands, probs):\n",
        "        print('\\t' + ' '.join([tokenizer.index_word[w] for w in c]))\n",
        "        print('\\t{}'.format(p))\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wtCodkorTj1"
      },
      "source": [
        "process_file('cafe.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A30xOTFyrZER"
      },
      "source": [
        "process_file('landscape.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}