{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "art-style-transfer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPl2aMNHCs95"
      },
      "source": [
        "# Art style transfer\n",
        "\n",
        "CÃ³digo basado en el paper [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vqlgx7Xj8fV"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_img(base):\n",
        "    return cv2.cvtColor(cv2.imread(base), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "style_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "\n",
        "content_layers = ['block4_conv2']\n",
        "\n",
        "model = VGG16(include_top=False)\n",
        "\n",
        "model.summary() \n",
        "\n",
        "#def preprocess_input(img):\n",
        "#    return img # / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b1DZhK1k0BR"
      },
      "source": [
        "base = load_img('tandil.jpg')\n",
        "img_rows = 400\n",
        "shape = (img_rows, int(base.shape[1] / base.shape[0] * img_rows), 3)\n",
        "\n",
        "base = cv2.resize(base, (shape[1], shape[0]))\n",
        "\n",
        "\n",
        "style = load_img('style.jpg')\n",
        "style = cv2.resize(style, (shape[1], shape[0]))\n",
        "\n",
        "base = np.expand_dims(preprocess_input(base), 0)\n",
        "\n",
        "style = np.expand_dims(preprocess_input(style), 0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwUpvviJmVXn"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "outs = []\n",
        "\n",
        "i = Input((None, None, None))\n",
        "ant = i\n",
        "inx = 0\n",
        "\n",
        "for l in model.layers[1:]:\n",
        "    if inx == len(style_layers):\n",
        "        break\n",
        "    ant = l(ant)\n",
        "    if l.name == style_layers[inx]:\n",
        "        inx += 1\n",
        "        outs.append(ant)\n",
        "\n",
        "modelStyle = Model(i, outs)\n",
        "\n",
        "\n",
        "outs = []\n",
        "\n",
        "i = Input((None, None, None))\n",
        "ant = i\n",
        "inx = 0\n",
        "\n",
        "for l in model.layers[1:]:\n",
        "    if inx == len(content_layers):\n",
        "        break\n",
        "    ant = l(ant)\n",
        "    if l.name == content_layers[inx]:\n",
        "        inx += 1\n",
        "        outs.append(ant)\n",
        "\n",
        "modelContent = Model(i, outs)\n",
        "modelContent.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi8Dm_mWnnRU"
      },
      "source": [
        "base_features = [x for x in modelContent.predict(base)]\n",
        "style_features = [x[0,...] for x in modelStyle.predict(style)]\n",
        "#style_features1 = [x[0, ...] for x in modelStyle.predict(style)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMwveUeCzZB9"
      },
      "source": [
        "#style_features[0] == style_features1[0]\n",
        "print(style_features[0].shape)\n",
        "print(base_features[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EyuelPpoB06"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def gram_mat(m):\n",
        "    m = tf.transpose(m, (2, 0, 1))\n",
        "    return tf.matmul(m, m, transpose_b=True)\n",
        "\n",
        "def gram_mat(m):\n",
        "    m = tf.transpose(m, (2, 0, 1))\n",
        "    m = tf.reshape(m, tf.stack([-1, tf.reduce_prod(m.shape[1:], None, False)]))\n",
        "    return tf.matmul(m, m, transpose_b=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huqF20G4o8gT"
      },
      "source": [
        "style_features = [gram_mat(x) for x in style_features]\n",
        "#base_features = [tf.constant(x) for x in base_features]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsFwPJ199-ZF"
      },
      "source": [
        "style_features[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qEOGvbyo-GD"
      },
      "source": [
        "def loss(x, a=1, b=1, c=1):\n",
        "    x = tf.reshape(x, (1,) + shape)\n",
        "    fc = modelContent(x)\n",
        "    fs = [gram_mat(l[0]) for l in modelStyle(x)]\n",
        "\n",
        "    loss_c = 0\n",
        "    for fci, fcb in zip(fc, base_features):\n",
        "        loss_c += tf.reduce_sum(tf.math.squared_difference(fci, fcb))#/ (4 * fci.shape[0] * fci.shape[1] * fci.shape[2]) / len(fc)\n",
        "    \n",
        "    #loss_c = loss_c / (4 * shape[0] * shape[1] * shape[2])\n",
        "    loss_s = 0\n",
        "    #for fsi, fss in zip(fs, style_features):\n",
        "    #    loss_s += tf.reduce_sum(tf.math.squared_difference(fsi, fss)) / (4 * fsi.shape[0] ** 2 * fsi.shape[1] ** 2 * fsi.shape[2] ** 2) / len(fs)\n",
        "    for fsi, fss in zip(fs, style_features):\n",
        "        loss_s += tf.reduce_sum(tf.math.squared_difference(fsi, fss)) / (4 * fsi.shape[0] ** 2 * fsi.shape[1] ** 2) / len(fs)\n",
        "\n",
        "\n",
        "    ai = tf.math.squared_difference(x[:, :-1, :-1, :], x[:, 1:, :-1, :])\n",
        "    bi = tf.math.squared_difference(x[:, :-1, :-1, :], x[:, :-1, 1:, :])\n",
        "    var_loss = tf.reduce_sum(tf.pow(ai+bi, 1.25))\n",
        "    var_loss = tf.cast(var_loss, tf.float32)\n",
        "    \n",
        "    return a * loss_c + b * loss_s + c * var_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djc9U-1UsKUX"
      },
      "source": [
        "a = np.random.rand(shape[0] * shape[1] * shape[2]) * 150\n",
        "\n",
        "def to_image(x): \n",
        "    x = np.reshape(x, shape)\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    #x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "'''def to_image(x): \n",
        "    x = np.reshape(x, shape)# * 255\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x'''\n",
        "\n",
        "loss(a, a=1, b=0, c=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfD3phQatwng"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "cv2_imshow(to_image(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hs4_PVt0WaT"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "w = tf.random.uniform(shape=[shape[0] * shape[1] * shape[2]], minval=-150, maxval=150)\n",
        "ciclos = 100\n",
        "lr = 0.001#0.01 \n",
        "momentum = 0.9\n",
        "errors = []\n",
        "for i in tqdm(range(ciclos)):\n",
        "    with tf.GradientTape() as g:\n",
        "        g.watch([w])\n",
        "        l = loss(w,a=1500, b=10, c=0) \n",
        "        errors.append(l.numpy())\n",
        "    gw = tf.clip_by_value(g.gradient(l, w), -10000, 10000)#mejor\n",
        "    w = tf.clip_by_value(w - lr * gw, -200, 200) #mejor\n",
        "    #cv2_imshow(to_image(w.numpy()))\n",
        "    if i > 40:\n",
        "        lr = 0.0001\n",
        "\n",
        "print('Errores a medida que se actualiza el valor de w')\n",
        "plt.plot(errors)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0FhU8c62u1y"
      },
      "source": [
        "cv2_imshow(to_image(w.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP4R46JJ6BtA"
      },
      "source": [
        "w = tf.random.uniform(shape=[shape[0] * shape[1] * shape[2]], minval=-150, maxval=150)\n",
        "ciclos = 5\n",
        "lr = 0.001 \n",
        "cant = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TGkRDtIhgCJ"
      },
      "source": [
        "cv2_imshow(to_image(w.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm1jj1sQ6CBM"
      },
      "source": [
        "for i in tqdm(range(ciclos)):\n",
        "    with tf.GradientTape() as g:\n",
        "        g.watch([w])\n",
        "        l = loss(w,a=500, b=2, c=0)\n",
        "        errors.append(l.numpy())\n",
        "    gw = tf.clip_by_value(g.gradient(l, w), -10000, 10000)\n",
        "    w = tf.clip_by_value(w - lr * gw, -200, 200)\n",
        "cv2_imshow(to_image(w.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}