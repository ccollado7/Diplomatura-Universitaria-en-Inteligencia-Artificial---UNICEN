{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwqr8F8wT7Lk"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAW2mQkEqvxS"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import gensim\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Euy9Yj9oap0"
      },
      "source": [
        "glove2word2vec(glove_input_file='glove.6B.300d.txt', word2vec_output_file='vectors.txt')\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXvgeEvSqu1A"
      },
      "source": [
        "words = ['king', 'queen', 'man', 'woman', 'actor', 'actress']\n",
        "print('Vector king: {}'.format(model['king']))\n",
        "\n",
        "vec = np.empty((len(words), 300))\n",
        "for i, w in enumerate(words):\n",
        "    vec[i, :] = model[w]\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "x = TSNE().fit_transform(vec)\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x[:, 0], x[:, 1])\n",
        "for i, w in enumerate(words):\n",
        "    ax.annotate(w, (x[i, 0], x[i, 1]))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZs9nOhTr6dp"
      },
      "source": [
        "## Entrenando Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf6vYoS4r695"
      },
      "source": [
        "from keras.preprocessing.sequence import skipgrams\n",
        "sentence = list(range(1, 7))\n",
        "print(sentence)\n",
        "x, y = skipgrams(sentence, 100, window_size=2, negative_samples=1.0, shuffle=False)\n",
        "print('Skipgrams: ')\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBgBOdBpsEou"
      },
      "source": [
        "!pip install bs4\n",
        "!pip install tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "import re \n",
        "\n",
        "def preprocessor(text):\n",
        "    # remove HTML tags\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    \n",
        "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
        "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
        "    emoticons = re.findall(r, text)\n",
        "    text = re.sub(r, '', text)\n",
        "    \n",
        "    # convert to lowercase and append all emoticons behind (with space in between)\n",
        "    # replace('-','') removes nose of emoticons\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
        "    return text\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "#Baja los stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def tokenizer_stem_nostop(text):\n",
        "    porter = PorterStemmer()\n",
        "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
        "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
        "\n",
        "\n",
        "def tokenizer_simple(text):\n",
        "    return [w for w in re.split('\\s+', text.strip()) \\\n",
        "            if re.match('[a-zA-Z]+', w)]\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = [\n",
        "    'rec.autos',\n",
        "    'rec.motorcycles',\n",
        "    'rec.sport.baseball',\n",
        "    'rec.sport.hockey',\n",
        "    'sci.crypt',\n",
        "    'sci.electronics',\n",
        "    'sci.med',\n",
        "    'sci.space',\n",
        "]\n",
        "\n",
        "remove = ('headers', 'footers', 'quotes')\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories,\n",
        "                                     shuffle=True, random_state=0,\n",
        "                                     remove=remove)\n",
        "\n",
        "from collections import Counter, deque\n",
        "\n",
        "def process_corpus(data, words_id=None, min_reps=5):\n",
        "    corpus = []\n",
        "    for text in tqdm(data):\n",
        "        corpus.append(tokenizer_stem_nostop(preprocessor(text)))\n",
        "        #corpus.append(tokenizer_simple(preprocessor(text)))\n",
        "        \n",
        "    if words_id is None:\n",
        "        #Cuenta palabras en el corpus\n",
        "        words = Counter()\n",
        "\n",
        "        for s in corpus:\n",
        "            for w in s:\n",
        "                words[w] += 1\n",
        "\n",
        "        #Elimina palabras con menos de 5 repeticiones\n",
        "        words_id = {}\n",
        "        id_next = 0\n",
        "        for w, c in words.items():\n",
        "            if c >= min_reps:\n",
        "                words_id[w] = id_next\n",
        "                id_next += 1\n",
        "\n",
        "    id_words = { v:k for k, v in words_id.items()}\n",
        "    corpus_id = [[words_id[w] for w in s if w in words_id] for s in corpus]\n",
        "    return corpus_id, words_id, id_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O59V6Fe0seOA"
      },
      "source": [
        "x = deque()\n",
        "y = deque()\n",
        "\n",
        "corpus_id, words_id, id_words = process_corpus(newsgroups.data)\n",
        "\n",
        "#Crea los skipgrams de entrenamiento\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "for s in tqdm(corpus_id):\n",
        "    x1, y1 = skipgrams(s, len(id_words), window_size=3, negative_samples=5)\n",
        "    x.extend(x1)\n",
        "    y.extend(y1)\n",
        "\n",
        "\n",
        "x1, x2 = zip(*x)\n",
        "import numpy as np\n",
        "x1 = np.asarray(x1)\n",
        "x2 = np.asarray(x2)\n",
        "y = np.asarray(y)\n",
        "\n",
        "print('Vocabulario: {}'.format(len(id_words)))\n",
        "print('Skipgrams: {}'.format(x1.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qTP1Cbkshf-"
      },
      "source": [
        "from tensorflow.keras.layers import dot, Embedding, Input, Activation, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "#Función de error basada en log-likelihood\n",
        "def minus_max_likelihood(y_true, y_pred):\n",
        "    max_like = y_true * K.log(1+ K.exp(-y_pred)) + (1 - y_true) * K.log(1+ K.exp(y_pred)) \n",
        "    return max_like\n",
        "\n",
        "context_emb = Embedding(len(id_words), 64, name='Emb_context')\n",
        "target_emb = Embedding(len(id_words), 64, name='Emb_target')\n",
        "\n",
        "context = Input((1,), name='context')\n",
        "emb = context_emb(context)\n",
        "target = Input((1,), name='target')\n",
        "embT = target_emb(target)\n",
        "lam = dot([emb, embT], axes=(-1))\n",
        "lam = Flatten()(lam) \n",
        "#lam = Activation('sigmoid')(lam)\n",
        "\n",
        "model = Model(inputs=[context, target], outputs=lam)\n",
        "model.compile('adam', minus_max_likelihood)\n",
        "#model.compile('adam', 'binary_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "#Entrenamos poco \n",
        "model.fit([x1, x2], y.astype(np.float32), epochs=1, batch_size=1000)\n",
        "#Obtención de los embeddings\n",
        "vectors = K.get_value(target_emb.embeddings)\n",
        "#Recuperar memoria\n",
        "del context_emb\n",
        "del target_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOUTJTCHauQT"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "del model\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H2IskYMKVPD"
      },
      "source": [
        "print(x1[:10])\n",
        "print(x2[:10])\n",
        "print(y[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJHOlWQHuhF2"
      },
      "source": [
        "def cos(v1, v2):\n",
        "    return np.dot(v1, v2.T) / (np.dot(v1, v1.T) ** 0.5 * np.sum(v2 * v2, axis=-1) ** 0.5)\n",
        "\n",
        "\n",
        "def nearest(voc, wv, top=11):\n",
        "    dist = cos(wv, voc)\n",
        "    a = range(len(dist))\n",
        "    a = sorted(a, key=lambda x: dist[x], reverse=True)\n",
        "    return a[0:top], [dist[x] for x in a[0:top]]\n",
        "\n",
        "print('Similares a car:')\n",
        "for i, d in zip(*nearest(vectors, vectors[words_id['car'], :])):\n",
        "    print('\\t{} {}'.format(id_words[i], d))\n",
        "\n",
        "print('Similares a ford:')\n",
        "for i, d in zip(*nearest(vectors, vectors[words_id['ford'], :])):\n",
        "    print('\\t{} {}'.format(id_words[i], d))\n",
        "\n",
        "print('Similares a law:')\n",
        "for i, d in zip(*nearest(vectors, vectors[words_id['law'], :])):\n",
        "    print('\\t{} {}'.format(id_words[i], d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAyIDVgtvht9"
      },
      "source": [
        "## GloVe\n",
        "\n",
        "\n",
        "https://github.com/erwtokritos/keras-glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DSGgnQoyER7"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def bigram_count(token_list, window_size, cache):\n",
        "    sentence_size = len(token_list)\n",
        "\n",
        "    for central_index, central_word_id in enumerate(token_list):\n",
        "        for distance in range(1, window_size + 1):\n",
        "            if central_index + distance < sentence_size:\n",
        "                first_id, second_id = sorted([central_word_id, token_list[central_index + distance]])\n",
        "                cache[first_id][second_id] += 1.0 / distance\n",
        "    pass\n",
        "\n",
        "\n",
        "def build_cooccurrences(sequences, cache, window=3):\n",
        "    for seq in tqdm(sequences):\n",
        "        bigram_count(token_list=seq, cache=cache, window_size=window)\n",
        "\n",
        "\n",
        "def process_coocurrence_matrix(sentences, window_size=3):\n",
        "    cache = defaultdict(lambda : defaultdict(int))\n",
        "\n",
        "    build_cooccurrences(sentences, cache=cache, window=window_size)\n",
        "    first, second, x_ijs = deque(), deque(), deque()\n",
        "\n",
        "    for first_id in cache.keys():\n",
        "        for second_id in cache[first_id].keys():\n",
        "            x_ij = cache[first_id][second_id]\n",
        "\n",
        "            first.append(first_id)\n",
        "            second.append(second_id)\n",
        "            x_ijs.append(x_ij)\n",
        "\n",
        "            first.append(second_id)\n",
        "            second.append(first_id)\n",
        "            x_ijs.append(x_ij)\n",
        "\n",
        "    return np.array(first), np.array(second), np.array(x_ijs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUrpBqacyEvS"
      },
      "source": [
        "x1, x2, y = process_coocurrence_matrix(corpus_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O6Yz8G-RcCV"
      },
      "source": [
        "print(x1[:10])\n",
        "print(x2[:10])\n",
        "print(y[:10])\n",
        "print(max(y))\n",
        "print(np.mean(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbJNsL0GyE5l"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Add\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def custom_loss(y_true, y_pred, a = 3.0/4.0, X_MAX=100):\n",
        "    \"\"\"\n",
        "    This is GloVe's loss function\n",
        "    :param y_true: The actual values, in our case the 'observed' X_ij co-occurrence values\n",
        "    :param y_pred: The predicted (log-)co-occurrences from the model\n",
        "    :return: The loss associated with this batch\n",
        "    \"\"\"\n",
        "    return K.sum(K.pow(K.clip(y_true / X_MAX, 0.0, 1.0), a) * K.square(y_pred - K.log(y_true)), axis=-1)\n",
        "\n",
        "\n",
        "def glove_model(vocab_size=10, vector_dim=64):\n",
        "    \"\"\"\n",
        "    A Keras implementation of the GloVe architecture\n",
        "    :param vocab_size: The number of distinct words\n",
        "    :param vector_dim: The vector dimension of each word\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    input_target = Input((1,), name='central_word_id')\n",
        "    input_context = Input((1,), name='context_word_id')\n",
        "\n",
        "    central_embedding = Embedding(vocab_size+1, vector_dim, input_length=1, name='central_emb')\n",
        "    central_bias = Embedding(vocab_size+1, 1, input_length=1, name='central_bias')\n",
        "\n",
        "    context_embedding = Embedding(vocab_size, vector_dim, input_length=1, name='context_emb')\n",
        "    context_bias = Embedding(vocab_size, 1, input_length=1, name='context_bias')\n",
        "\n",
        "    vector_target = central_embedding(input_target)\n",
        "    vector_context = context_embedding(input_context)\n",
        "\n",
        "    bias_target = central_bias(input_target)\n",
        "    bias_context = context_bias(input_context)\n",
        "\n",
        "    dot_product = Dot(axes=-1)([vector_target, vector_context])\n",
        "    dot_product = Reshape((1, ))(dot_product)\n",
        "    bias_target = Reshape((1,))(bias_target)\n",
        "    bias_context = Reshape((1,))(bias_context)\n",
        "\n",
        "    prediction = Add()([dot_product, bias_target, bias_context])\n",
        "\n",
        "    model = Model(inputs=[input_target, input_context], outputs=prediction)\n",
        "    model.compile(loss=custom_loss, optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH6w3XOe1zmz"
      },
      "source": [
        "model = glove_model(len(words_id), 64)\n",
        "model.summary()\n",
        "\n",
        "model.fit([x1, x2], y, epochs=5, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYAYzI2eUed-"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4WyVREX2vcU"
      },
      "source": [
        "#Obtención de los embeddings\n",
        "vectors = K.get_value(model.layers[2].embeddings)\n",
        "#Recuperar memoria\n",
        "#del model\n",
        "\n",
        "print('Similares a car:')\n",
        "for i, d in zip(*nearest(vectors, vectors[words_id['car'], :])):\n",
        "    print('\\t{} {}'.format(id_words[i], d))\n",
        "\n",
        "print('Similares a ford:')\n",
        "for i, d in zip(*nearest(vectors, vectors[words_id['ford'], :])):\n",
        "    print('\\t{} {}'.format(id_words[i], d))\n",
        "\n",
        "print('Similares a law:')\n",
        "for i, d in zip(*nearest(vectors, vectors[words_id['law'], :])):\n",
        "    print('\\t{} {}'.format(id_words[i], d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1dFSqhcus9P"
      },
      "source": [
        "## ¿Cómo usar los embeddings?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Ont2PCuvp8"
      },
      "source": [
        "#Modelo pre entrenado\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDPNoQpjve68"
      },
      "source": [
        "x_train, y_train = fetch_20newsgroups(return_X_y=True, subset='train')\n",
        "x_test, y_test = fetch_20newsgroups(return_X_y=True, subset='test')\n",
        "x_train = list(map(preprocessor, x_train))\n",
        "x_test = list(map(preprocessor, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YHm0fWgD6xu"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "xp_train = tfidf.fit_transform(x_train)\n",
        "xp_test = tfidf.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsuP_c0wwDGO"
      },
      "source": [
        "cls = SVC()\n",
        "cls.fit(xp_train, y_train)\n",
        "\n",
        "print(classification_report(y_test, cls.predict(xp_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFaxMCBJyuK4"
      },
      "source": [
        "def avg_vector(x, model):\n",
        "    res = []\n",
        "    for s in tqdm(x):\n",
        "        s = tokenizer_simple(s)\n",
        "        v = []\n",
        "        for w in s:\n",
        "            if w in model:\n",
        "                v.append(model[w])\n",
        "        res.append(np.mean(np.asarray(v), axis=0))\n",
        "    return np.asarray(res)\n",
        "\n",
        "xv_train = avg_vector(x_train, model)\n",
        "xv_test = avg_vector(x_test, model)\n",
        "\n",
        "cls = SVC()\n",
        "cls.fit(xv_train, y_train)\n",
        "\n",
        "print(classification_report(y_test, cls.predict(xv_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgh2FhSU9Q4u"
      },
      "source": [
        "def tfidf_vector(x, x_tfidf, tfidf, model):\n",
        "    res = []\n",
        "    tokenizer = tfidf.build_analyzer()\n",
        "    tfidf_map = {w: i for i, w in enumerate(tfidf.get_feature_names())}\n",
        "    for i, s in tqdm(enumerate(x), total=len(x)):\n",
        "        s = tokenizer(s)\n",
        "        v = []\n",
        "        total = 0\n",
        "        for w in s:\n",
        "            if w in model:\n",
        "                if w not in tfidf_map:\n",
        "                    continue\n",
        "                j = tfidf_map[w]\n",
        "                total += x_tfidf[i, j]\n",
        "                v.append(x_tfidf[i, j] * model[w])\n",
        "        res.append(np.sum(np.asarray(v), axis=0) / total)\n",
        "    return np.asarray(res)\n",
        "\n",
        "xv_train = tfidf_vector(x_train, xp_train, tfidf, model)\n",
        "xv_test = tfidf_vector(x_test, xp_test, tfidf, model)\n",
        "\n",
        "cls = SVC()\n",
        "cls.fit(xv_train, y_train)\n",
        "\n",
        "print(classification_report(y_test, cls.predict(xv_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKQ83ZUIOccW"
      },
      "source": [
        "## Recomendación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUNTNrvJweA"
      },
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip ml-latest-small.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUwgQC26OwBX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "df = pd.read_csv('ml-latest-small/ratings.csv')\n",
        "users = df.values[:, 0].astype(np.int32)\n",
        "movies = df.values[:, 1].astype(np.int32)\n",
        "m_ids = list(set(movies))\n",
        "m_ids.sort()\n",
        "m_ids = {idx: i for i, idx in enumerate(m_ids, start=1)}\n",
        "movies = np.asarray([m_ids[i] for i in movies], dtype=np.int32)\n",
        "ratings = df.values[:, 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDHV-4RdPLkt"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "u_train, u_test, m_train, m_test, r_train, r_test = train_test_split(users, movies, ratings, random_state=42, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uha_wieUcYYw"
      },
      "source": [
        "min_r = min(r_train)\n",
        "max_r = max(r_train)\n",
        "\n",
        "r_train = (r_train - min_r) / (max_r - min_r)\n",
        "r_test = (r_test - min_r) / (max_r - min_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAVJIGEKVzja"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "iu = Input((1,), name='user_i')\n",
        "ue = Embedding(len(set(users))+1, 50, name='emb_user', embeddings_regularizer=l2(1e-6))(iu)\n",
        "ub = Embedding(len(set(users))+1, 1, name='bias_user')(iu)\n",
        "\n",
        "im = Input((1,), name='movie_i')\n",
        "me = Embedding(len(set(movies))+1,50, name='emb_movie', embeddings_regularizer=l2(1e-6))(im)\n",
        "mb = Embedding(len(set(movies))+1, 1, name='bias_movie')(im)\n",
        "\n",
        "dot = Dot(axes=-1)([ue, me])\n",
        "\n",
        "biases = Add()([dot, ub, mb])\n",
        "\n",
        "out = Activation('sigmoid')(Flatten()(biases))\n",
        "\n",
        "model = Model([iu, im], out)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['mae'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS1kCBP2ZIsL"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4imqkCDmZG_p"
      },
      "source": [
        "print(u_train[:10])\n",
        "print(m_train[:10])\n",
        "print(r_train[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a1zKPFTXPBm"
      },
      "source": [
        "model.fit([np.expand_dims(u_train, axis=-1), np.expand_dims(m_train, axis=-1)], np.expand_dims(r_train, axis=-1),\n",
        "          epochs=10, batch_size=512,\n",
        "          validation_data=([np.expand_dims(u_test, axis=-1), np.expand_dims(m_test, axis=-1)], np.expand_dims(r_test, axis=-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB6nxfP9aMka"
      },
      "source": [
        "r_pred = model.predict([np.expand_dims(u_test, axis=-1), np.expand_dims(m_test, axis=-1)])\n",
        "\n",
        "for t, p in zip(r_test[:100], r_pred[:100, 0]):\n",
        "    print(\"Real: {} Predicho: {}\".format(t * (max_r - min_r) + min_r, p* (max_r - min_r) + min_r))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMyTv6RphtHy"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Concatenate, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "iu = Input((1,), name='user_i')\n",
        "ue = Embedding(len(set(users))+1, 50, name='emb_user')(iu)\n",
        "\n",
        "im = Input((1,), name='movie_i')\n",
        "me = Embedding(len(set(movies))+1,50, name='emb_movie')(im)\n",
        "\n",
        "f = Concatenate(axis=-1)([Flatten()(ue), Flatten()(me)])\n",
        "f = Dropout(0.5)(f)\n",
        "\n",
        "d = Dense(50)(f)\n",
        "d = Dropout(0.5)(d)\n",
        "d = Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "model = Model([iu, im], d)\n",
        "\n",
        "model.compile(loss='mae', optimizer='nadam', metrics=['mae'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s4syWKWZJzd"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "from IPython.display import Image\n",
        "Image(retina=True, filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO2RexUfiWUK"
      },
      "source": [
        "model.fit([np.expand_dims(u_train, axis=-1), np.expand_dims(m_train, axis=-1)], np.expand_dims(r_train, axis=-1),\n",
        "          epochs=10, batch_size=128,\n",
        "          validation_data=([np.expand_dims(u_test, axis=-1), np.expand_dims(m_test, axis=-1)], np.expand_dims(r_test, axis=-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LiNj3m5ieVZ"
      },
      "source": [
        "r_pred = model.predict([np.expand_dims(u_test, axis=-1), np.expand_dims(m_test, axis=-1)])\n",
        "\n",
        "for t, p in zip(r_test[:100], r_pred[:100, 0]):\n",
        "    print(\"Real: {} Predicho: {}\".format(t * (max_r - min_r) + min_r, p* (max_r - min_r) + min_r))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}