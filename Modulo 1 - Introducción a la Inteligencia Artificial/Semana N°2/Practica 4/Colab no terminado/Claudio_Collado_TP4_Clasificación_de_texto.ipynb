{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Claudio Collado - TP4 - Clasificación de texto",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iKL0rYTQ1qD"
      },
      "source": [
        "#Introduccion a la Inteligencia Artificial\n",
        "\n",
        "## TP4 - Clasificacion con sklearn - Clasificacion de Texto\n",
        "\n",
        "### Autor: Collado Claudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y-V2WoOydq-"
      },
      "source": [
        "## 0. Librerias Clasicas\n",
        "\n",
        "Importo las librerias clasicas con las cuales trabajamos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Bkjf2lRyXm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYyvdKwARV3S"
      },
      "source": [
        "## 1. Dataset con el cual voy a trabajar\n",
        "\n",
        "Para la lectura de los archivos json utilizo las funcionalidades incluidas en el Notebook\n",
        "\n",
        "Importo el dataset con el cual voy a trabajar: Obtengo los vectores de features y target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQbv5Ba9u_5Q",
        "outputId": "cf1593a1-d9c5-452b-df0f-d7a43fa54e93"
      },
      "source": [
        "\n",
        "# CARGA UNO A UNO LOS JSON DESDE GDRIVE, MUY LENTO\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import glob # El modulo glob permite buscar todos los pathnames que machean con un patrón espécifico de acuerdo a las reglas de la shell de UNIX\n",
        "import json\n",
        "\n",
        "file_path = glob.glob(\"/content/gdrive/My Drive/ColabNotebooks/jsons-spam/*.json\")\n",
        "X = []\n",
        "y = []\n",
        "for file in file_path:\n",
        "    with open(file, 'r') as j:\n",
        "      json_data = json.load(j)\n",
        "    X.append(json_data['v2'])\n",
        "    y.append(json_data['v1'])\n",
        "\n",
        "print(X)\n",
        "print(y) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5jvROAz7TWR",
        "outputId": "ae7c2ef5-4e5c-4bc6-cb6a-92f0033f3ed2"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "# Montamos la unidad de Google Drive (solicitará autorización)\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Copia el zip al entorno de ejecución y los descomprime\n",
        "zip_path = \"/content/gdrive/My Drive/Colab Notebooks/jsons-spam/*.json\"\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q jsons-spam.zip\n",
        "\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "cp: cannot stat '/content/gdrive/My Drive/Colab Notebooks/jsons-spam/*.json': No such file or directory\n",
            "unzip:  cannot find or open jsons-spam.zip, jsons-spam.zip.zip or jsons-spam.zip.ZIP.\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rikIWZ-H-fIQ",
        "outputId": "ed16c276-b712-4b02-8e92-459427387ca9"
      },
      "source": [
        "import glob # El modulo glob permite buscar todos los pathnames que machean con un patrón espécifico de acuerdo a las reglas de la shell de UNIX\n",
        "import json\n",
        "\n",
        "# Busca los archivos json en el directorio jsons-spam\n",
        "file_path = glob.glob(\"/content/jsons-spam/*.json\")\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Lee cada archivo y carga la información en las listas X e y\n",
        "for file in file_path:\n",
        "    with open(file, 'r') as j:\n",
        "      json_data = json.load(j) # Carga el json\n",
        "    X.append(json_data['v2'])\n",
        "    y.append(json_data['v1'])\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s1CobtOfp_i"
      },
      "source": [
        "De esta forma tengo disponibles los feaures y los targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvu3oFuugQaU"
      },
      "source": [
        "## 3. Preprocesamiento\n",
        "\n",
        "Para el preprocesamiento de los datos voy a realizar las siguientes actividades:\n",
        "\n",
        "*   Tokenizacion\n",
        "*   Remocion de stropwords\n",
        "*   Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tau8LAOof4q2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Creamos el stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Construimos un TOKENIZER con la tokenización que provee CountVectorizer\n",
        "tokenizer = TfidfVectorizer().build_tokenizer()\n",
        "\n",
        "# Obtenemos las stopwords que provee SciKit Learn\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union('like')\n",
        "\n",
        "# Definimos una función que aplica el stemming luego de tokenizar y remover stopwords\n",
        "def  stem_tokenizer(doc):\n",
        "    # Aplica la tokenización\n",
        "    tokens = tokenizer(doc)\n",
        "    # Retorna la lista de tokens luego de aplicar stemming si no es un stopword\n",
        "    return list(stemmer.stem(w) for w in tokens if w not in stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DegffRaNf4oY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqvIktJJf4l6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quh8uGTJf4kA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUOzKahAf4hz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oyy1cbof4fX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GTqsFYSf4di"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhPvLsrdf4a2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBeEFGe4f4ZD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsDahtgXf4XZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e9sigGef4Uw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "671RHqgzf4S-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpPEzyNof4P-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHrMrttGf4Nw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoQGwSI3f4LZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV1V8A5Bc4SE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}