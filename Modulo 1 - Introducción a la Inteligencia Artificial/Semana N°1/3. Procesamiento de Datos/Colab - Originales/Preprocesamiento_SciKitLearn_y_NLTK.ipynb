{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perspectiva Práctica - Preprocesamiento NL con SciKitLearn y NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPpOGj7OKfiZ"
      },
      "source": [
        "# Preprocesamiento con CountVectorizer\n",
        "El CountVectorizer convierte una colección de documentos de texto a una matriz de recuento de tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK1g-btZKc_g"
      },
      "source": [
        "## Tokenización y representación sin otro tipo de preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StgiXDyIKRjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "f2720107-7036-422e-d49d-10daf9a4eba2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['This is the first fictitious document.', \n",
        "          'This document is the second Document documented.', \n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "features  = vectorizer.get_feature_names()\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n",
        "print(X)\n",
        "\n",
        "# Convertimos a un DataFrame (no recomendable)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(X.toarray())\n",
        "df.columns = vectorizer.get_feature_names()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'documented', 'fictitious', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "[[0 1 0 1 1 1 0 0 1 0 1]\n",
            " [0 2 1 0 0 1 0 1 1 0 1]\n",
            " [1 0 0 0 0 1 1 0 1 1 1]\n",
            " [0 1 0 0 1 1 0 0 1 0 1]]\n",
            "  (0, 10)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 1)\t1\n",
            "  (1, 10)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 1)\t2\n",
            "  (1, 7)\t1\n",
            "  (1, 2)\t1\n",
            "  (2, 10)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 6)\t1\n",
            "  (3, 10)\t1\n",
            "  (3, 5)\t1\n",
            "  (3, 8)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 1)\t1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>and</th>\n",
              "      <th>document</th>\n",
              "      <th>documented</th>\n",
              "      <th>fictitious</th>\n",
              "      <th>first</th>\n",
              "      <th>is</th>\n",
              "      <th>one</th>\n",
              "      <th>second</th>\n",
              "      <th>the</th>\n",
              "      <th>third</th>\n",
              "      <th>this</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   and  document  documented  fictitious  first  ...  one  second  the  third  this\n",
              "0    0         1           0           1      1  ...    0       0    1      0     1\n",
              "1    0         2           1           0      0  ...    0       1    1      0     1\n",
              "2    1         0           0           0      0  ...    1       0    1      1     1\n",
              "3    0         1           0           0      1  ...    0       0    1      0     1\n",
              "\n",
              "[4 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za5kLIeseJRo"
      },
      "source": [
        "**Visualización de la frecuencia de los tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLSRnuIgdb0L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "76364b57-693d-4b2a-d6e2-dd5193b21376"
      },
      "source": [
        "from yellowbrick.text import FreqDistVisualizer\n",
        "visualizer = FreqDistVisualizer(features=features, n=min(50,len(features)),orient='h')\n",
        "visualizer.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrequencyVisualizer(ax=<matplotlib.axes._subplots.AxesSubplot object at 0x7fed5f68a290>,\n",
              "                    color=None,\n",
              "                    features=['and', 'document', 'documented', 'fictitious',\n",
              "                              'first', 'is', 'one', 'second', 'the', 'third',\n",
              "                              'this'],\n",
              "                    n=None, orient='h')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAFKCAYAAAB8cXekAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de1RVdf7/8dc5h0t5ozDFyEojKy5qDQJjrbFyzExriWZajuYUNd3AplyFRX29pJFWkpnmZGWKoZaZ5mi5yktNpmBHzYWUEjkFakkXRzgl1/37o+VZoXzU+h3Y7uPz8U+HfeG833xanVef/Tl7uyzLsgQAANAIt90FAACAkxdBAQAAGBEUAACAEUEBAAAYERQAAIBRiN0FnGzq6+vl8/kUGhoql8tldzkAADQpy7JUU1Ojli1byu0+ev6AoHAEn8+nXbt22V0GAADN6qKLLlLr1q2P2k5QOEJoaKikX/9gYWFhNlcTOIWFhUpISLC7jIAKxp6k4OyLnpyBnpwh0D1VV1dr165d/s+/IxEUjnD4ckNYWJjCw8Ntriawgq0fKTh7koKzL3pyBnpyhqboyXS5ncWMAADAiKAAAACMCAoAAMCIoAAAAIwICgAAwIigAAAAjAgKAADAiKAAAACMCAoAAMCIoAAAAIxclmVZdhdxMqmqqlJhYaEGLi/WPl+N3eUAANBAwfA4JSYmBuz3Hf7cS0hIaPTW0MwoAAAAI4ICAAAwIigAAAAjggIAADAiKAAAACOCAgAAMHJ0UFi9erWWLl2qKVOmHLXvgQce0KFDh2yoCgCA4OHYoFBWVqaVK1ca9+fk5Oi0005rxooAAAg+jg0KEydOVEFBgfbu3av9+/crIyND1113nZYsWSJJ6t27t3w+nz7++GMNGTJEI0aM0P3336+aGm6iBADAiXJsUEhLS1NycrKio6NVWlqq5557TjNnzlRubm6D4xYsWKCxY8dqwYIFGjBggA4cOGBTxQAAOI9jg8Jvde/eXR6PR1FRUaqoqGiwr1+/fho3bpxmz56t2NhYtWvXzqYqAQBwnqAICiEhIcZ9qampmj9/vs4880zdc889KikpacbKAABwNscGBbfbrdra2uMeN3PmTIWEhGjYsGHq378/QQEAgN/B/L/iJ7mYmBgVFRVp8+bNGjJkiPG46Oho3XbbbWrTpo3atGmj2267rRmrBADA2XjM9BF4zDQA4GTGY6YBAMBJg6AAAACMCAoAAMCIoAAAAIwICgAAwMixX49saiVZgxpd/elUXq83oKtkTwbB2JMUnH3RkzPQkzN4vd5mfT9mFAAAgBFBAQAAGBEUAACAEUEBAAAYcQvnI3ALZwDAyYxbOAMAgJMGQQEAABgRFAAAgBFBAQAAGBEUAACAUVDcwnn16tXy+XwqLi5WZmam3eUAABA0HD+jUFZWppUrV9pdBgAAQcnxQWHixIkqKCjQ3r17tX//fmVkZOi6667TkiVLJEmffvqphg8frltvvVWZmZmqrq62uWIAAJzD8UEhLS1NycnJio6OVmlpqZ577jnNnDlTubm5kqRJkyZp1qxZmj9/vtq2bav33nvP5ooBAHCOoFijcFj37t3l8XgUFRWliooKff/99/r666+VkZEhSfr555915pln2lwlAADOEVRBISSkYTuhoaFq3769f3YBAAD8Po6/9OB2u1VbW9vovoiICEnSl19+KUnKzc3VF1980Wy1AQDgdI6fUYiJiVFRUZE2b96sIUOGHLV/8uTJeuSRR/yzC8OGDbOhSgAAnMnxQSEyMlLr169vsK1ly5Zau3atJKlHjx568803bagMAADnc/ylBwAA0HQICgAAwIigAAAAjAgKAADAiKAAAACMHP+th6ZSkjVI4eHhdpcRMF6vV4mJiXaXEVDB2JMUnH3RkzPQkzN4vd5mfT9mFAAAgBFBAQAAGBEUAACAEUEBAAAYuSzLsuwu4mRSVVWlwsJCDVxerH2+GrvLAQCggYLhcQFdoHn4cy8hIaHRRfzMKAAAACOCAgAAMCIoAAAAI4ICAAAwIigAAACjoA4KH330kfLy8uwuAwAAxwrqZz306tXL7hIAAHC0oA4KS5cu1a5du/Ttt9+qvLxc1dXVysjIIEAAAHCCgjooSFJRUZFcLpdef/11HTx4UB9++KHdJQEA4BhBvUZBkmJjY+Xz+fTQQw9p06ZNGjBggN0lAQDgGEEfFNxut9544w0NGzZMH374obKysuwuCQAAxwj6oLBjxw6tWLFCPXr00Pjx41VSUmJ3SQAAOEbQr1Ho2LGj3nnnHS1evFgej0dpaWl2lwQAgGMEdVAYPHiwBg8ebHcZAAA4VtBfegAAAH8cQQEAABgRFAAAgBFBAQAAGBEUAACAUVB/6+H/R0nWIIWHh9tdRsB4vV4lJibaXUZABWNPUnD2RU/OQE/O4PV6m/X9mFEAAABGBAUAAGBEUAAAAEYEBQAAYOSyLMuyu4iTSVVVlQoLCzVwebH2+WrsLgcAgAYKhscFdIHm4c+9hISERhfxM6MAAACMCAoAAMCIoAAAAIwICgAAwIigAAAAjJosKPh8PvXu3bupfv0fUllZqY8//tjuMgAAcIxTakZhx44d2rBhg91lAADgGAF9KFRlZaUyMjJUVVXl/45nfn6+cnJyFBISoqioKGVnZ8vlcmns2LHas2ePwsPDNXXqVG3YsEHFxcXKzMyUz+fTDTfcoLVr16pPnz4aOnSo3nvvPZ1//vmKj4/3v3722Wf13XffKSsrSzU1NfJ4PJo0aZKio6N1zTXXqE+fPtqyZYtat26tl156SRMnTlRlZaU6deqkYcOGBbJ1AACCUkBnFJYvX64uXbooLy9PsbGxkqRx48YpJydHCxYsUEREhFasWKFly5bprLPO0qJFizR06FCtWbPG+Dvr6+sVFxent956S1u2bNE555yjJUuWyOv16uDBg5o+fbpuv/12zZs3T6NGjdKsWbMkSaWlpRo4cKAWL16sgwcPaufOnUpLS1P//v0JCQAAnKCAziiUlJQoKSlJkpScnKwDBw4oKipKZ599tiQpJSVFmzdvVm1trXr27ClJGjBggCRp6dKlxt/brVs3uVwutW3bVnFxcZKkyMhIVVRUaOvWrdq9e7defPFF1dXVKTIyUpLUqlUrXXLJJZKkDh06qKKiIpCtAgBwSghoULAsS273r5MU9fX1crlc+u0domtqauRyueTxeFRfX9/gXJfL5X9dW1vbYJ/H42n0tWVZCg0N1fTp09W+fXvjOYePBQAAv09ALz107txZhYWFkn5dmxARESGXy6W9e/dKkgoKCpSQkKCuXbtq06ZNkqR169Zp9uzZatWqlfbv3y9J8nq9J/ye3bt31wcffCBJ2rhxo1asWGE81u12HxVCAACAWUCDQmpqqrZt26ZRo0Zp9+7dkqQnnnhCY8aM0ciRI1VbW6sBAwaof//++uWXXzRixAjNmzdPgwYNUs+ePbV7926NHDlSX331VYMZhmNJT0/XmjVr9Le//U0zZ87UpZdeajw2Li5O7777rl555ZWA9AsAQLDj6ZFH4OmRAICTGU+PBAAAJw2CAgAAMCIoAAAAI4ICAAAwIigAAACjgN5wKZiUZA1qdPWnU3m93oCukj0ZBGNPUnD2RU/OQE/O8HvuNRQIzCgAAAAjggIAADAiKAAAACOCAgAAMGIxo0HM5LeD7xbOeUV2VxB4wdiTFHR9FQyPs7sEAH8QMwoAAMCIoAAAAIwICgAAwIigAAAAjAgKAADAiKAAAACMHBsUampqdNNNN+nqq6/W+++/f0LnrFmzRtXV1U1cGQAAwcOx91EoLy9XdXW11q1bd8LnvPbaa/rzn/+ssLCwJqwMAIDg4digkJ2drW+++UaPPPKI4uPj1aVLF7366qv6+eeflZmZqWXLlqmwsFB1dXW65ZZb5Ha7tW3bNt1555167bXXCAsAAJwAx156yMzMVOfOnRUdHe3ftmvXLr3yyivq2LGj1q9fr0WLFikvL0+1tbVKTU1Vu3btNGfOHEICAAAnyLEzCo25+OKLFRYWprCwMHXq1En33HOP+vXrp9TUVLtLAwDAkRw7o9CY384UvPzyy0pPT9cXX3yhu+++28aqAABwrqAKCoeVlZVp/vz5io+PV2Zmpg4cOCBJcrlcqqurs7k6AACcI6guPRzWvn17bd26VatWrVJoaKhuvPFGSVJycrKGDx+u+fPnKzIy0uYqAQA4+Tk2KHTs2FFLly5tsC0lJUXSr5cgcnJyjjonOzu7WWoDACBYBOWlBwAAEBgEBQAAYERQAAAARgQFAABg5NjFjE2tJGuQwsPD7S4jYLxerxITE+0uI6CCsScpOPvyer12lwDgD2JGAQAAGBEUAACAEUEBAAAYERQAAIARixkNYia/rX2+GrvLCKy8IrsrCKiC4XF2lwAAQY8ZBQAAYERQAAAARgQFAABgRFAAAABGBAUAAGBEUAAAAEaOCwqrV6/W0qVLNWXKlKP2PfDAAzp06JDxXJ/Pp969ezdleQAABBVHBYWysjKtXLnSuD8nJ0ennXZaM1YEAEBwc9QNlyZOnKjt27froosu0v79+5WRkaEvv/xSaWlpGjJkiHr37q0VK1boiSeeUGhoqA4cOKDs7GxlZGSoqqoq6J7IBwBAU3PUjEJaWpqSk5MVHR2t0tJSPffcc5o5c6Zyc3OPOjYiIkIzZszQ8uXL1aVLF+Xl5Sk2NtaGqgEAcC5HBYXf6t69uzwej6KiolRRUXHU/m7dukmSSkpKdNlll0mSkpOTm7VGAACczrFBISTk2FdNQkNDJUmWZcnt/rXN+vr6Jq8LAIBg4qig4Ha7VVtb+7vO6dy5swoLCyVJ+fn5TVEWAABBy1FBISYmRkVFRcrOzj7hc1JTU7Vt2zaNGjVKu3fvbsLqAAAIPo761kNkZKTWr1/fYFvLli21du1aSfL/86mnnvLvb9OmTYPFjqNHj276QgEACBKOmlEAAADNi6AAAACMCAoAAMCIoAAAAIwctZixOZVkDVJ4eLjdZQSM1+sNultYe71eu0sAgKDHjAIAADAiKAAAACOCAgAAMCIoAAAAIxYzGsRMflv7fDV2lxFYeUV2VxBQBcPj7C4BAIIeMwoAAMCIoAAAAIwICgAAwIigAAAAjAgKAADAiKAAAACMTqmgsGDBAs2YMcPuMgAAcIxTKigAAIDfx7YbLu3du1cPPfSQ3G636urq9PTTT2vmzJkqLS1VbW2tRo8erZ49e6qoqEgTJkyQy+XSZZddpszMTO3cuVMTJ06U2+1Wy5Yt9dRTT2nnzp16/fXX5XK59NVXX+naa69Venq6Nm7cqCeffFJnnXWW2rVrp3PPPdeulgEAcBzbZhRWr16tyy+/XLm5ucrKytKyZcvUrl075ebmaubMmXryySclSZMmTdKECRO0aNEi/fDDD9qzZ48mT56shx9+WLm5uUpKStL8+fMlSdu3b9dTTz2lRYsWKTc3V5L07LPP6umnn9bcuXP1008/2dUuAACOZNuMwhVXXKH09HRVVFTo2muv1f79++X1erVlyxZJUlVVlaqrq7V7925dcsklkqSpU6dKkkpKStS9e3dJUkpKil544QWlpKQoLi5Op59+eoP32bNnj//8pKQkVVVVNVeLAAA4nm1B4aKLLtLy5cu1YcMGTZs2TXv27NGDDz6o66+/vsFxbvexJz1qamr8x4SEHN3Ob8+3LCsAlQMAcOqw7dLDypUrVVxcrD59+uj+++9XaGio1qxZI0n64YcfNG3aNElSTEyMPvvsM0nSo48+qpKSEnXp0kVbt26VJG3evFkJCQnG94mKitJXX30ly7JUUFDQxF0BABBcbJtR6NSpk8aNG6cWLVrI4/Ho+eef1/z583XzzTerrq5O6enpkqSsrCyNHz9eknTppZcqJiZGjz32mH+BY0REhLKzs7Vjx45G3+ef//yn7r//fkVHR6tDhw7N1R4AAEHBtqAQHx+vJUuWNNg2efLko467+OKLtXDhwgbbLrzwQv9ixcNSUlKUkpLi/zk/P1+S1KtXL/Xq1StQZQMAcErhPgoAAMCIoAAAAIwICgAAwIigAAAAjAgKAADAyLZvPZzsSrIGKTw83O4yAsbr9SoxMdHuMgLK6/XaXQIABD1mFAAAgBFBAQAAGBEUAACAEUEBAAAYsZjRIGby29rnq7G7jMDKK7K7goAqGB5ndwkAEPSYUQAAAEYEBQAAYERQAAAARgQFAABgRFAAAABGBAUAAGBEUAAAAEaOvY9CTU2N/u///k+lpaWqrq7W6NGjNWHCBA0bNkzr1q1TdXW15s6dq9NPP12PP/64SktLVVtbq9GjR6tnz552lw8AgCM4NiisXLlSYWFhWrBggb777jvdeuutqqur0wUXXKA77rhDDzzwgDZt2qTKykq1a9dOTz75pH788UeNGjVKK1assLt8AAAcwbFBobCwUCkpKZKkqKgohYWFqby8XD169JAkdejQQRUVFdq2bZu8Xq+2bNkiSaqqqlJ1dbXCwsJsqx0AAKdwbFCQJMuy/K+rq6vldrvl8Xga7A8NDdXdd9+t66+/3o4SAQBwNMcuZuzatavy8/MlSfv27ZPb7VabNm2OOq579+5as2aNJOmHH37QtGnTmrVOAACczLFBYcCAAaqrq9PIkSP1wAMPaOLEiY0ed91116lFixa6+eabdffddysxMbGZKwUAwLkce+khJCREkydPbrBt7dq1/teZmZn+10ceBwAAToxjZxQAAEDTIygAAAAjggIAADAiKAAAACOCAgAAMHLstx6aWknWIIWHh9tdRsB4vd6g+2qo1+u1uwQACHrMKAAAACOCAgAAMCIoAAAAI4ICAAAwYjGjQczkt7XPV2N3GYGVV2R3BQFVMDzO7hIAIOgxowAAAIwICgAAwIigAAAAjAgKAADAiKAAAACMmi0o1NTU6KabbtLVV1+t999/33jce++9J0n66KOPlJeX12Db559/rueff77piwUAAJKa8euR5eXlqq6u1rp164zHVFdX67XXXlO/fv3Uq1cv//aXXnpJ/fr1U2xsrGJjY5ujXAAAoGYMCtnZ2frmm2/0yCOPKD4+XiNGjNCkSZO0fft2eTweTZgwQQsXLtTOnTs1fvx4devWTcXFxWrbtq127typ9PR0jRw5Uq+//rqef/55rVq1Sq+99po8Ho/i4+P12GOPacaMGTrzzDM1YsQI7dq1S0888YRyc3M1adIkFRYWqq6uTrfccosGDx7cXG0DAOBozXbpITMzU507d1Z0dLQk6ZNPPtG3336rN954Qw8++KBWrVqltLQ0de7cWePHj/efd8cdd6hVq1Z64YUX/Nt8Pp9ycnI0d+5cLVy4UGVlZdq0aVOj73vgwAGtX79eixYtUl5enmpra5u0TwAAgoltd2bcsWOH/vSnP0mSkpKSlJSUpLKyshM697///a/OP/98tWzZUpKUnJyszz//vNFjzzjjDHXq1En33HOP+vXrp9TU1MA0AADAKcC2bz14PB7V19f/oXNdLpcsy/L/XFNTI5fLJZfL5d/225mDl19+Wenp6friiy909913//GiAQA4xdgWFLp27ar8/HxJUlFRkSZMmCC32626urqjjv1tKJCkTp066euvv1ZlZaUkqaCgQAkJCWrVqpXKy8slSV6vV5JUVlam+fPnKz4+XpmZmTpw4EBTtgUAQFCx7dJDUlKS1qxZo+HDh0uSxo0bp3bt2qmmpkajR4/WVVdd5T82NjZWQ4YM0UMPPSRJatGihR5++GHdcccdcrvdSkxMVI8ePXT22Wfrrrvu0vbt29WjRw9JUvv27bV161atWrVKoaGhuvHGG5u9VwAAnKrZgkLHjh21dOnSBtvGjh171HGrVq06atu8efP8r1NSUiRJffv2Vd++fRscd8455+jf//63/+f77rtPkpSTk/PHCwcA4BTGnRkBAIARQQEAABgRFAAAgBFBAQAAGBEUAACAkW1fjzzZlWQNUnh4uN1lBIzX61ViYqLdZQTU4XtlAACaDjMKAADAiKAAAACMCAoAAMCIoAAAAIxYzGgQM/lt7fPV2F1GYOUV2V1BQBUMj7O7BAAIeswoAAAAI4ICAAAwIigAAAAjggIAADAiKAAAAKPfFRR8Pp969+7dVLX8IZWVlfr4449P+PjRo0crPz+/CSsCACB4OH5GYceOHdqwYYPdZQAAEJSOex+FyspKZWRkqKqqyv9Qofz8fOXk5CgkJERRUVHKzs6Wy+XS2LFjtWfPHoWHh2vq1KnasGGDiouLlZmZKZ/PpxtuuEFr165Vnz59NHToUL333ns6//zzFR8f73/97LPP6rvvvlNWVpZqamrk8Xg0adIkRUdH65prrlGfPn20ZcsWtW7dWi+99JImTpyoyspKderUSVdddVWj582ZM0crV65UdHS0Kisrm/yPCgBAsDjujMLy5cvVpUsX5eXlKTY2VpI0btw45eTkaMGCBYqIiNCKFSu0bNkynXXWWVq0aJGGDh2qNWvWGH9nfX294uLi9NZbb2nLli0655xztGTJEnm9Xh08eFDTp0/X7bffrnnz5mnUqFGaNWuWJKm0tFQDBw7U4sWLdfDgQe3cuVNpaWnq37+/hg0b1uh5Bw8e1MKFC7V48WJNnTpVxcXFAfrTAQAQ/I47o1BSUqKkpCRJUnJysg4cOKCoqCidffbZkqSUlBRt3rxZtbW16tmzpyRpwIABkqSlS5caf2+3bt3kcrnUtm1bxcX9eoe9yMhIVVRUaOvWrdq9e7defPFF1dXVKTIyUpLUqlUrXXLJJZKkDh06qKKiosHvbOy8r7/+WhdeeKHCw8MVHh6u+Pj43/UHAgDgVHbcoGBZltzuXyce6uvr5XK5ZFmWf39NTY1cLpc8Ho/q6+sbnOtyufyva2trG+zzeDyNvrYsS6GhoZo+fbrat29vPOfwsb/V2Hnbt2/319/YOQAAwOy4lx46d+6swsJCSb+uTYiIiJDL5dLevXslSQUFBUpISFDXrl21adMmSdK6des0e/ZstWrVSvv375ckeb3eEy6qe/fu+uCDDyRJGzdu1IoVK8wNuN3+ENLYeeedd55KSkpUXV2tyspKfy8AAOD4jjujkJqaqvvuu0+jRo3yL2Z84oknNGbMGIWEhOjcc8/VgAEDVF9fr08++UQjRoxQSEiIpkyZopYtW+rFF1/UyJEjdeWVVzaYYTiW9PR0Pfroo1q5cqVcLpeys7ONx8bFxemZZ55Rhw4dGj3vjDPOUGpqqm6++WZ17NhRXbt2PcE/DQAAcFnMxTdQVVWlwsJCDVxeHHxPjwwyBcPj/OE1mHi93qDri56cgZ6cIdA9Hf7cS0hIUHh4+FH7HX8fBQAA0HQICgAAwIigAAAAjAgKAADAiKAAAACMjvv1yFNVSdagRld/OlWwrvwFADQtZhQAAIARQQEAABgRFAAAgBFBAQAAGLGY0SBm8tvBdwvnvCK7KwioguFxdpcAAEGPGQUAAGBEUAAAAEYEBQAAYERQAAAARgQFAABgRFAAAABGp1RQ8Pl86t27t91lAADgGKdUUAAAAL+PY2+4VFlZqTFjxujnn3/WoUOH9Pjjj2vMmDEaNmyY1q1bp+rqas2dO1eSlJGRoaqqqqB7eiIAAE3NsTMK5eXluummm5Sbm6sHH3xQc+bMUV1dnS644AK9/vrr6tixozZt2qTly5erS5cuysvLU2xsrN1lAwDgKI6dUTjrrLM0a9YsvfLKK6qurlaLFi0kST169JAkdejQQRUVFSopKVFSUpIkKTk52bZ6AQBwIsfOKMybN09RUVFauHChxo8f79/u8Xj8ry3LkmVZcrt/bbO+vr65ywQAwNEcGxR++uknnXfeeZKkDz74QDU1jT/AqXPnziosLJQk5efnN1t9AAAEA8cGhYEDB2ru3Lm6/fbb1a1bN5WXl8uyrKOOS01N1bZt2zRq1Cjt3r3bhkoBAHAux65R6Natm959913/z3/9618b7M/MzPS/zs3N9b8ePXp00xcHAECQcOyMAgAAaHoEBQAAYERQAAAARgQFAABg5NjFjE2tJGuQwsPD7S4jYLxeb9Ddwtrr9dpdAgAEPWYUAACAEUEBAAAYERQAAIARQQEAABgRFAAAgBFBAQAAGBEUAACAEUEBAAAYERQAAIARQQEAABgRFAAAgBFBAQAAGPFQqCNYliVJqq6utrmSwKuqqrK7hIALxp6k4OyLnpyBnpwhkD0d/rw7/Pl3JJdl2nOKqqio0K5du+wuAwCAZnXRRRepdevWR20nKByhvr5ePp9PoaGhcrlcdpcDAECTsixLNTU1atmypdzuo1ckEBQAAIARixkBAIARQQEAABgRFAAAgBFBAQAAGJ3S91F48skn9dlnn8nlcunRRx9Vt27d/Ps++eQTTZs2TR6PR7169dJ9991nY6W/z7H66t27tzp06CCPxyNJeuaZZxQVFWVXqSds165duvfee/X3v/9dI0aMaLDPqWN1rJ6cOk5Tp06V1+tVbW2t7rrrLvXt29e/z6njdKyenDhOv/zyi8aOHasffvhBVVVVuvfee3X11Vf79ztxnI7XkxPH6bBDhw7p+uuv17333qvBgwf7tzfrOFmnqPz8fOsf//iHZVmW9eWXX1pDhw5tsP+6666z9u7da9XV1Vm33HKLVVxcbEeZv9vx+rr66qutyspKO0r7w3w+nzVixAjrscces3Jzc4/a78SxOl5PThynjRs3WnfccYdlWZb1448/WldeeWWD/U4cp+P15MRxWrlypfXSSy9ZlmVZZWVlVt++fRvsd+I4Ha8nJ47TYdOmTbMGDx5svfXWWw22N+c4nbKXHjZu3Kg+ffpIkmJiYvS///1PlZWVkqTS0lJFRETo7LPPltvt1pVXXqmNGzfaWe4JO1ZfThUWFqY5c+aoffv2R+1z6lgdqyenSkpK0vTp0yVJbdq00S+//KK6ujpJzh2nY/XkVP3799edd94pSdq3b1+D/7N26jgdqycnKykp0ZdffqmrrrqqwfbmHqdT9tLD999/r/j4eP/PkZGRKi8vV6tWrVReXq7IyMgG+0pLS+0o83c7Vl+HjRs3Tnv27FFiYoNLC88AAALkSURBVKLGjBlz0t9YKiQkRCEhjf+r6tSxOlZPhzltnDwej1q0aCFJWrJkiXr16uWf6nXqOB2rp8OcNk6H3Xzzzfr22281e/Zs/zanjtNhjfV0mBPHacqUKXr88ce1bNmyBtube5xO2aBwJCtI7zt1ZF+jR4/WX/7yF0VEROi+++7T6tWr1a9fP5uqg4mTx+mDDz7QkiVL9Oqrr9pdSsCYenLyOC1atEiff/65HnroIb3zzjuO+OA8HlNPThynZcuW6dJLL9W5555rdymn7rce2rdvr++//97/8/79+9WuXbtG93333XeOmSI+Vl+SlJqaqrZt2yokJES9evVy/HMtnDxWx+LUcfrPf/6j2bNna86cOQ3uGe/kcTL1JDlznAoLC7Vv3z5JUmxsrOrq6vTjjz9Kcu44HasnyZnjtH79eq1Zs0ZDhw7Vm2++qVmzZumTTz6R1PzjdMoGhSuuuEKrV6+WJO3YsUPt27f3T8937NhRlZWVKisrU21trdatW6crrrjCznJP2LH6qqioUFpamv9JYZs3b1aXLl1sqzUQnDxWJk4dp4qKCk2dOlX/+te/dMYZZzTY59RxOlZPTh2nTz/91D8z8v333+vnn3/WmWeeKcm543Ssnpw6Ts8995zeeustvfHGG7rpppt077336vLLL5fU/ON0Sj/r4ZlnntGnn34ql8ulcePGqaioSK1bt9Y111yjzZs365lnnpEk9e3bV2lpaTZXe+KO1de8efO0bNkyhYeHKy4uTo8//vhJP+VYWFioKVOmaM+ePQoJCVFUVJR69+6tjh07OnasjteTE8dp8eLFmjFjhjp37uzflpKSoosvvtix43S8npw4TocOHVJWVpb27dunQ4cOKT09XQcOHHD0f/uO15MTx+m3ZsyYoXPOOUeSbBmnUzooAACAYztlLz0AAIDjIygAAAAjggIAADAiKAAAACOCAgAAMCIoAAAAI4ICAAAwIigAAACj/wfLfLwy39PjNwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkoUvjxTMDEY"
      },
      "source": [
        "## Tokenización con remoción de stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-2xeATDMJ_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b29b9d-4246-4b0c-84bd-798cb7af9b1e"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['This is the first fictitious document.', \n",
        "          'This document is the second Document documented.', \n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words=\"english\") # Tambien es posible pasar una lista propia [\"is\",\"the\"]\n",
        "print(vectorizer.get_stop_words())\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "features  = vectorizer.get_feature_names()\n",
        "\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozenset({'further', 'yourself', 'will', 'over', 'yet', 'top', 'meanwhile', 'namely', 'neither', 'another', 'thence', 'being', 'less', 'whereafter', 'due', 'find', 'two', 'almost', 'any', 'give', 'even', 'latterly', 'from', 'ourselves', 'has', 'he', 'thick', 'alone', 'that', 'four', 'part', 'during', 'via', 'once', 'keep', 'however', 'can', 'nowhere', 'whole', 'each', 'cannot', 'fill', 'becomes', 'first', 'toward', 'together', 'itself', 'though', 'something', 'nothing', 'go', 'these', 'ours', 'while', 'system', 'thin', 'back', 'under', 'not', 'interest', 'might', 'noone', 'what', 'wherever', 'hereafter', 'for', 'nevertheless', 'call', 'thru', 'and', 'done', 'thereafter', 'empty', 'many', 'seemed', 'put', 'is', 'mill', 'those', 'perhaps', 'never', 'had', 'whenever', 'also', 'own', 'anyhow', 'whoever', 'fifty', 'yours', 'was', 'get', 'if', 'where', 'i', 'hers', 'at', 'until', 'a', 'amoungst', 'on', 'you', 'except', 'amongst', 'an', 'eleven', 'made', 'such', 'front', 'besides', 'five', 'beyond', 'indeed', 'our', 'either', 'by', 'etc', 'up', 'thereby', 'off', 'fifteen', 'should', 'been', 'herself', 'of', 'please', 'in', 'myself', 'else', 'after', 'ever', 'through', 'since', 'must', 'rather', 'who', 'beside', 'her', 'whether', 'we', 'herein', 'out', 'may', 'nine', 'six', 'co', 'formerly', 'former', 'there', 'why', 'yourselves', 'most', 'seem', 'into', 'above', 'couldnt', 'us', 'moreover', 'they', 'de', 'inc', 'latter', 'than', 'whereas', 'per', 'hasnt', 'down', 'their', 'again', 'bottom', 'both', 'were', 'thus', 'along', 'anywhere', 'because', 'next', 'twelve', 'ltd', 'themselves', 'three', 'beforehand', 'every', 'cant', 'have', 'around', 'found', 'few', 'still', 'would', 'eg', 'no', 'whence', 'do', 'she', 'but', 'his', 'someone', 'fire', 'others', 'full', 'show', 'whom', 'none', 'other', 'them', 'nor', 'are', 'whither', 'describe', 'against', 'between', 'take', 'ten', 'behind', 'hereby', 'therein', 'third', 'seems', 'see', 'anyone', 'then', 'whereupon', 'towards', 'everywhere', 'ie', 'wherein', 'sixty', 'everything', 'very', 'him', 'more', 'often', 'too', 'my', 'hundred', 'himself', 'or', 'it', 'twenty', 'everyone', 'amount', 'bill', 'how', 'became', 'much', 'least', 'thereupon', 'hereupon', 'several', 'am', 'so', 'with', 'last', 'mostly', 'name', 'side', 'could', 'this', 'sincere', 'which', 'sometime', 'otherwise', 'about', 'same', 'somehow', 'one', 'mine', 'onto', 'whatever', 'elsewhere', 'as', 'some', 'nobody', 'somewhere', 'cry', 'anyway', 'whereby', 'become', 'con', 'sometimes', 'move', 'to', 'already', 'among', 'well', 'serious', 'across', 'all', 'hence', 'forty', 'its', 'only', 're', 'although', 'anything', 'seeming', 'enough', 'always', 'un', 'upon', 'detail', 'now', 'eight', 'whose', 'here', 'before', 'afterwards', 'me', 'therefore', 'when', 'throughout', 'without', 'the', 'be', 'becoming', 'your', 'within', 'below'})\n",
            "['document', 'documented', 'fictitious', 'second']\n",
            "[[1 0 1 0]\n",
            " [2 1 0 1]\n",
            " [0 0 0 0]\n",
            " [1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4tcRGpLf_Eu"
      },
      "source": [
        "**Visualización de la frecuencia de los tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hPrDKqJf_ZT"
      },
      "source": [
        "from yellowbrick.text import FreqDistVisualizer\n",
        "visualizer = FreqDistVisualizer(features=features, n=min(50,len(features)),orient='h')\n",
        "visualizer.fit(X)\n",
        "visualizer.poof()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEknmoNQdedm"
      },
      "source": [
        "## Tokenización con remoción de stopwords agregando stopwords adicionales a los provistos por SciKit Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKV-pZPVdS6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445465c1-ed3c-423b-ec12-d6ab8ecd7090"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['This is the first fictitious document.', \n",
        "          'This document is the second Document documented.', \n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "\n",
        "\n",
        "# Agregamos stopwords a la lista predefinida en Scikit Learn \n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(['second'])\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words=stop_words)\n",
        "\n",
        "print(vectorizer.get_stop_words())\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "features  = vectorizer.get_feature_names()\n",
        "\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozenset({'ltd', 'further', 'yourself', 'themselves', 'will', 'over', 'three', 'yet', 'top', 'beforehand', 'meanwhile', 'every', 'namely', 'neither', 'cant', 'have', 'another', 'around', 'found', 'few', 'thence', 'still', 'being', 'less', 'whereafter', 'would', 'due', 'find', 'two', 'eg', 'no', 'almost', 'any', 'whence', 'do', 'give', 'even', 'latterly', 'from', 'ourselves', 'she', 'but', 'has', 'his', 'he', 'someone', 'thick', 'alone', 'fire', 'that', 'four', 'part', 'others', 'during', 'via', 'once', 'keep', 'full', 'show', 'however', 'whom', 'can', 'none', 'nowhere', 'other', 'whole', 'them', 'each', 'nor', 'cannot', 'fill', 'are', 'whither', 'becomes', 'first', 'describe', 'toward', 'together', 'against', 'itself', 'between', 'take', 'ten', 'behind', 'though', 'something', 'hereby', 'therein', 'nothing', 'third', 'seems', 'go', 'see', 'these', 'ours', 'anyone', 'while', 'then', 'system', 'whereupon', 'towards', 'everywhere', 'ie', 'thin', 'wherein', 'sixty', 'everything', 'back', 'under', 'not', 'interest', 'very', 'him', 'more', 'might', 'noone', 'what', 'wherever', 'often', 'hereafter', 'for', 'nevertheless', 'too', 'call', 'thru', 'my', 'hundred', 'himself', 'or', 'and', 'it', 'twenty', 'done', 'everyone', 'thereafter', 'amount', 'empty', 'many', 'bill', 'how', 'seemed', 'became', 'put', 'is', 'much', 'least', 'thereupon', 'hereupon', 'several', 'mill', 'those', 'am', 'perhaps', 'never', 'so', 'had', 'whenever', 'with', 'also', 'own', 'last', 'anyhow', 'whoever', 'mostly', 'fifty', 'name', 'yours', 'was', 'get', 'if', 'where', 'i', 'hers', 'side', 'could', 'this', 'at', 'until', 'a', 'sincere', 'amoungst', 'on', 'you', 'except', 'amongst', 'an', 'which', 'eleven', 'made', 'sometime', 'otherwise', 'such', 'about', 'same', 'front', 'somehow', 'besides', 'one', 'mine', 'five', 'beyond', 'indeed', 'our', 'either', 'onto', 'whatever', 'elsewhere', 'as', 'by', 'etc', 'some', 'up', 'thereby', 'nobody', 'off', 'fifteen', 'should', 'somewhere', 'cry', 'been', 'herself', 'of', 'please', 'anyway', 'in', 'myself', 'else', 'whereby', 'become', 'con', 'sometimes', 'move', 'after', 'ever', 'through', 'to', 'already', 'since', 'must', 'among', 'rather', 'who', 'well', 'beside', 'serious', 'across', 'her', 'whether', 'we', 'herein', 'out', 'may', 'nine', 'six', 'all', 'hence', 'forty', 'its', 'co', 'formerly', 'former', 'there', 'only', 're', 'why', 'although', 'anything', 'seeming', 'enough', 'yourselves', 'always', 'un', 'upon', 'detail', 'most', 'now', 'eight', 'whose', 'here', 'before', 'seem', 'into', 'above', 'couldnt', 'us', 'afterwards', 'moreover', 'me', 'they', 'de', 'therefore', 'inc', 'latter', 'than', 'whereas', 'per', 'hasnt', 'down', 'their', 'when', 'again', 'bottom', 'throughout', 'both', 'without', 'were', 'the', 'thus', 'along', 'second', 'be', 'anywhere', 'becoming', 'your', 'because', 'within', 'next', 'below', 'twelve'})\n",
            "['document', 'documented', 'fictitious']\n",
            "[[1 0 1]\n",
            " [2 1 0]\n",
            " [0 0 0]\n",
            " [1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSmuakfbgk9P"
      },
      "source": [
        "**Visualización de la frecuencia de los tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMrJA4gkgqw4"
      },
      "source": [
        "from yellowbrick.text import FreqDistVisualizer\n",
        "visualizer = FreqDistVisualizer(features=features, n=min(50,len(features)),orient='h')\n",
        "visualizer.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDmKtwvHOa-7"
      },
      "source": [
        "## Tokenización con remoción de stopwords y n-grams\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygNtY3itOhoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfba451-498b-43b7-e7af-155b2ecdcc64"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['This is the first fictitious document.', \n",
        "          'This document is the second Document documented.', \n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(1, 3))\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "features  = vectorizer.get_feature_names()\n",
        "\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['document', 'document documented', 'document second', 'document second document', 'documented', 'fictitious', 'fictitious document', 'second', 'second document', 'second document documented']\n",
            "[[1 0 0 0 0 1 1 0 0 0]\n",
            " [2 1 1 1 1 0 0 1 1 1]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "088_9chGkQ7V"
      },
      "source": [
        "from yellowbrick.text import FreqDistVisualizer\n",
        "visualizer = FreqDistVisualizer(features=features, n=min(50,len(features)),orient='h')\n",
        "visualizer.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h4plAeoQ9Bm"
      },
      "source": [
        "## Tokenización con remoción de stopwords + stemming con NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MRLaK8yRFz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e93ab5-9072-4679-82fe-f77d40583b2f"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['This is the first fictitious document.', \n",
        "          'This document is the second Document documented.', \n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "# Creamos el stemmer\n",
        "stemmer = PorterStemmer()\n",
        "# Construimos un analyzer con el preprocesamiento que si provee CountVectorizer\n",
        "analyzer = CountVectorizer(stop_words='english').build_analyzer()\n",
        "\n",
        "# Definimos una función que suma el stemming al preprocesamiento que provee el analyzer\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer utilizando la función como analyzer\n",
        "vectorizer = CountVectorizer(analyzer=stemmed_words, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "features  = vectorizer.get_feature_names()\n",
        "\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['document', 'fictiti', 'second']\n",
            "[[1 1 0]\n",
            " [3 0 1]\n",
            " [0 0 0]\n",
            " [1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWVgE1ISktP2"
      },
      "source": [
        "from yellowbrick.text import FreqDistVisualizer\n",
        "visualizer = FreqDistVisualizer(features=features, n=min(50,len(features)),orient='h')\n",
        "visualizer.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fftj9GQ8ghi"
      },
      "source": [
        "##Tokenización con remoción de stopwords y n-grams + stemming con NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsOgbSFW8xIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442034d8-ad3a-4ced-ea9e-1e310a67bfb6"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['This is the first fictitious document.', \n",
        "          'This document is the second Document documented.', \n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "\n",
        "# Creamos el stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Construimos un TOKENIZER con la tokenización que provee CountVectorizer\n",
        "tokenizer = CountVectorizer().build_tokenizer()\n",
        "\n",
        "# Obtenemos las stopwords que provee SciKit Learn\n",
        "stop_words = text.ENGLISH_STOP_WORDS\n",
        "\n",
        "# Definimos una función que aplica el stemming luego de tokenizar y remover stopwords\n",
        "def stem_tokenizer(doc):\n",
        "    # Aplica la tokenización\n",
        "    tokens = tokenizer(doc)\n",
        "    # Retorna la lista de tokens luego de aplicar stemming si no es un stopword\n",
        "    return list(stemmer.stem(w) for w in tokens if w not in stop_words)\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer utilizando la función como analyzer\n",
        "vectorizer = CountVectorizer(tokenizer=stem_tokenizer, analyzer='word', ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['document', 'document document', 'document second', 'fictiti', 'fictiti document', 'second', 'second document']\n",
            "[[1 0 0 1 1 0 0]\n",
            " [3 1 1 0 0 1 1]\n",
            " [0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84_UsGjXEypU"
      },
      "source": [
        "##Tokenización con remoción de stopwords y n-grams + stemming con NLTK y control de frecuencia mínima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9hG56OAFQjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796a0531-a391-4787-9bc0-cde63005f48e"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['This is the first fictitious document on Artificial Intelligence.', \n",
        "          'This document is the second Document documented.', \n",
        "          'And this is the third one. The title is Artificial Intelligence, a modern approach.',\n",
        "          'Is this the first document?']\n",
        "\n",
        "# Creamos el stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Construimos un TOKENIZER con la tokenización que provee CountVectorizer\n",
        "tokenizer = CountVectorizer().build_tokenizer()\n",
        "\n",
        "# Obtenemos las stopwords que provee SciKit Learn\n",
        "stop_words = text.ENGLISH_STOP_WORDS\n",
        "\n",
        "# Definimos una función que aplica el stemming luego de tokenizar y remover stopwords\n",
        "def  stem_tokenizer(doc):\n",
        "    # Aplica la tokenización\n",
        "    tokens = tokenizer(doc)\n",
        "    # Retorna la lista de tokens luego de aplicar stemming si no es un stopword\n",
        "    return list(stemmer.stem(w) for w in tokens if w not in stop_words)\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer utilizando la función como analyzer\n",
        "vectorizer = CountVectorizer(tokenizer=stem_tokenizer, ngram_range=(1,2), min_df=2) # puede usarse también un porcentaje .5\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['artifici', 'artifici intellig', 'document', 'intellig']\n",
            "[[1 1 1 1]\n",
            " [0 0 3 0]\n",
            " [1 1 0 1]\n",
            " [0 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuRWH5IcT1xI"
      },
      "source": [
        "##TF-IDF transformer sobre resultado de la tokenización\n",
        "Transforma una matriz de recuento de tokens a una representación normalizada TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVQ30BU4TzaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40adc00-bb1c-46c9-ead9-edf000ddc84e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "# Aplicamos Tf-Idf sobre la matrix de tokens\n",
        "X_tfidf = tfidf_transformer.fit_transform(X)\n",
        "X_tfidf.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.52303503, 0.52303503, 0.42344193, 0.52303503],\n",
              "       [0.        , 0.        , 1.        , 0.        ],\n",
              "       [0.57735027, 0.57735027, 0.        , 0.57735027],\n",
              "       [0.        , 0.        , 1.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxKzQZ6DjbZI"
      },
      "source": [
        "##Ejemplo en español"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEaesR7Nje84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c874d7bf-f9dd-4552-fccd-92a3506feb46"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['Este es el primer documento ficticio.', \n",
        "          'Este es el segundo documento que documenté. ¿Hay un documentador que documente?', \n",
        "          'Y este es el tercero mío. Tengo un método, muy metódico.',\n",
        "          'Es este el primer documento?']\n",
        "\n",
        "# Creamos el stemmer para español\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "# Obtenemos stopwords para español\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('spanish')\n",
        "print(stop_words)\n",
        "\n",
        "# Construimos un TOKENIZER con la tokenización que provee CountVectorizer\n",
        "tokenizer = CountVectorizer().build_tokenizer()\n",
        "\n",
        "# Definimos una función que aplica el stemming luego de tokenizar y remover stopwords\n",
        "def  stem_tokenizer(doc):\n",
        "    # Aplica la tokenización\n",
        "    tokens = tokenizer(doc)\n",
        "    # Retorna la lista de tokens luego de aplicar stemming si no es un stopword\n",
        "    return list(stemmer.stem(w) for w in tokens if w not in stop_words)\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer utilizando la función como analyzer\n",
        "vectorizer = CountVectorizer(tokenizer=stem_tokenizer, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "features  = vectorizer.get_feature_names()\n",
        "\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n",
            "['document', 'document document', 'document fictici', 'fictici', 'metod', 'metod metod', 'prim', 'prim document', 'segund', 'segund document', 'tercer', 'tercer metod']\n",
            "[[1 0 1 1 0 0 1 1 0 0 0 0]\n",
            " [4 3 0 0 0 0 0 0 1 1 0 0]\n",
            " [0 0 0 0 2 1 0 0 0 0 1 1]\n",
            " [1 0 0 0 0 0 1 1 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Ki-WDQlE9E"
      },
      "source": [
        "from yellowbrick.text import FreqDistVisualizer\n",
        "visualizer = FreqDistVisualizer(features=features, n=min(50,len(features)),orient='h')\n",
        "visualizer.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w48I3koyhvYv"
      },
      "source": [
        "# Preprocesamiento con TfidfVectorizer\n",
        "El TfidfVectorizer convierte una colección de documentos a una matriz TF-IDF.\n",
        "Es equivalente a aplicar CountVectorizer seguido de TfidfTransformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3jPG62th_sO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04164e8-0f1e-4233-fd10-abdce198b71d"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Creamos el conjunto de documentos que queremos analizar\n",
        "corpus = ['This is the first fictitious document.', \n",
        "          'This document is the second Document documented.', \n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?']\n",
        "# Creamos el stemmer\n",
        "stemmer = PorterStemmer()\n",
        "# Construimos un analyzer con el preprocesamiento que si provee CountVectorizer\n",
        "analyzer = TfidfVectorizer(stop_words='english').build_analyzer()\n",
        "\n",
        "# Definimos una función que suma el stemming al preprocesamiento que provee el analyzer\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "#Inicializamos y aplicamos el CountVectorizer utilizando la función como analyzer\n",
        "vectorizer = TfidfVectorizer(analyzer=stemmed_words)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['document', 'fictiti', 'second']\n",
            "[[0.53802897 0.84292635 0.        ]\n",
            " [0.88640595 0.         0.46290873]\n",
            " [0.         0.         0.        ]\n",
            " [1.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}